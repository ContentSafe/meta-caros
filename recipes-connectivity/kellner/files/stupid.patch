diff --git a/stupid.go b/stupid.go
new file mode 100644
index 0000000..8c428e9
--- /dev/null
+++ b/stupid.go
@@ -0,0 +1,18 @@
+// The only reason for this file to exist is that importing zmq will magically
+// fix the linked libdir from /lib64 to /lib.
+// Yeah, stupid, right?!
+
+package main
+import (
+    zmq "github.com/pebbe/zmq2"
+    "fmt"
+)
+
+type zmqSocket struct {
+    *zmq.Socket
+}
+
+func bogusZmqUse() {
+    s123 := zmqSocket{}
+    fmt.Println(s123)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/LICENSE.txt b/vendor/src/github.com/pebbe/zmq2/LICENSE.txt
new file mode 100644
index 0000000..5b2a4b2
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/LICENSE.txt
@@ -0,0 +1,25 @@
+Copyright (c) 2013-2014, Peter Kleiweg
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
+TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
+PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/vendor/src/github.com/pebbe/zmq2/README.md b/vendor/src/github.com/pebbe/zmq2/README.md
new file mode 100644
index 0000000..a267c2b
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/README.md
@@ -0,0 +1,28 @@
+A Go interface to [ZeroMQ](http://www.zeromq.org/) version 2.
+
+Requires ZeroMQ version 2.1 or 2.2
+
+For ZeroMQ version 4, see: http://github.com/pebbe/zmq4
+
+For ZeroMQ version 3, see: http://github.com/pebbe/zmq3
+
+Including all examples of [ØMQ - The Guide](http://zguide.zeromq.org/page:all).
+
+Keywords: zmq, zeromq, 0mq, networks, distributed computing, message passing, fanout, pubsub, pipeline, request-reply
+
+## Install
+
+    go get github.com/pebbe/zmq2
+
+## Docs
+
+ * [package help](http://godoc.org/github.com/pebbe/zmq2)
+ * [wiki](https://github.com/pebbe/zmq4/wiki) (for zmq4)
+
+## Support for ZeroMQ version 2.1
+
+ * The following functions are not supported in ZeroMQ version 2.1, and will return an error:
+  * (*Socket) GetRcvtimeo
+  * (*Socket) GetSndtimeo
+  * (*Socket) SetRcvtimeo
+  * (*Socket) SetSndtimeo
diff --git a/vendor/src/github.com/pebbe/zmq2/doc.go b/vendor/src/github.com/pebbe/zmq2/doc.go
new file mode 100644
index 0000000..dc619ed
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/doc.go
@@ -0,0 +1,26 @@
+/*
+A Go interface to ZeroMQ (zmq, 0mq) version 2.
+
+For ZeroMQ version 4, see: http://github.com/pebbe/zmq4
+
+For ZeroMQ version 3, see: http://github.com/pebbe/zmq3
+
+Requires ZeroMQ version 2.1 or 2.2
+
+The following functions return ErrorNotImplemented in 0MQ version 2.1:
+
+(*Socket)GetRcvtimeo, (*Socket)GetSndtimeo, (*Socket)SetRcvtimeo, (*Socket)SetSndtimeo
+
+http://www.zeromq.org/
+
+See also the wiki (for zmq4): https://github.com/pebbe/zmq4/wiki
+
+A note on the use of a context:
+
+This package provides a default context. This is what will be used by
+the functions without a context receiver, that create a socket or
+manipulate the context. Package developers that import this package
+should probably not use the default context with its associated
+functions, but create their own context(s). See: type Context.
+*/
+package zmq2
diff --git a/vendor/src/github.com/pebbe/zmq2/dummy.c b/vendor/src/github.com/pebbe/zmq2/dummy.c
new file mode 100644
index 0000000..0fca94d
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/dummy.c
@@ -0,0 +1,5 @@
+/*
+
+You need CGO_ENABLED=1 to build this package
+
+*/
diff --git a/vendor/src/github.com/pebbe/zmq2/errors.go b/vendor/src/github.com/pebbe/zmq2/errors.go
new file mode 100644
index 0000000..58db0c6
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/errors.go
@@ -0,0 +1,86 @@
+package zmq2
+
+/*
+#include <zmq.h>
+#ifndef ENOTSOCK
+#define ENOTSOCK (ZMQ_HAUSNUMERO + 9)
+#endif
+*/
+import "C"
+
+import (
+	"syscall"
+)
+
+// An Errno is an unsigned number describing an error condition as returned by a call to ZeroMQ.
+// It implements the error interface.
+// The number is either a standard system error, or an error defined by the C library of ZeroMQ.
+type Errno uintptr
+
+const (
+	// Error conditions defined by the C library of ZeroMQ.
+
+	// On Windows platform some of the standard POSIX errnos are not defined.
+	EADDRINUSE      = Errno(C.EADDRINUSE)
+	EADDRNOTAVAIL   = Errno(C.EADDRNOTAVAIL)
+	ECONNREFUSED    = Errno(C.ECONNREFUSED)
+	EINPROGRESS     = Errno(C.EINPROGRESS)
+	ENETDOWN        = Errno(C.ENETDOWN)
+	ENOBUFS         = Errno(C.ENOBUFS)
+	ENOTSOCK        = Errno(C.ENOTSOCK)
+	ENOTSUP         = Errno(C.ENOTSUP)
+	EPROTONOSUPPORT = Errno(C.EPROTONOSUPPORT)
+
+	// Native 0MQ error codes.
+	EFSM           = Errno(C.EFSM)
+	EMTHREAD       = Errno(C.EMTHREAD)
+	ENOCOMPATPROTO = Errno(C.ENOCOMPATPROTO)
+	ETERM          = Errno(C.ETERM)
+)
+
+func errget(err error) error {
+	eno, ok := err.(syscall.Errno)
+	if ok {
+		return Errno(eno)
+	}
+	return err
+}
+
+// Return Errno as string.
+func (errno Errno) Error() string {
+	if errno >= C.ZMQ_HAUSNUMERO {
+		return C.GoString(C.zmq_strerror(C.int(errno)))
+	}
+	return syscall.Errno(errno).Error()
+}
+
+/*
+Convert error to Errno.
+
+Example usage:
+
+    switch AsErrno(err) {
+
+    case zmq.Errno(syscall.EINTR):
+        // standard system error
+
+        // call was interrupted
+
+    case zmq.ETERM:
+        // error defined by ZeroMQ
+
+        // context was terminated
+
+    }
+
+See also: examples/interrupt.go
+*/
+func AsErrno(err error) Errno {
+	if eno, ok := err.(Errno); ok {
+		return eno
+	}
+	if eno, ok := err.(syscall.Errno); ok {
+		return Errno(eno)
+	}
+	return Errno(0)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/Build.sh b/vendor/src/github.com/pebbe/zmq2/examples/Build.sh
new file mode 100755
index 0000000..fcee508
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/Build.sh
@@ -0,0 +1,69 @@
+#!/bin/sh
+
+go get github.com/pborman/uuid
+
+for i in bstar mdapi flcliapi kvsimple kvmsg clone intface
+do
+    go install github.com/pebbe/zmq2/examples/$i
+done
+
+cd `dirname $0`
+
+goos=`go env GOOS`
+gobin=`go env GOBIN`
+if [ "$gobin" = "" ]
+then
+    gobin=`go env GOPATH`
+    if [ "$gobin" = "" ]
+    then
+	gobin=`go env GOROOT`
+    fi
+    gobin=`echo $gobin | sed -e 's/:.*//'`
+    gobin=$gobin/bin
+fi
+
+dir=$gobin/zmq2-examples
+
+echo Installing examples in $dir
+
+mkdir -p $dir
+
+for i in *.sh
+do
+    if [ $i != Build.sh ]
+    then
+	cp -u $i $dir
+    fi
+done
+
+src=''
+for i in *.go
+do
+    if [ $i = interrupt.go ]
+    then
+	if [ $goos = windows -o $goos = plan9 ]
+	then
+	    continue
+	fi
+    fi
+    bin=$dir/`basename $i .go`
+    if [ ! -f $bin -o $i -nt $bin ]
+    then
+	src="$src $i"
+    fi
+done
+
+libs=`pkg-config --libs-only-L libzmq`
+if [ "$libs" = "" ]
+then
+    for i in $src
+    do
+	go build -o $dir/`basename $i .go` $i
+    done
+else
+    libs="-r `echo $libs | sed -e 's/-L//; s/  *-L/:/g'`"
+    for i in $src
+    do
+	go build -ldflags="$libs" -o $dir/`basename $i .go` $i
+    done
+fi
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/README.md b/vendor/src/github.com/pebbe/zmq2/examples/README.md
new file mode 100644
index 0000000..764a7ea
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/README.md
@@ -0,0 +1,2 @@
+These are examples from [ØMQ - The Guide](http://zguide.zeromq.org/page:all), 
+re-implemented for the current Go package.
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/asyncsrv.go b/vendor/src/github.com/pebbe/zmq2/examples/asyncsrv.go
new file mode 100644
index 0000000..a3a6e44
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/asyncsrv.go
@@ -0,0 +1,138 @@
+//
+//  Asynchronous client-to-server (DEALER to ROUTER).
+//
+//  While this example runs in a single process, that is just to make
+//  it easier to start and stop the example. Each task has its own
+//  context and conceptually acts as a separate process.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"log"
+	"math/rand"
+	"sync"
+	"time"
+)
+
+//  ---------------------------------------------------------------------
+//  This is our client task
+//  It connects to the server, and then sends a request once per second
+//  It collects responses as they arrive, and it prints them out. We will
+//  run several client tasks in parallel, each with a different random ID.
+
+func client_task() {
+	var mu sync.Mutex
+
+	client, _ := zmq.NewSocket(zmq.DEALER)
+	defer client.Close()
+
+	//  Set random identity to make tracing easier
+	set_id(client)
+	client.Connect("tcp://localhost:5570")
+
+	go func() {
+		for request_nbr := 1; true; request_nbr++ {
+			time.Sleep(time.Second)
+			mu.Lock()
+			client.SendMessage(fmt.Sprintf("request #%d", request_nbr))
+			mu.Unlock()
+		}
+	}()
+
+	for {
+		time.Sleep(10 * time.Millisecond)
+		mu.Lock()
+		msg, err := client.RecvMessage(zmq.NOBLOCK)
+		if err == nil {
+			id, _ := client.GetIdentity()
+			fmt.Println(msg[0], id)
+		}
+		mu.Unlock()
+	}
+}
+
+//  This is our server task.
+//  It uses the multithreaded server model to deal requests out to a pool
+//  of workers and route replies back to clients. One worker can handle
+//  one request at a time but one client can talk to multiple workers at
+//  once.
+
+func server_task() {
+
+	//  Frontend socket talks to clients over TCP
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	defer frontend.Close()
+	frontend.Bind("tcp://*:5570")
+
+	//  Backend socket talks to workers over inproc
+	backend, _ := zmq.NewSocket(zmq.DEALER)
+	defer backend.Close()
+	backend.Bind("inproc://backend")
+
+	//  Launch pool of worker threads, precise number is not critical
+	for i := 0; i < 5; i++ {
+		go server_worker()
+	}
+
+	//  Connect backend to frontend via a proxy
+	err := zmq.Proxy(frontend, backend, nil)
+	log.Fatalln("Proxy interrupted:", err)
+}
+
+//  Each worker task works on one request at a time and sends a random number
+//  of replies back, with random delays between replies:
+
+func server_worker() {
+
+	worker, _ := zmq.NewSocket(zmq.DEALER)
+	defer worker.Close()
+	worker.Connect("inproc://backend")
+
+	for {
+		//  The DEALER socket gives us the reply envelope and message
+		msg, _ := worker.RecvMessage(0)
+		identity, content := pop(msg)
+
+		//  Send 0..4 replies back
+		replies := rand.Intn(5)
+		for reply := 0; reply < replies; reply++ {
+			//  Sleep for some fraction of a second
+			time.Sleep(time.Duration(rand.Intn(1000)+1) * time.Millisecond)
+			worker.SendMessage(identity, content)
+		}
+	}
+}
+
+//  The main thread simply starts several clients, and a server, and then
+//  waits for the server to finish.
+
+func main() {
+	rand.Seed(time.Now().UnixNano())
+
+	go client_task()
+	go client_task()
+	go client_task()
+	go server_task()
+
+	//  Run for 5 seconds then quit
+	time.Sleep(5 * time.Second)
+}
+
+func set_id(soc *zmq.Socket) {
+	identity := fmt.Sprintf("%04X-%04X", rand.Intn(0x10000), rand.Intn(0x10000))
+	soc.SetIdentity(identity)
+}
+
+func pop(msg []string) (head, tail []string) {
+	if msg[1] == "" {
+		head = msg[:2]
+		tail = msg[2:]
+	} else {
+		head = msg[:1]
+		tail = msg[1:]
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/bstar/bstar.go b/vendor/src/github.com/pebbe/zmq2/examples/bstar/bstar.go
new file mode 100644
index 0000000..a14d569
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/bstar/bstar.go
@@ -0,0 +1,275 @@
+//  bstar - Binary Star reactor.
+package bstar
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"errors"
+	"log"
+	"strconv"
+	"time"
+)
+
+const (
+	PRIMARY = true
+	BACKUP  = false
+)
+
+//  States we can be in at any point in time
+type state_t int
+
+const (
+	_             = state_t(iota)
+	state_PRIMARY //  Primary, waiting for peer to connect
+	state_BACKUP  //  Backup, waiting for peer to connect
+	state_ACTIVE  //  Active - accepting connections
+	state_PASSIVE //  Passive - not accepting connections
+)
+
+//  Events, which start with the states our peer can be in
+type event_t int
+
+const (
+	_              = event_t(iota)
+	peer_PRIMARY   //  HA peer is pending primary
+	peer_BACKUP    //  HA peer is pending backup
+	peer_ACTIVE    //  HA peer is active
+	peer_PASSIVE   //  HA peer is passive
+	client_REQUEST //  Client makes request
+)
+
+//  Structure of our class
+
+type Bstar struct {
+	Reactor     *zmq.Reactor            //  Reactor loop
+	statepub    *zmq.Socket             //  State publisher
+	statesub    *zmq.Socket             //  State subscriber
+	state       state_t                 //  Current state
+	event       event_t                 //  Current event
+	peer_expiry time.Time               //  When peer is considered 'dead'
+	voter_fn    func(*zmq.Socket) error //  Voting socket handler
+	active_fn   func() error            //  Call when become active
+	passive_fn  func() error            //  Call when become passive
+}
+
+//  The finite-state machine is the same as in the proof-of-concept server.
+//  To understand this reactor in detail, first read the CZMQ zloop class.
+
+//  We send state information every this often
+//  If peer doesn't respond in two heartbeats, it is 'dead'
+const (
+	bstar_HEARTBEAT = 1000 * time.Millisecond //  In msecs
+)
+
+//  ---------------------------------------------------------------------
+//  Binary Star finite state machine (applies event to state)
+//  Returns error if there was an exception, nil if event was valid.
+
+func (bstar *Bstar) execute_fsm() (exception error) {
+	//  Primary server is waiting for peer to connect
+	//  Accepts client_REQUEST events in this state
+	if bstar.state == state_PRIMARY {
+		if bstar.event == peer_BACKUP {
+			log.Println("I: connected to backup (passive), ready as active")
+			bstar.state = state_ACTIVE
+			if bstar.active_fn != nil {
+				bstar.active_fn()
+			}
+		} else if bstar.event == peer_ACTIVE {
+			log.Println("I: connected to backup (active), ready as passive")
+			bstar.state = state_PASSIVE
+			if bstar.passive_fn != nil {
+				bstar.passive_fn()
+			}
+		} else if bstar.event == client_REQUEST {
+			// Allow client requests to turn us into the active if we've
+			// waited sufficiently long to believe the backup is not
+			// currently acting as active (i.e., after a failover)
+			if time.Now().After(bstar.peer_expiry) {
+				log.Println("I: request from client, ready as active")
+				bstar.state = state_ACTIVE
+				if bstar.active_fn != nil {
+					bstar.active_fn()
+				}
+			} else {
+				// Don't respond to clients yet - it's possible we're
+				// performing a failback and the backup is currently active
+				exception = errors.New("Exception")
+			}
+		}
+	} else if bstar.state == state_BACKUP {
+		//  Backup server is waiting for peer to connect
+		//  Rejects client_REQUEST events in this state
+		if bstar.event == peer_ACTIVE {
+			log.Println("I: connected to primary (active), ready as passive")
+			bstar.state = state_PASSIVE
+			if bstar.passive_fn != nil {
+				bstar.passive_fn()
+			}
+		} else if bstar.event == client_REQUEST {
+			exception = errors.New("Exception")
+		}
+	} else if bstar.state == state_ACTIVE {
+		//  Server is active
+		//  Accepts client_REQUEST events in this state
+		//  The only way out of ACTIVE is death
+		if bstar.event == peer_ACTIVE {
+			//  Two actives would mean split-brain
+			log.Println("E: fatal error - dual actives, aborting")
+			exception = errors.New("Exception")
+		}
+	} else if bstar.state == state_PASSIVE {
+		//  Server is passive
+		//  client_REQUEST events can trigger failover if peer looks dead
+		if bstar.event == peer_PRIMARY {
+			//  Peer is restarting - become active, peer will go passive
+			log.Println("I: primary (passive) is restarting, ready as active")
+			bstar.state = state_ACTIVE
+		} else if bstar.event == peer_BACKUP {
+			//  Peer is restarting - become active, peer will go passive
+			log.Println("I: backup (passive) is restarting, ready as active")
+			bstar.state = state_ACTIVE
+		} else if bstar.event == peer_PASSIVE {
+			//  Two passives would mean cluster would be non-responsive
+			log.Println("E: fatal error - dual passives, aborting")
+			exception = errors.New("Exception")
+		} else if bstar.event == client_REQUEST {
+			//  Peer becomes active if timeout has passed
+			//  It's the client request that triggers the failover
+			if time.Now().After(bstar.peer_expiry) {
+				//  If peer is dead, switch to the active state
+				log.Println("I: failover successful, ready as active")
+				bstar.state = state_ACTIVE
+			} else {
+				//  If peer is alive, reject connections
+				exception = errors.New("Exception")
+			}
+		}
+		//  Call state change handler if necessary
+		if bstar.state == state_ACTIVE && bstar.active_fn != nil {
+			bstar.active_fn()
+		}
+	}
+	return
+}
+
+func (bstar *Bstar) update_peer_expiry() {
+	bstar.peer_expiry = time.Now().Add(2 * bstar_HEARTBEAT)
+}
+
+//  ---------------------------------------------------------------------
+//  Reactor event handlers...
+
+//  Publish our state to peer
+func (bstar *Bstar) send_state() (err error) {
+	_, err = bstar.statepub.SendMessage(int(bstar.state))
+	return
+}
+
+//  Receive state from peer, execute finite state machine
+func (bstar *Bstar) recv_state() (err error) {
+	msg, err := bstar.statesub.RecvMessage(0)
+	if err == nil {
+		e, _ := strconv.Atoi(msg[0])
+		bstar.event = event_t(e)
+	}
+	return bstar.execute_fsm()
+}
+
+//  Application wants to speak to us, see if it's possible
+func (bstar *Bstar) voter_ready(socket *zmq.Socket) error {
+	//  If server can accept input now, call appl handler
+	bstar.event = client_REQUEST
+	err := bstar.execute_fsm()
+	if err == nil {
+		bstar.voter_fn(socket)
+	} else {
+		//  Destroy waiting message, no-one to read it
+		socket.RecvMessage(0)
+	}
+	return nil
+}
+
+//  This is the constructor for our bstar class. We have to tell it whether
+//  we're primary or backup server, and our local and remote endpoints to
+//  bind and connect to:
+
+func New(primary bool, local, remote string) (bstar *Bstar, err error) {
+
+	bstar = &Bstar{}
+
+	//  Initialize the Binary Star
+	bstar.Reactor = zmq.NewReactor()
+	if primary {
+		bstar.state = state_PRIMARY
+	} else {
+		bstar.state = state_BACKUP
+	}
+
+	//  Create publisher for state going to peer
+	bstar.statepub, err = zmq.NewSocket(zmq.PUB)
+	bstar.statepub.Bind(local)
+
+	//  Create subscriber for state coming from peer
+	bstar.statesub, err = zmq.NewSocket(zmq.SUB)
+	bstar.statesub.SetSubscribe("")
+	bstar.statesub.Connect(remote)
+
+	//  Set-up basic reactor events
+	bstar.Reactor.AddChannelTime(time.Tick(bstar_HEARTBEAT), 1,
+		func(i interface{}) error { return bstar.send_state() })
+	bstar.Reactor.AddSocket(bstar.statesub, zmq.POLLIN,
+		func(e zmq.State) error { return bstar.recv_state() })
+
+	return
+}
+
+//  The voter method registers a client voter socket. Messages received
+//  on this socket provide the client_REQUEST events for the Binary Star
+//  FSM and are passed to the provided application handler. We require
+//  exactly one voter per bstar instance:
+
+func (bstar *Bstar) Voter(endpoint string, socket_type zmq.Type, handler func(*zmq.Socket) error) {
+	//  Hold actual handler so we can call this later
+	socket, _ := zmq.NewSocket(socket_type)
+	socket.Bind(endpoint)
+	if bstar.voter_fn != nil {
+		panic("Double voter function")
+	}
+	bstar.voter_fn = handler
+	bstar.Reactor.AddSocket(socket, zmq.POLLIN,
+		func(e zmq.State) error { return bstar.voter_ready(socket) })
+}
+
+//  Register handlers to be called each time there's a state change:
+
+func (bstar *Bstar) NewActive(handler func() error) {
+	if bstar.active_fn != nil {
+		panic("Double Active")
+	}
+	bstar.active_fn = handler
+}
+
+func (bstar *Bstar) NewPassive(handler func() error) {
+	if bstar.passive_fn != nil {
+		panic("Double Passive")
+	}
+	bstar.passive_fn = handler
+}
+
+//  Enable/disable verbose tracing, for debugging:
+
+func (bstar *Bstar) SetVerbose(verbose bool) {
+	bstar.Reactor.SetVerbose(verbose)
+}
+
+//?  Finally, start the configured reactor. It will end if any handler
+//?  returns error to the reactor, or if the process receives SIGINT or SIGTERM:
+
+func (bstar *Bstar) Start() error {
+	if bstar.voter_fn == nil {
+		panic("Missing voter function")
+	}
+	bstar.update_peer_expiry()
+	return bstar.Reactor.Run(bstar_HEARTBEAT / 5)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/bstarcli.go b/vendor/src/github.com/pebbe/zmq2/examples/bstarcli.go
new file mode 100644
index 0000000..05352b4
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/bstarcli.go
@@ -0,0 +1,83 @@
+//
+//  Binary Star client proof-of-concept implementation. This client does no
+//  real work; it just demonstrates the Binary Star failover model.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"strconv"
+	"time"
+)
+
+const (
+	REQUEST_TIMEOUT = 1000 * time.Millisecond //  msecs
+	SETTLE_DELAY    = 2000 * time.Millisecond //  Before failing over
+)
+
+func main() {
+
+	server := []string{"tcp://localhost:5001", "tcp://localhost:5002"}
+	server_nbr := 0
+
+	fmt.Printf("I: connecting to server at %s...\n", server[server_nbr])
+	client, _ := zmq.NewSocket(zmq.REQ)
+	client.Connect(server[server_nbr])
+
+	poller := zmq.NewPoller()
+	poller.Add(client, zmq.POLLIN)
+
+	sequence := 0
+LOOP:
+	for {
+		//  We send a request, then we work to get a reply
+		sequence++
+		client.SendMessage(sequence)
+
+		for expect_reply := true; expect_reply; {
+			//  Poll socket for a reply, with timeout
+			polled, err := poller.Poll(REQUEST_TIMEOUT)
+			if err != nil {
+				break LOOP //  Interrupted
+			}
+
+			//  We use a Lazy Pirate strategy in the client. If there's no
+			//  reply within our timeout, we close the socket and try again.
+			//  In Binary Star, it's the client vote which decides which
+			//  server is primary; the client must therefore try to connect
+			//  to each server in turn:
+
+			if len(polled) == 1 {
+				//  We got a reply from the server, must match sequence
+				reply, _ := client.RecvMessage(0)
+				seq, _ := strconv.Atoi(reply[0])
+				if seq == sequence {
+					fmt.Printf("I: server replied OK (%s)\n", reply[0])
+					expect_reply = false
+					time.Sleep(time.Second) //  One request per second
+				} else {
+					fmt.Printf("E: bad reply from server: %q\n", reply)
+				}
+
+			} else {
+				fmt.Println("W: no response from server, failing over")
+
+				//  Old socket is confused; close it and open a new one
+				client.Close()
+				server_nbr = 1 - server_nbr
+				time.Sleep(SETTLE_DELAY)
+				fmt.Printf("I: connecting to server at %s...\n", server[server_nbr])
+				client, _ = zmq.NewSocket(zmq.REQ)
+				client.Connect(server[server_nbr])
+
+				poller = zmq.NewPoller()
+				poller.Add(client, zmq.POLLIN)
+
+				//  Send request again, on new socket
+				client.SendMessage(sequence)
+			}
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/bstarsrv.go b/vendor/src/github.com/pebbe/zmq2/examples/bstarsrv.go
new file mode 100644
index 0000000..cc4d289
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/bstarsrv.go
@@ -0,0 +1,194 @@
+//
+//  Binary Star server proof-of-concept implementation. This server does no
+//  real work; it just demonstrates the Binary Star failover model.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"os"
+	"strconv"
+	"time"
+)
+
+//  States we can be in at any point in time
+type state_t int
+
+const (
+	_             = state_t(iota)
+	STATE_PRIMARY //  Primary, waiting for peer to connect
+	STATE_BACKUP  //  Backup, waiting for peer to connect
+	STATE_ACTIVE  //  Active - accepting connections
+	STATE_PASSIVE //  Passive - not accepting connections
+)
+
+//  Events, which start with the states our peer can be in
+type event_t int
+
+const (
+	_              = event_t(iota)
+	PEER_PRIMARY   //  HA peer is pending primary
+	PEER_BACKUP    //  HA peer is pending backup
+	PEER_ACTIVE    //  HA peer is active
+	PEER_PASSIVE   //  HA peer is passive
+	CLIENT_REQUEST //  Client makes request
+)
+
+//  Our finite state machine
+type bstar_t struct {
+	state       state_t   //  Current state
+	event       event_t   //  Current event
+	peer_expiry time.Time //  When peer is considered 'dead'
+}
+
+//  We send state information every this often
+//  If peer doesn't respond in two heartbeats, it is 'dead'
+const (
+	HEARTBEAT = 1000 * time.Millisecond //  In msecs
+)
+
+//  The heart of the Binary Star design is its finite-state machine (FSM).
+//  The FSM runs one event at a time. We apply an event to the current state,
+//  which checks if the event is accepted, and if so sets a new state:
+
+func StateMachine(fsm *bstar_t) (exception bool) {
+	//  These are the PRIMARY and BACKUP states; we're waiting to become
+	//  ACTIVE or PASSIVE depending on events we get from our peer:
+	if fsm.state == STATE_PRIMARY {
+		if fsm.event == PEER_BACKUP {
+			fmt.Println("I: connected to backup (passive), ready as active")
+			fsm.state = STATE_ACTIVE
+		} else if fsm.event == PEER_ACTIVE {
+			fmt.Println("I: connected to backup (active), ready as passive")
+			fsm.state = STATE_PASSIVE
+		}
+		//  Accept client connections
+	} else if fsm.state == STATE_BACKUP {
+		if fsm.event == PEER_ACTIVE {
+			fmt.Println("I: connected to primary (active), ready as passive")
+			fsm.state = STATE_PASSIVE
+		} else if fsm.event == CLIENT_REQUEST {
+			//  Reject client connections when acting as backup
+			exception = true
+		}
+	} else if fsm.state == STATE_ACTIVE {
+		//  These are the ACTIVE and PASSIVE states:
+		if fsm.event == PEER_ACTIVE {
+			//  Two actives would mean split-brain
+			fmt.Println("E: fatal error - dual actives, aborting")
+			exception = true
+		}
+	} else if fsm.state == STATE_PASSIVE {
+		//  Server is passive
+		//  CLIENT_REQUEST events can trigger failover if peer looks dead
+		if fsm.event == PEER_PRIMARY {
+			//  Peer is restarting - become active, peer will go passive
+			fmt.Println("I: primary (passive) is restarting, ready as active")
+			fsm.state = STATE_ACTIVE
+		} else if fsm.event == PEER_BACKUP {
+			//  Peer is restarting - become active, peer will go passive
+			fmt.Println("I: backup (passive) is restarting, ready as active")
+			fsm.state = STATE_ACTIVE
+		} else if fsm.event == PEER_PASSIVE {
+			//  Two passives would mean cluster would be non-responsive
+			fmt.Println("E: fatal error - dual passives, aborting")
+			exception = true
+		} else if fsm.event == CLIENT_REQUEST {
+			//  Peer becomes active if timeout has passed
+			//  It's the client request that triggers the failover
+			if time.Now().After(fsm.peer_expiry) {
+				//  If peer is dead, switch to the active state
+				fmt.Println("I: failover successful, ready as active")
+				fsm.state = STATE_ACTIVE
+			} else {
+				//  If peer is alive, reject connections
+				exception = true
+			}
+		}
+	}
+	return
+}
+
+//  This is our main task. First we bind/connect our sockets with our
+//  peer and make sure we will get state messages correctly. We use
+//  three sockets; one to publish state, one to subscribe to state, and
+//  one for client requests/replies:
+
+func main() {
+	//  Arguments can be either of:
+	//      -p  primary server, at tcp://localhost:5001
+	//      -b  backup server, at tcp://localhost:5002
+	statepub, _ := zmq.NewSocket(zmq.PUB)
+	statesub, _ := zmq.NewSocket(zmq.SUB)
+	statesub.SetSubscribe("")
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	fsm := &bstar_t{peer_expiry: time.Now().Add(2 * HEARTBEAT)}
+
+	if len(os.Args) == 2 && os.Args[1] == "-p" {
+		fmt.Println("I: Primary active, waiting for backup (passive)")
+		frontend.Bind("tcp://*:5001")
+		statepub.Bind("tcp://*:5003")
+		statesub.Connect("tcp://localhost:5004")
+		fsm.state = STATE_PRIMARY
+	} else if len(os.Args) == 2 && os.Args[1] == "-b" {
+		fmt.Println("I: Backup passive, waiting for primary (active)")
+		frontend.Bind("tcp://*:5002")
+		statepub.Bind("tcp://*:5004")
+		statesub.Connect("tcp://localhost:5003")
+		fsm.state = STATE_BACKUP
+	} else {
+		fmt.Println("Usage: bstarsrv { -p | -b }")
+		return
+	}
+	//  We now process events on our two input sockets, and process these
+	//  events one at a time via our finite-state machine. Our "work" for
+	//  a client request is simply to echo it back:
+
+	//  Set timer for next outgoing state message
+	send_state_at := time.Now().Add(HEARTBEAT)
+
+	poller := zmq.NewPoller()
+	poller.Add(frontend, zmq.POLLIN)
+	poller.Add(statesub, zmq.POLLIN)
+
+LOOP:
+	for {
+		time_left := send_state_at.Sub(time.Now())
+		if time_left < 0 {
+			time_left = 0
+		}
+		polled, err := poller.Poll(time_left)
+		if err != nil {
+			break //  Context has been shut down
+		}
+		for _, socket := range polled {
+			switch socket.Socket {
+			case frontend:
+				//  Have a client request
+				msg, _ := frontend.RecvMessage(0)
+				fsm.event = CLIENT_REQUEST
+				if !StateMachine(fsm) {
+					//  Answer client by echoing request back
+					frontend.SendMessage(msg)
+				}
+			case statesub:
+				//  Have state from our peer, execute as event
+				message, _ := statesub.RecvMessage(0)
+				i, _ := strconv.Atoi(message[0])
+				fsm.event = event_t(i)
+				if StateMachine(fsm) {
+					break LOOP //  Error, so exit
+				}
+				fsm.peer_expiry = time.Now().Add(2 * HEARTBEAT)
+			}
+		}
+		//  If we timed-out, send state to peer
+		if time.Now().After(send_state_at) {
+			statepub.SendMessage(int(fsm.state))
+			send_state_at = time.Now().Add(HEARTBEAT)
+		}
+	}
+	fmt.Println("W: interrupted")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/bstarsrv2.go b/vendor/src/github.com/pebbe/zmq2/examples/bstarsrv2.go
new file mode 100644
index 0000000..c891f55
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/bstarsrv2.go
@@ -0,0 +1,43 @@
+//
+//  Binary Star server, using bstar reactor.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/bstar"
+
+	"fmt"
+	"os"
+)
+
+//  Echo service
+func echo(socket *zmq.Socket) (err error) {
+	msg, err := socket.RecvMessage(0)
+	if err != nil {
+		return
+	}
+	_, err = socket.SendMessage(msg)
+	return
+}
+
+func main() {
+	//  Arguments can be either of:
+	//      -p  primary server, at tcp://localhost:5001
+	//      -b  backup server, at tcp://localhost:5002
+	var bst *bstar.Bstar
+	if len(os.Args) == 2 && os.Args[1] == "-p" {
+		fmt.Println("I: Primary active, waiting for backup (passive)")
+		bst, _ = bstar.New(bstar.PRIMARY, "tcp://*:5003", "tcp://localhost:5004")
+		bst.Voter("tcp://*:5001", zmq.ROUTER, echo)
+	} else if len(os.Args) == 2 && os.Args[1] == "-b" {
+		fmt.Println("I: Backup passive, waiting for primary (active)")
+		bst, _ = bstar.New(bstar.BACKUP, "tcp://*:5004", "tcp://localhost:5003")
+		bst.Voter("tcp://*:5002", zmq.ROUTER, echo)
+	} else {
+		fmt.Println("Usage: bstarsrvs { -p | -b }")
+		return
+	}
+	bst.Start()
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clone/clone.go b/vendor/src/github.com/pebbe/zmq2/examples/clone/clone.go
new file mode 100644
index 0000000..0cee2a5
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clone/clone.go
@@ -0,0 +1,304 @@
+// Clone client API stack (multithreaded).
+package clone
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvmsg"
+
+	"fmt"
+	"strconv"
+	"time"
+)
+
+//  =====================================================================
+//  Synchronous part, works in our application thread
+
+//  ---------------------------------------------------------------------
+//  Structure of our class
+
+var (
+	pipe_nmb uint64
+)
+
+type Clone struct {
+	pipe *zmq.Socket //  Pipe through to clone agent
+}
+
+//  This is the thread that handles our real clone class
+
+//  Constructor for the clone class. Note that we create
+//  the pipe that connects our frontend to the
+//  backend agent:
+
+func New() (clone *Clone) {
+	clone = &Clone{}
+	clone.pipe, _ = zmq.NewSocket(zmq.PAIR)
+	pipename := fmt.Sprint("inproc://pipe", pipe_nmb)
+	pipe_nmb++
+	clone.pipe.Bind(pipename)
+	go clone_agent(pipename)
+	return
+}
+
+//  Specify subtree for snapshot and updates, which we must do before
+//  connecting to a server since the subtree specification is sent as
+//  first command to the server. Sends a [SUBTREE][subtree] command to
+//  the agent:
+
+func (clone *Clone) Subtree(subtree string) {
+	clone.pipe.SendMessage("SUBTREE", subtree)
+}
+
+//  Connect to a new server endpoint. We can connect to at most two
+//  servers. Sends [CONNECT][endpoint][service] to the agent:
+
+func (clone *Clone) Connect(address, service string) {
+	clone.pipe.SendMessage("CONNECT", address, service)
+}
+
+//  Set a new value in the shared hashmap. Sends a [SET][key][value][ttl]
+//  command through to the agent which does the actual work:
+
+func (clone *Clone) Set(key, value string, ttl int) {
+	clone.pipe.SendMessage("SET", key, value, ttl)
+}
+
+//  Look-up value in distributed hash table. Sends [GET][key] to the agent and
+//  waits for a value response. If there is no value available, will eventually
+//  return error:
+
+func (clone *Clone) Get(key string) (value string, err error) {
+
+	clone.pipe.SendMessage("GET", key)
+
+	reply, e := clone.pipe.RecvMessage(0)
+	if e != nil {
+		err = e
+		return
+	}
+	value = reply[0]
+	return
+}
+
+//  The back-end agent manages a set of servers, which we implement using
+//  our simple class model:
+
+type server_t struct {
+	address    string      //  Server address
+	port       int         //  Server port
+	snapshot   *zmq.Socket //  Snapshot socket
+	subscriber *zmq.Socket //  Incoming updates
+	expiry     time.Time   //  When server expires
+	requests   int64       //  How many snapshot requests made?
+}
+
+func server_new(address string, port int, subtree string) (server *server_t) {
+	server = &server_t{}
+
+	fmt.Printf("I: adding server %s:%d...\n", address, port)
+	server.address = address
+	server.port = port
+
+	server.snapshot, _ = zmq.NewSocket(zmq.DEALER)
+	server.snapshot.Connect(fmt.Sprintf("%s:%d", address, port))
+	server.subscriber, _ = zmq.NewSocket(zmq.SUB)
+	server.subscriber.Connect(fmt.Sprintf("%s:%d", address, port+1))
+	server.subscriber.SetSubscribe(subtree)
+	return
+}
+
+//  Here is the implementation of the back-end agent itself:
+
+const (
+	//  Number of servers we will talk to
+	server_MAX = 2
+
+	//  Server considered dead if silent for this long
+	server_TTL = 5000 * time.Millisecond
+)
+
+const (
+	//  States we can be in
+	state_INITIAL = iota //  Before asking server for state
+	state_SYNCING        //  Getting state from server
+	state_ACTIVE         //  Getting new updates from server
+)
+
+type agent_t struct {
+	pipe        *zmq.Socket             //  Pipe back to application
+	kvmap       map[string]*kvmsg.Kvmsg //  Actual key/value table
+	subtree     string                  //  Subtree specification, if any
+	server      [server_MAX]*server_t
+	nbr_servers int         //  0 to SERVER_MAX
+	state       int         //  Current state
+	cur_server  int         //  If active, server 0 or 1
+	sequence    int64       //  Last kvmsg processed
+	publisher   *zmq.Socket //  Outgoing updates
+}
+
+func agent_new(pipe *zmq.Socket) (agent *agent_t) {
+	agent = &agent_t{}
+	agent.pipe = pipe
+	agent.kvmap = make(map[string]*kvmsg.Kvmsg)
+	agent.subtree = ""
+	agent.state = state_INITIAL
+	agent.publisher, _ = zmq.NewSocket(zmq.PUB)
+	return
+}
+
+//  Here we handle the different control messages from the front-end;
+//  SUBTREE, CONNECT, SET, and GET:
+
+func (agent *agent_t) control_message() (err error) {
+	msg, e := agent.pipe.RecvMessage(0)
+	if e != nil {
+		return e
+	}
+	command := msg[0]
+	msg = msg[1:]
+
+	switch command {
+	case "SUBTREE":
+		agent.subtree = msg[0]
+	case "CONNECT":
+		address := msg[0]
+		service := msg[1]
+		if agent.nbr_servers < server_MAX {
+			serv, _ := strconv.Atoi(service)
+			agent.server[agent.nbr_servers] = server_new(address, serv, agent.subtree)
+			agent.nbr_servers++
+			//  We broadcast updates to all known servers
+			agent.publisher.Connect(fmt.Sprintf("%s:%d", address, serv+2))
+		} else {
+			fmt.Printf("E: too many servers (max. %d)\n", server_MAX)
+		}
+	case "SET":
+		//  When we set a property, we push the new key-value pair onto
+		//  all our connected servers:
+		key := msg[0]
+		value := msg[1]
+		ttl := msg[2]
+
+		//  Send key-value pair on to server
+		kvmsg := kvmsg.NewKvmsg(0)
+		kvmsg.SetKey(key)
+		kvmsg.SetUuid()
+		kvmsg.SetBody(value)
+		kvmsg.SetProp("ttl", ttl)
+		kvmsg.Store(agent.kvmap)
+		kvmsg.Send(agent.publisher)
+	case "GET":
+		key := msg[0]
+		value := ""
+		if kvmsg, ok := agent.kvmap[key]; ok {
+			value, _ = kvmsg.GetBody()
+		}
+		agent.pipe.SendMessage(value)
+	}
+	return
+}
+
+//  The asynchronous agent manages a server pool and handles the
+//  request/reply dialog when the application asks for it:
+
+func clone_agent(pipename string) {
+
+	pipe, _ := zmq.NewSocket(zmq.PAIR)
+	pipe.Connect(pipename)
+
+	agent := agent_new(pipe)
+
+LOOP:
+	for {
+		poller := zmq.NewPoller()
+		poller.Add(pipe, zmq.POLLIN)
+		server := agent.server[agent.cur_server]
+		switch agent.state {
+		case state_INITIAL:
+			//  In this state we ask the server for a snapshot,
+			//  if we have a server to talk to...
+			if agent.nbr_servers > 0 {
+				fmt.Printf("I: waiting for server at %s:%d...\n", server.address, server.port)
+				if server.requests < 2 {
+					server.snapshot.SendMessage("ICANHAZ?", agent.subtree)
+					server.requests++
+				}
+				server.expiry = time.Now().Add(server_TTL)
+				agent.state = state_SYNCING
+				poller.Add(server.snapshot, zmq.POLLIN)
+			}
+
+		case state_SYNCING:
+			//  In this state we read from snapshot and we expect
+			//  the server to respond, else we fail over.
+			poller.Add(server.snapshot, zmq.POLLIN)
+
+		case state_ACTIVE:
+			//  In this state we read from subscriber and we expect
+			//  the server to give hugz, else we fail over.
+			poller.Add(server.subscriber, zmq.POLLIN)
+			break
+		}
+		poll_timer := time.Duration(-1)
+		if server != nil {
+			poll_timer = server.expiry.Sub(time.Now())
+			if poll_timer < 0 {
+				poll_timer = 0
+			}
+		}
+		//  We're ready to process incoming messages; if nothing at all
+		//  comes from our server within the timeout, that means the
+		//  server is dead:
+
+		polled, err := poller.Poll(poll_timer)
+		if err != nil {
+			break
+		}
+
+		if len(polled) > 0 {
+			for _, item := range polled {
+				switch socket := item.Socket; socket {
+				case pipe:
+
+					err = agent.control_message()
+					if err != nil {
+						break LOOP
+					}
+
+				default:
+					kvmsg, e := kvmsg.RecvKvmsg(socket)
+					if e != nil {
+						err = e
+						break LOOP
+					}
+
+					//  Anything from server resets its expiry time
+					server.expiry = time.Now().Add(server_TTL)
+					if agent.state == state_SYNCING {
+						//  Store in snapshot until we're finished
+						server.requests = 0
+						if key, _ := kvmsg.GetKey(); key == "KTHXBAI" {
+							agent.sequence, _ = kvmsg.GetSequence()
+							agent.state = state_ACTIVE
+							fmt.Printf("I: received from %s:%d snapshot=%d\n", server.address, server.port, agent.sequence)
+						} else {
+							kvmsg.Store(agent.kvmap)
+						}
+					} else if agent.state == state_ACTIVE {
+						//  Discard out-of-sequence updates, incl. hugz
+						if seq, _ := kvmsg.GetSequence(); seq > agent.sequence {
+							agent.sequence = seq
+							kvmsg.Store(agent.kvmap)
+							fmt.Printf("I: received from %s:%d update=%d\n", server.address, server.port, agent.sequence)
+						}
+					}
+				}
+			}
+		} else {
+			//  Server has died, failover to next
+			fmt.Printf("I: server at %s:%d didn't give hugz\n", server.address, server.port)
+			agent.cur_server = (agent.cur_server + 1) % agent.nbr_servers
+			agent.state = state_INITIAL
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonecli1.go b/vendor/src/github.com/pebbe/zmq2/examples/clonecli1.go
new file mode 100644
index 0000000..9a3ab3f
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonecli1.go
@@ -0,0 +1,31 @@
+//
+//  Clone client Model One
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+)
+
+func main() {
+	//  Prepare our context and updates socket
+	updates, _ := zmq.NewSocket(zmq.SUB)
+	updates.SetSubscribe("")
+	updates.Connect("tcp://localhost:5556")
+
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+
+	sequence := int64(0)
+	for ; true; sequence++ {
+		kvmsg, err := kvsimple.RecvKvmsg(updates)
+		if err != nil {
+			break //  Interrupted
+		}
+		kvmsg.Store(kvmap)
+	}
+	fmt.Printf("Interrupted\n%d messages in\n", sequence)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonecli2.go b/vendor/src/github.com/pebbe/zmq2/examples/clonecli2.go
new file mode 100644
index 0000000..1486370
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonecli2.go
@@ -0,0 +1,69 @@
+//
+//  Clone client Model Two
+//
+//  In the original C example, the client misses updates between snapshot
+//  and further updates. Sometimes, it even misses the END message of
+//  the snapshot, so it waits for it forever.
+//  This Go implementation has some modifications to improve this, but it
+//  is still not fully reliable.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	snapshot, _ := zmq.NewSocket(zmq.DEALER)
+	snapshot.Connect("tcp://localhost:5556")
+
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	subscriber.SetSubscribe("")
+	subscriber.Connect("tcp://localhost:5557")
+
+	time.Sleep(time.Second) // or messages between snapshot and next are lost
+
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+
+	//  Get state snapshot
+	sequence := int64(0)
+	snapshot.SendMessage("ICANHAZ?")
+	for {
+		kvmsg, err := kvsimple.RecvKvmsg(snapshot)
+		if err != nil {
+			fmt.Println(err)
+			break //  Interrupted
+		}
+		if key, _ := kvmsg.GetKey(); key == "KTHXBAI" {
+			sequence, _ = kvmsg.GetSequence()
+			fmt.Printf("Received snapshot=%d\n", sequence)
+			break //  Done
+		}
+		kvmsg.Store(kvmap)
+	}
+	snapshot.Close()
+
+	first := true
+	//  Now apply pending updates, discard out-of-sequence messages
+	for {
+		kvmsg, err := kvsimple.RecvKvmsg(subscriber)
+		if err != nil {
+			fmt.Println(err)
+			break //  Interrupted
+		}
+		if seq, _ := kvmsg.GetSequence(); seq > sequence {
+			sequence, _ = kvmsg.GetSequence()
+			kvmsg.Store(kvmap)
+			if first {
+				// Show what the first regular update is after the snapshot,
+				// to see if we missed updates.
+				first = false
+				fmt.Println("Next:", sequence)
+			}
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonecli3.go b/vendor/src/github.com/pebbe/zmq2/examples/clonecli3.go
new file mode 100644
index 0000000..3f44b04
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonecli3.go
@@ -0,0 +1,83 @@
+//
+//  Clone client Model Three
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+func main() {
+	snapshot, _ := zmq.NewSocket(zmq.DEALER)
+	snapshot.Connect("tcp://localhost:5556")
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	subscriber.SetSubscribe("")
+	subscriber.Connect("tcp://localhost:5557")
+	publisher, _ := zmq.NewSocket(zmq.PUSH)
+	publisher.Connect("tcp://localhost:5558")
+
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+	rand.Seed(time.Now().UnixNano())
+
+	//  We first request a state snapshot:
+	sequence := int64(0)
+	snapshot.SendMessage("ICANHAZ?")
+	for {
+		kvmsg, err := kvsimple.RecvKvmsg(snapshot)
+		if err != nil {
+			break //  Interrupted
+		}
+		if key, _ := kvmsg.GetKey(); key == "KTHXBAI" {
+			sequence, _ := kvmsg.GetSequence()
+			fmt.Println("I: received snapshot =", sequence)
+			break //  Done
+		}
+		kvmsg.Store(kvmap)
+	}
+	snapshot.Close()
+
+	//  Now we wait for updates from the server, and every so often, we
+	//  send a random key-value update to the server:
+
+	poller := zmq.NewPoller()
+	poller.Add(subscriber, zmq.POLLIN)
+	alarm := time.Now().Add(1000 * time.Millisecond)
+	for {
+		tickless := alarm.Sub(time.Now())
+		if tickless < 0 {
+			tickless = 0
+		}
+		polled, err := poller.Poll(tickless)
+		if err != nil {
+			break //  Context has been shut down
+		}
+		if len(polled) == 1 {
+			kvmsg, err := kvsimple.RecvKvmsg(subscriber)
+			if err != nil {
+				break //  Interrupted
+			}
+
+			//  Discard out-of-sequence kvmsgs, incl. heartbeats
+			if seq, _ := kvmsg.GetSequence(); seq > sequence {
+				sequence = seq
+				kvmsg.Store(kvmap)
+				fmt.Println("I: received update =", sequence)
+			}
+		}
+		//  If we timed-out, generate a random kvmsg
+		if time.Now().After(alarm) {
+			kvmsg := kvsimple.NewKvmsg(0)
+			kvmsg.SetKey(fmt.Sprint(rand.Intn(10000)))
+			kvmsg.SetBody(fmt.Sprint(rand.Intn(1000000)))
+			kvmsg.Send(publisher)
+			alarm = time.Now().Add(1000 * time.Millisecond)
+		}
+	}
+	fmt.Printf("Interrupted\n%d messages in\n", sequence)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonecli4.go b/vendor/src/github.com/pebbe/zmq2/examples/clonecli4.go
new file mode 100644
index 0000000..a0ab768
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonecli4.go
@@ -0,0 +1,84 @@
+//
+//  Clone client Model Four
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+const (
+	SUBTREE = "/client/"
+)
+
+func main() {
+	snapshot, _ := zmq.NewSocket(zmq.DEALER)
+	snapshot.Connect("tcp://localhost:5556")
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	subscriber.SetSubscribe(SUBTREE)
+	subscriber.Connect("tcp://localhost:5557")
+	publisher, _ := zmq.NewSocket(zmq.PUSH)
+	publisher.Connect("tcp://localhost:5558")
+
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+	rand.Seed(time.Now().UnixNano())
+
+	//  We first request a state snapshot:
+	sequence := int64(0)
+	snapshot.SendMessage("ICANHAZ?", SUBTREE)
+	for {
+		kvmsg, err := kvsimple.RecvKvmsg(snapshot)
+		if err != nil {
+			break //  Interrupted
+		}
+		if key, _ := kvmsg.GetKey(); key == "KTHXBAI" {
+			sequence, _ := kvmsg.GetSequence()
+			fmt.Println("I: received snapshot =", sequence)
+			break //  Done
+		}
+		kvmsg.Store(kvmap)
+	}
+	snapshot.Close()
+
+	poller := zmq.NewPoller()
+	poller.Add(subscriber, zmq.POLLIN)
+	alarm := time.Now().Add(1000 * time.Millisecond)
+	for {
+		tickless := alarm.Sub(time.Now())
+		if tickless < 0 {
+			tickless = 0
+		}
+		polled, err := poller.Poll(tickless)
+		if err != nil {
+			break //  Context has been shut down
+		}
+		if len(polled) == 1 {
+			kvmsg, err := kvsimple.RecvKvmsg(subscriber)
+			if err != nil {
+				break //  Interrupted
+			}
+
+			//  Discard out-of-sequence kvmsgs, incl. heartbeats
+			if seq, _ := kvmsg.GetSequence(); seq > sequence {
+				sequence = seq
+				kvmsg.Store(kvmap)
+				fmt.Println("I: received update =", sequence)
+			}
+		}
+		//  If we timed-out, generate a random kvmsg
+		if time.Now().After(alarm) {
+			kvmsg := kvsimple.NewKvmsg(0)
+			kvmsg.SetKey(fmt.Sprintf("%s%d", SUBTREE, rand.Intn(10000)))
+			kvmsg.SetBody(fmt.Sprint(rand.Intn(1000000)))
+			kvmsg.Send(publisher)
+			alarm = time.Now().Add(1000 * time.Millisecond)
+		}
+	}
+	fmt.Printf("Interrupted\n%d messages in\n", sequence)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonecli5.go b/vendor/src/github.com/pebbe/zmq2/examples/clonecli5.go
new file mode 100644
index 0000000..3f6fe5c
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonecli5.go
@@ -0,0 +1,85 @@
+//
+//  Clone client Model Five
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvmsg"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+const (
+	SUBTREE = "/client/"
+)
+
+func main() {
+	snapshot, _ := zmq.NewSocket(zmq.DEALER)
+	snapshot.Connect("tcp://localhost:5556")
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	subscriber.SetSubscribe(SUBTREE)
+	subscriber.Connect("tcp://localhost:5557")
+	publisher, _ := zmq.NewSocket(zmq.PUSH)
+	publisher.Connect("tcp://localhost:5558")
+
+	kvmap := make(map[string]*kvmsg.Kvmsg)
+	rand.Seed(time.Now().UnixNano())
+
+	//  We first request a state snapshot:
+	sequence := int64(0)
+	snapshot.SendMessage("ICANHAZ?", SUBTREE)
+	for {
+		kvmsg, err := kvmsg.RecvKvmsg(snapshot)
+		if err != nil {
+			break //  Interrupted
+		}
+		if key, _ := kvmsg.GetKey(); key == "KTHXBAI" {
+			sequence, _ := kvmsg.GetSequence()
+			fmt.Println("I: received snapshot =", sequence)
+			break //  Done
+		}
+		kvmsg.Store(kvmap)
+	}
+	snapshot.Close()
+
+	poller := zmq.NewPoller()
+	poller.Add(subscriber, zmq.POLLIN)
+	alarm := time.Now().Add(1000 * time.Millisecond)
+	for {
+		tickless := alarm.Sub(time.Now())
+		if tickless < 0 {
+			tickless = 0
+		}
+		polled, err := poller.Poll(tickless)
+		if err != nil {
+			break //  Context has been shut down
+		}
+		if len(polled) == 1 {
+			kvmsg, err := kvmsg.RecvKvmsg(subscriber)
+			if err != nil {
+				break //  Interrupted
+			}
+
+			//  Discard out-of-sequence kvmsgs, incl. heartbeats
+			if seq, _ := kvmsg.GetSequence(); seq > sequence {
+				sequence = seq
+				kvmsg.Store(kvmap)
+				fmt.Println("I: received update =", sequence)
+			}
+		}
+		//  If we timed-out, generate a random kvmsg
+		if time.Now().After(alarm) {
+			kvmsg := kvmsg.NewKvmsg(0)
+			kvmsg.SetKey(fmt.Sprintf("%s%d", SUBTREE, rand.Intn(10000)))
+			kvmsg.SetBody(fmt.Sprint(rand.Intn(1000000)))
+			kvmsg.SetProp("ttl", fmt.Sprintf("%d", rand.Intn((30)))) // seconds
+			kvmsg.Send(publisher)
+			alarm = time.Now().Add(1000 * time.Millisecond)
+		}
+	}
+	fmt.Printf("Interrupted\n%d messages in\n", sequence)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonecli6.go b/vendor/src/github.com/pebbe/zmq2/examples/clonecli6.go
new file mode 100644
index 0000000..21c9723
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonecli6.go
@@ -0,0 +1,41 @@
+//
+//  Clone client Model Six
+//
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/clone"
+
+	"fmt"
+	"log"
+	"math/rand"
+	"time"
+)
+
+const (
+	SUBTREE = "/client/"
+)
+
+func main() {
+	//  Create distributed hash instance
+	clone := clone.New()
+
+	//  Specify configuration
+	clone.Subtree(SUBTREE)
+	clone.Connect("tcp://localhost", "5556")
+	clone.Connect("tcp://localhost", "5566")
+
+	//  Set random tuples into the distributed hash
+	for {
+		//  Set random value, check it was stored
+		key := fmt.Sprintf("%s%d", SUBTREE, rand.Intn(10000))
+		value := fmt.Sprint(rand.Intn(1000000))
+		clone.Set(key, value, rand.Intn(30))
+		v, _ := clone.Get(key)
+		if v != value {
+			log.Fatalf("Set: %v - Get: %v - Equal: %v\n", value, v, value == v)
+		}
+		time.Sleep(time.Second)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonesrv1.go b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv1.go
new file mode 100644
index 0000000..9428c1b
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv1.go
@@ -0,0 +1,38 @@
+//
+//  Clone server Model One
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+func main() {
+	//  Prepare our context and publisher socket
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	publisher.Bind("tcp://*:5556")
+	time.Sleep(200 * time.Millisecond)
+
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+	rand.Seed(time.Now().UnixNano())
+
+	sequence := int64(1)
+	for ; true; sequence++ {
+		//  Distribute as key-value message
+		kvmsg := kvsimple.NewKvmsg(sequence)
+		kvmsg.SetKey(fmt.Sprint(rand.Intn(10000)))
+		kvmsg.SetBody(fmt.Sprint(rand.Intn(1000000)))
+		err := kvmsg.Send(publisher)
+		kvmsg.Store(kvmap)
+		if err != nil {
+			break
+		}
+	}
+	fmt.Printf("Interrupted\n%d messages out\n", sequence)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonesrv2.go b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv2.go
new file mode 100644
index 0000000..ae87d40
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv2.go
@@ -0,0 +1,119 @@
+//
+//  Clone server Model Two
+//
+//  In the original C example, the client misses updates between snapshot
+//  and further updates. Sometimes, it even misses the END message of
+//  the snapshot, so it waits for it forever.
+//  This Go implementation has some modifications to improve this, but it
+//  is still not fully reliable.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+func main() {
+	//  Prepare our context and sockets
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	publisher.Bind("tcp://*:5557")
+
+	sequence := int64(0)
+	rand.Seed(time.Now().UnixNano())
+
+	//  Start state manager and wait for synchronization signal
+	updates, _ := zmq.NewSocket(zmq.PAIR)
+	updates.Bind("inproc://pipe")
+	go state_manager()
+	updates.RecvMessage(0) // "READY"
+
+	for {
+		//  Distribute as key-value message
+		sequence++
+		kvmsg := kvsimple.NewKvmsg(sequence)
+		kvmsg.SetKey(fmt.Sprint(rand.Intn(10000)))
+		kvmsg.SetBody(fmt.Sprint(rand.Intn(1000000)))
+		if kvmsg.Send(publisher) != nil {
+			break
+		}
+		if kvmsg.Send(updates) != nil {
+			break
+		}
+	}
+	fmt.Printf("Interrupted\n%d messages out\n", sequence)
+}
+
+//  The state manager task maintains the state and handles requests from
+//  clients for snapshots:
+
+func state_manager() {
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+
+	pipe, _ := zmq.NewSocket(zmq.PAIR)
+	pipe.Connect("inproc://pipe")
+	pipe.SendMessage("READY")
+	snapshot, _ := zmq.NewSocket(zmq.ROUTER)
+	snapshot.Bind("tcp://*:5556")
+
+	poller := zmq.NewPoller()
+	poller.Add(pipe, zmq.POLLIN)
+	poller.Add(snapshot, zmq.POLLIN)
+	sequence := int64(0) //  Current snapshot version number
+LOOP:
+	for {
+		polled, err := poller.Poll(-1)
+		if err != nil {
+			break //  Context has been shut down
+		}
+		for _, item := range polled {
+			switch socket := item.Socket; socket {
+			case pipe:
+				//  Apply state update from main thread
+				kvmsg, err := kvsimple.RecvKvmsg(pipe)
+				if err != nil {
+					break LOOP //  Interrupted
+				}
+				sequence, _ = kvmsg.GetSequence()
+				kvmsg.Store(kvmap)
+			case snapshot:
+				//  Execute state snapshot request
+				msg, err := snapshot.RecvMessage(0)
+				if err != nil {
+					break LOOP //  Interrupted
+				}
+				identity := msg[0]
+				//  Request is in second frame of message
+				request := msg[1]
+				if request != "ICANHAZ?" {
+					fmt.Println("E: bad request, aborting")
+					break LOOP
+				}
+				//  Send state snapshot to client
+
+				//  For each entry in kvmap, send kvmsg to client
+				for _, kvmsg := range kvmap {
+					snapshot.Send(identity, zmq.SNDMORE)
+					kvmsg.Send(snapshot)
+				}
+
+				// Give client some time to deal with it.
+				// This reduces the risk that the client won't see
+				// the END message, but it doesn't eliminate the risk.
+				time.Sleep(100 * time.Millisecond)
+
+				//  Now send END message with sequence number
+				fmt.Printf("Sending state shapshot=%d\n", sequence)
+				snapshot.Send(identity, zmq.SNDMORE)
+				kvmsg := kvsimple.NewKvmsg(sequence)
+				kvmsg.SetKey("KTHXBAI")
+				kvmsg.SetBody("")
+				kvmsg.Send(snapshot)
+			}
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonesrv3.go b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv3.go
new file mode 100644
index 0000000..4242013
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv3.go
@@ -0,0 +1,84 @@
+//
+//  Clone server Model Three
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	snapshot, _ := zmq.NewSocket(zmq.ROUTER)
+	snapshot.Bind("tcp://*:5556")
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	publisher.Bind("tcp://*:5557")
+	collector, _ := zmq.NewSocket(zmq.PULL)
+	collector.Bind("tcp://*:5558")
+
+	//  The body of the main task collects updates from clients and
+	//  publishes them back out to clients:
+
+	sequence := int64(0)
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+
+	poller := zmq.NewPoller()
+	poller.Add(collector, zmq.POLLIN)
+	poller.Add(snapshot, zmq.POLLIN)
+LOOP:
+	for {
+		polled, err := poller.Poll(1000 * time.Millisecond)
+		if err != nil {
+			break
+		}
+		for _, item := range polled {
+			switch socket := item.Socket; socket {
+			case collector:
+				//  Apply state update sent from client
+				kvmsg, err := kvsimple.RecvKvmsg(collector)
+				if err != nil {
+					break LOOP //  Interrupted
+				}
+				sequence++
+				kvmsg.SetSequence(sequence)
+				kvmsg.Send(publisher)
+				kvmsg.Store(kvmap)
+				fmt.Println("I: publishing update", sequence)
+			case snapshot:
+				//  Execute state snapshot request
+				msg, err := snapshot.RecvMessage(0)
+				if err != nil {
+					break LOOP
+				}
+				identity := msg[0]
+
+				//  Request is in second frame of message
+				request := msg[1]
+				if request != "ICANHAZ?" {
+					fmt.Println("E: bad request, aborting")
+					break LOOP
+				}
+				//  Send state snapshot to client
+
+				//  For each entry in kvmap, send kvmsg to client
+				for _, kvmsg := range kvmap {
+					snapshot.Send(identity, zmq.SNDMORE)
+					kvmsg.Send(snapshot)
+				}
+
+				//  Now send END message with sequence number
+				fmt.Println("I: sending shapshot =", sequence)
+				snapshot.Send(identity, zmq.SNDMORE)
+				kvmsg := kvsimple.NewKvmsg(sequence)
+				kvmsg.SetKey("KTHXBAI")
+				kvmsg.SetBody("")
+				kvmsg.Send(snapshot)
+			}
+		}
+	}
+	fmt.Printf("Interrupted\n%d messages handled\n", sequence)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonesrv4.go b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv4.go
new file mode 100644
index 0000000..1098cbb
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv4.go
@@ -0,0 +1,91 @@
+//
+//  Clone server Model Four
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvsimple"
+
+	"fmt"
+	"strings"
+	"time"
+)
+
+//  The main task is identical to clonesrv3 except for where it
+//  handles subtrees.
+
+func main() {
+	snapshot, _ := zmq.NewSocket(zmq.ROUTER)
+	snapshot.Bind("tcp://*:5556")
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	publisher.Bind("tcp://*:5557")
+	collector, _ := zmq.NewSocket(zmq.PULL)
+	collector.Bind("tcp://*:5558")
+
+	//  The body of the main task collects updates from clients and
+	//  publishes them back out to clients:
+
+	sequence := int64(0)
+	kvmap := make(map[string]*kvsimple.Kvmsg)
+
+	poller := zmq.NewPoller()
+	poller.Add(collector, zmq.POLLIN)
+	poller.Add(snapshot, zmq.POLLIN)
+LOOP:
+	for {
+		polled, err := poller.Poll(1000 * time.Millisecond)
+		if err != nil {
+			break
+		}
+		for _, item := range polled {
+			switch socket := item.Socket; socket {
+			case collector:
+				//  Apply state update sent from client
+				kvmsg, err := kvsimple.RecvKvmsg(collector)
+				if err != nil {
+					break LOOP //  Interrupted
+				}
+				sequence++
+				kvmsg.SetSequence(sequence)
+				kvmsg.Send(publisher)
+				kvmsg.Store(kvmap)
+				fmt.Println("I: publishing update", sequence)
+			case snapshot:
+				//  Execute state snapshot request
+				msg, err := snapshot.RecvMessage(0)
+				if err != nil {
+					break LOOP
+				}
+				identity := msg[0]
+
+				//  Request is in second frame of message
+				request := msg[1]
+				if request != "ICANHAZ?" {
+					fmt.Println("E: bad request, aborting")
+					break LOOP
+				}
+				subtree := msg[2]
+				//  Send state snapshot to client
+
+				//  For each entry in kvmap, send kvmsg to client
+				for _, kvmsg := range kvmap {
+					if key, _ := kvmsg.GetKey(); strings.HasPrefix(key, subtree) {
+						snapshot.Send(identity, zmq.SNDMORE)
+						kvmsg.Send(snapshot)
+					}
+				}
+
+				//  Now send END message with sequence number
+				fmt.Println("I: sending shapshot =", sequence)
+				snapshot.Send(identity, zmq.SNDMORE)
+				kvmsg := kvsimple.NewKvmsg(sequence)
+				kvmsg.SetKey("KTHXBAI")
+				kvmsg.SetBody(subtree)
+				kvmsg.Send(snapshot)
+			}
+		}
+	}
+	fmt.Printf("Interrupted\n%d messages handled\n", sequence)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonesrv5.go b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv5.go
new file mode 100644
index 0000000..2b41f5a
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv5.go
@@ -0,0 +1,152 @@
+//
+//  Clone server Model Five
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/kvmsg"
+
+	"errors"
+	"fmt"
+	"log"
+	"strconv"
+	"strings"
+	"time"
+)
+
+//  Our server is defined by these properties
+type clonesrv_t struct {
+	kvmap     map[string]*kvmsg.Kvmsg //  Key-value store
+	port      int                     //  Main port we're working on
+	sequence  int64                   //  How many updates we're at
+	snapshot  *zmq.Socket             //  Handle snapshot requests
+	publisher *zmq.Socket             //  Publish updates to clients
+	collector *zmq.Socket             //  Collect updates from clients
+}
+
+func main() {
+
+	srv := &clonesrv_t{
+		port:  5556,
+		kvmap: make(map[string]*kvmsg.Kvmsg),
+	}
+
+	//  Set up our clone server sockets
+	srv.snapshot, _ = zmq.NewSocket(zmq.ROUTER)
+	srv.snapshot.Bind(fmt.Sprint("tcp://*:", srv.port))
+	srv.publisher, _ = zmq.NewSocket(zmq.PUB)
+	srv.publisher.Bind(fmt.Sprint("tcp://*:", srv.port+1))
+	srv.collector, _ = zmq.NewSocket(zmq.PULL)
+	srv.collector.Bind(fmt.Sprint("tcp://*:", srv.port+2))
+
+	//  Register our handlers with reactor
+	reactor := zmq.NewReactor()
+	reactor.AddSocket(srv.snapshot, zmq.POLLIN,
+		func(e zmq.State) error { return snapshots(srv) })
+	reactor.AddSocket(srv.collector, zmq.POLLIN,
+		func(e zmq.State) error { return collector(srv) })
+	reactor.AddChannelTime(time.Tick(1000*time.Millisecond), 1,
+		func(v interface{}) error { return flush_ttl(srv) })
+
+	log.Println(reactor.Run(100 * time.Millisecond)) // precision: .1 seconds
+}
+
+//  This is the reactor handler for the snapshot socket; it accepts
+//  just the ICANHAZ? request and replies with a state snapshot ending
+//  with a KTHXBAI message:
+
+func snapshots(srv *clonesrv_t) (err error) {
+
+	msg, err := srv.snapshot.RecvMessage(0)
+	if err != nil {
+		return
+	}
+	identity := msg[0]
+
+	//  Request is in second frame of message
+	request := msg[1]
+	if request != "ICANHAZ?" {
+		err = errors.New("E: bad request, aborting")
+		return
+	}
+	subtree := msg[2]
+
+	//  Send state socket to client
+	for _, kvmsg := range srv.kvmap {
+		if key, _ := kvmsg.GetKey(); strings.HasPrefix(key, subtree) {
+			srv.snapshot.Send(identity, zmq.SNDMORE)
+			kvmsg.Send(srv.snapshot)
+		}
+	}
+
+	//  Now send END message with sequence number
+	log.Println("I: sending shapshot =", srv.sequence)
+	srv.snapshot.Send(identity, zmq.SNDMORE)
+	kvmsg := kvmsg.NewKvmsg(srv.sequence)
+	kvmsg.SetKey("KTHXBAI")
+	kvmsg.SetBody(subtree)
+	kvmsg.Send(srv.snapshot)
+
+	return
+}
+
+//  We store each update with a new sequence number, and if necessary, a
+//  time-to-live. We publish updates immediately on our publisher socket:
+
+func collector(srv *clonesrv_t) (err error) {
+	kvmsg, err := kvmsg.RecvKvmsg(srv.collector)
+	if err != nil {
+		return
+	}
+
+	srv.sequence++
+	kvmsg.SetSequence(srv.sequence)
+	kvmsg.Send(srv.publisher)
+	if ttls, e := kvmsg.GetProp("ttl"); e == nil {
+		// change duration into specific time, using the same property: ugly!
+		ttl, e := strconv.ParseInt(ttls, 10, 64)
+		if e != nil {
+			err = e
+			return
+		}
+		kvmsg.SetProp("ttl", fmt.Sprint(time.Now().Add(time.Duration(ttl)*time.Second).Unix()))
+	}
+	kvmsg.Store(srv.kvmap)
+	log.Println("I: publishing update =", srv.sequence)
+
+	return
+}
+
+//  At regular intervals we flush ephemeral values that have expired. This
+//  could be slow on very large data sets:
+
+func flush_ttl(srv *clonesrv_t) (err error) {
+
+	for _, kvmsg := range srv.kvmap {
+
+		//  If key-value pair has expired, delete it and publish the
+		//  fact to listening clients.
+
+		if ttls, e := kvmsg.GetProp("ttl"); e == nil {
+			ttl, e := strconv.ParseInt(ttls, 10, 64)
+			if e != nil {
+				err = e
+				continue
+			}
+			if time.Now().After(time.Unix(ttl, 0)) {
+				srv.sequence++
+				kvmsg.SetSequence(srv.sequence)
+				kvmsg.SetBody("")
+				e = kvmsg.Send(srv.publisher)
+				if e != nil {
+					err = e
+				}
+				kvmsg.Store(srv.kvmap)
+				log.Println("I: publishing delete =", srv.sequence)
+			}
+		}
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/clonesrv6.go b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv6.go
new file mode 100644
index 0000000..b3a0f3c
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/clonesrv6.go
@@ -0,0 +1,336 @@
+//
+//  Clone server Model Six
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/bstar"
+	"github.com/pebbe/zmq2/examples/kvmsg"
+
+	"errors"
+	"fmt"
+	"log"
+	"os"
+	"strconv"
+	"strings"
+	"time"
+)
+
+//  Our server is defined by these properties
+type clonesrv_t struct {
+	kvmap      map[string]*kvmsg.Kvmsg //  Key-value store
+	kvmap_init bool
+	bstar      *bstar.Bstar   //  Bstar reactor core
+	sequence   int64          //  How many updates we're at
+	port       int            //  Main port we're working on
+	peer       int            //  Main port of our peer
+	publisher  *zmq.Socket    //  Publish updates and hugz
+	collector  *zmq.Socket    //  Collect updates from clients
+	subscriber *zmq.Socket    //  Get updates from peer
+	pending    []*kvmsg.Kvmsg //  Pending updates from clients
+	primary    bool           //  TRUE if we're primary
+	active     bool           //  TRUE if we're active
+	passive    bool           //  TRUE if we're passive
+}
+
+//  The main task parses the command line to decide whether to start
+//  as primary or backup server. We're using the Binary Star pattern
+//  for reliability. This interconnects the two servers so they can
+//  agree on which is primary, and which is backup. To allow the two
+//  servers to run on the same box, we use different ports for primary
+//  and backup. Ports 5003/5004 are used to interconnect the servers.
+//  Ports 5556/5566 are used to receive voting events (snapshot requests
+//  in the clone pattern). Ports 5557/5567 are used by the publisher,
+//  and ports 5558/5568 by the collector:
+
+func main() {
+	var err error
+
+	srv := &clonesrv_t{}
+
+	if len(os.Args) == 2 && os.Args[1] == "-p" {
+		log.Println("I: primary active, waiting for backup (passive)")
+		srv.bstar, err = bstar.New(bstar.PRIMARY, "tcp://*:5003", "tcp://localhost:5004")
+		if err != nil {
+			log.Println(err)
+			return
+		}
+		srv.bstar.Voter("tcp://*:5556", zmq.ROUTER, func(soc *zmq.Socket) error { return snapshots(soc, srv) })
+		srv.port = 5556
+		srv.peer = 5566
+		srv.primary = true
+	} else if len(os.Args) == 2 && os.Args[1] == "-b" {
+		log.Println("I: backup passive, waiting for primary (active)")
+		srv.bstar, err = bstar.New(bstar.BACKUP, "tcp://*:5004", "tcp://localhost:5003")
+		srv.bstar.Voter("tcp://*:5566", zmq.ROUTER, func(soc *zmq.Socket) error { return snapshots(soc, srv) })
+		srv.port = 5566
+		srv.peer = 5556
+		srv.primary = false
+	} else {
+		fmt.Println("Usage: clonesrv4 { -p | -b }")
+		return
+	}
+	//  Primary server will become first active
+	if srv.primary {
+		srv.kvmap = make(map[string]*kvmsg.Kvmsg, 0)
+		srv.kvmap_init = true
+	}
+
+	srv.pending = make([]*kvmsg.Kvmsg, 0)
+	srv.bstar.SetVerbose(true)
+
+	//  Set up our clone server sockets
+	srv.publisher, _ = zmq.NewSocket(zmq.PUB)
+	srv.collector, _ = zmq.NewSocket(zmq.SUB)
+	srv.collector.SetSubscribe("")
+	srv.publisher.Bind(fmt.Sprint("tcp://*:", srv.port+1))
+	srv.collector.Bind(fmt.Sprint("tcp://*:", srv.port+2))
+
+	//  Set up our own clone client interface to peer
+	srv.subscriber, _ = zmq.NewSocket(zmq.SUB)
+	srv.subscriber.SetSubscribe("")
+	srv.subscriber.Connect(fmt.Sprint("tcp://localhost:", srv.peer+1))
+
+	//  After we've set-up our sockets we register our binary star
+	//  event handlers, and then start the bstar reactor. This finishes
+	//  when the user presses Ctrl-C, or the process receives a SIGINT
+	//  interrupt:
+
+	//  Register state change handlers
+	srv.bstar.NewActive(func() error { return new_active(srv) })
+	srv.bstar.NewPassive(func() error { return new_passive(srv) })
+
+	//  Register our other handlers with the bstar reactor
+	srv.bstar.Reactor.AddSocket(srv.collector, zmq.POLLIN,
+		func(e zmq.State) error { return collector(srv) })
+	srv.bstar.Reactor.AddChannelTime(time.Tick(1000*time.Millisecond), 1,
+		func(i interface{}) error {
+			if e := flush_ttl(srv); e != nil {
+				return e
+			}
+			return send_hugz(srv)
+		})
+
+	err = srv.bstar.Start()
+	log.Println(err)
+}
+
+func snapshots(socket *zmq.Socket, srv *clonesrv_t) (err error) {
+
+	msg, err := socket.RecvMessage(0)
+	if err != nil {
+		return
+	}
+	identity := msg[0]
+
+	//  Request is in second frame of message
+	request := msg[1]
+	if request != "ICANHAZ?" {
+		err = errors.New("E: bad request, aborting")
+		return
+	}
+	subtree := msg[2]
+
+	//  Send state socket to client
+	for _, kvmsg := range srv.kvmap {
+		if key, _ := kvmsg.GetKey(); strings.HasPrefix(key, subtree) {
+			socket.Send(identity, zmq.SNDMORE)
+			kvmsg.Send(socket)
+		}
+	}
+
+	//  Now send END message with sequence number
+	log.Println("I: sending shapshot =", srv.sequence)
+	socket.Send(identity, zmq.SNDMORE)
+	kvmsg := kvmsg.NewKvmsg(srv.sequence)
+	kvmsg.SetKey("KTHXBAI")
+	kvmsg.SetBody(subtree)
+	kvmsg.Send(socket)
+
+	return
+}
+
+//  The collector is more complex than in the clonesrv5 example since how
+//  process updates depends on whether we're active or passive. The active
+//  applies them immediately to its kvmap, whereas the passive queues them
+//  as pending:
+
+//  If message was already on pending list, remove it and return TRUE,
+//  else return FALSE.
+
+func (srv *clonesrv_t) was_pending(kvmsg *kvmsg.Kvmsg) bool {
+	uuid1, _ := kvmsg.GetUuid()
+	for i, msg := range srv.pending {
+		if uuid2, _ := msg.GetUuid(); uuid1 == uuid2 {
+			srv.pending = append(srv.pending[:i], srv.pending[i+1:]...)
+			return true
+		}
+	}
+	return false
+}
+
+func collector(srv *clonesrv_t) (err error) {
+
+	kvmsg, err := kvmsg.RecvKvmsg(srv.collector)
+	if err != nil {
+		return
+	}
+
+	if srv.active {
+		srv.sequence++
+		kvmsg.SetSequence(srv.sequence)
+		kvmsg.Send(srv.publisher)
+		if ttls, e := kvmsg.GetProp("ttl"); e == nil {
+			ttl, e := strconv.ParseInt(ttls, 10, 64)
+			if e != nil {
+				err = e
+				return
+			}
+			kvmsg.SetProp("ttl", fmt.Sprint(time.Now().Add(time.Duration(ttl)*time.Second).Unix()))
+		}
+		kvmsg.Store(srv.kvmap)
+		log.Println("I: publishing update =", srv.sequence)
+	} else {
+		//  If we already got message from active, drop it, else
+		//  hold on pending list
+		if !srv.was_pending(kvmsg) {
+			srv.pending = append(srv.pending, kvmsg)
+		}
+	}
+	return
+}
+
+//  We purge ephemeral values using exactly the same code as in
+//  the previous clonesrv5 example.
+//  If key-value pair has expired, delete it and publish the
+//  fact to listening clients.
+
+func flush_ttl(srv *clonesrv_t) (err error) {
+	for _, kvmsg := range srv.kvmap {
+
+		//  If key-value pair has expired, delete it and publish the
+		//  fact to listening clients.
+
+		if ttls, e := kvmsg.GetProp("ttl"); e == nil {
+			ttl, e := strconv.ParseInt(ttls, 10, 64)
+			if e != nil {
+				err = e
+				continue
+			}
+			if time.Now().After(time.Unix(ttl, 0)) {
+				srv.sequence++
+				kvmsg.SetSequence(srv.sequence)
+				kvmsg.SetBody("")
+				e = kvmsg.Send(srv.publisher)
+				if e != nil {
+					err = e
+				}
+				kvmsg.Store(srv.kvmap)
+				log.Println("I: publishing delete =", srv.sequence)
+			}
+		}
+	}
+	return
+}
+
+// //  We send a HUGZ message once a second to all subscribers so that they
+// //  can detect if our server dies. They'll then switch over to the backup
+// //  server, which will become active:
+
+func send_hugz(srv *clonesrv_t) (err error) {
+
+	kvmsg := kvmsg.NewKvmsg(srv.sequence)
+	kvmsg.SetKey("HUGZ")
+	kvmsg.SetBody("")
+	err = kvmsg.Send(srv.publisher)
+	return
+}
+
+//  When we switch from passive to active, we apply our pending list so that
+//  our kvmap is up-to-date. When we switch to passive, we wipe our kvmap
+//  and grab a new snapshot from the active:
+
+func new_active(srv *clonesrv_t) (err error) {
+
+	srv.active = true
+	srv.passive = false
+
+	//  Stop subscribing to updates
+	srv.bstar.Reactor.RemoveSocket(srv.subscriber)
+
+	//  Apply pending list to own hash table
+	for _, msg := range srv.pending {
+		srv.sequence++
+		msg.SetSequence(srv.sequence)
+		msg.Send(srv.publisher)
+		msg.Store(srv.kvmap)
+		fmt.Println("I: publishing pending =", srv.sequence)
+	}
+	srv.pending = srv.pending[0:0]
+	return
+}
+
+func new_passive(srv *clonesrv_t) (err error) {
+
+	srv.kvmap = make(map[string]*kvmsg.Kvmsg)
+	srv.kvmap_init = false
+	srv.active = false
+	srv.passive = true
+
+	//  Start subscribing to updates
+	srv.bstar.Reactor.AddSocket(srv.subscriber, zmq.POLLIN,
+		func(e zmq.State) error { return subscriber(srv) })
+
+	return
+}
+
+//  When we get an update, we create a new kvmap if necessary, and then
+//  add our update to our kvmap. We're always passive in this case:
+
+func subscriber(srv *clonesrv_t) (err error) {
+	//  Get state snapshot if necessary
+	if !srv.kvmap_init {
+		srv.kvmap_init = true
+		snapshot, _ := zmq.NewSocket(zmq.DEALER)
+		snapshot.Connect(fmt.Sprint("tcp://localhost:", srv.peer))
+		fmt.Printf("I: asking for snapshot from: tcp://localhost:%v\n", srv.peer)
+		snapshot.SendMessage("ICANHAZ?", "") // blank subtree to get all
+		for {
+			kvmsg, e := kvmsg.RecvKvmsg(snapshot)
+			if e != nil {
+				err = e
+				break
+			}
+			if key, _ := kvmsg.GetKey(); key == "KTHXBAI" {
+				srv.sequence, _ = kvmsg.GetSequence()
+				break //  Done
+			}
+			kvmsg.Store(srv.kvmap)
+		}
+		fmt.Println("I: received snapshot =", srv.sequence)
+	}
+	//  Find and remove update off pending list
+	kvmsg, e := kvmsg.RecvKvmsg(srv.subscriber)
+	if e != nil {
+		err = e
+		return
+	}
+
+	if key, _ := kvmsg.GetKey(); key != "HUGZ" {
+		if !srv.was_pending(kvmsg) {
+			//  If active update came before client update, flip it
+			//  around, store active update (with sequence) on pending
+			//  list and use to clear client update when it comes later
+			srv.pending = append(srv.pending, kvmsg)
+		}
+		//  If update is more recent than our kvmap, apply it
+		if seq, _ := kvmsg.GetSequence(); seq > srv.sequence {
+			srv.sequence = seq
+			kvmsg.Store(srv.kvmap)
+			fmt.Println("I: received update =", srv.sequence)
+		}
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/eagain.go b/vendor/src/github.com/pebbe/zmq2/examples/eagain.go
new file mode 100644
index 0000000..3997512
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/eagain.go
@@ -0,0 +1,28 @@
+//
+//  Shows how to provoke EAGAIN when reaching HWM
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+)
+
+func main() {
+
+	mailbox, _ := zmq.NewSocket(zmq.DEALER)
+	mailbox.SetHwm(4)
+	mailbox.SetSndtimeo(0)
+	mailbox.Connect("tcp://localhost:9876")
+
+	for count := 0; count < 10; count++ {
+		fmt.Println("Sending message", count)
+		_, err := mailbox.SendMessage(fmt.Sprint("message ", count))
+		if err != nil {
+			fmt.Println(err)
+			break
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/espresso.go b/vendor/src/github.com/pebbe/zmq2/examples/espresso.go
new file mode 100644
index 0000000..6b3f1f7
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/espresso.go
@@ -0,0 +1,89 @@
+//
+//  Espresso Pattern
+//  This shows how to capture data using a pub-sub proxy
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+//  The subscriber thread requests messages starting with
+//  A and B, then reads and counts incoming messages.
+
+func subscriber_thread() {
+	//  Subscribe to "A" and "B"
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	subscriber.Connect("tcp://localhost:6001")
+	subscriber.SetSubscribe("A")
+	subscriber.SetSubscribe("B")
+	defer subscriber.Close() // cancel subscribe
+
+	for count := 0; count < 5; count++ {
+		_, err := subscriber.RecvMessage(0)
+		if err != nil {
+			break //  Interrupted
+		}
+	}
+}
+
+//  The publisher sends random messages starting with A-J:
+
+func publisher_thread() {
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	publisher.Bind("tcp://*:6000")
+
+	for {
+		s := fmt.Sprintf("%c-%05d", rand.Intn(10)+'A', rand.Intn(100000))
+		_, err := publisher.SendMessage(s)
+		if err != nil {
+			break //  Interrupted
+		}
+		time.Sleep(100 * time.Millisecond) //  Wait for 1/10th second
+	}
+}
+
+//  The listener receives all messages flowing through the proxy, on its
+//  pipe. In CZMQ, the pipe is a pair of ZMQ_PAIR sockets that connects
+//  attached child threads. In other languages your mileage may vary:
+
+func listener_thread() {
+	pipe, _ := zmq.NewSocket(zmq.PAIR)
+	pipe.Bind("inproc://pipe")
+
+	//  Print everything that arrives on pipe
+	for {
+		msg, err := pipe.RecvMessage(0)
+		if err != nil {
+			break //  Interrupted
+		}
+		fmt.Printf("%q\n", msg)
+	}
+}
+
+//  The main task starts the subscriber and publisher, and then sets
+//  itself up as a listening proxy. The listener runs as a child thread:
+
+func main() {
+	//  Start child threads
+	go publisher_thread()
+	go subscriber_thread()
+	go listener_thread()
+
+	time.Sleep(100 * time.Millisecond)
+
+	subscriber, _ := zmq.NewSocket(zmq.XSUB)
+	subscriber.Connect("tcp://localhost:6000")
+	publisher, _ := zmq.NewSocket(zmq.XPUB)
+	publisher.Bind("tcp://*:6001")
+	listener, _ := zmq.NewSocket(zmq.PAIR)
+	listener.Connect("inproc://pipe")
+	zmq.Proxy(subscriber, publisher, listener)
+
+	fmt.Println("interrupted")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/fileio1.go b/vendor/src/github.com/pebbe/zmq2/examples/fileio1.go
new file mode 100644
index 0000000..30b4c90
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/fileio1.go
@@ -0,0 +1,97 @@
+//  File Transfer model #1
+//
+//  In which the server sends the entire file to the client in
+//  large chunks with no attempt at flow control.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"io"
+	"os"
+)
+
+const (
+	CHUNK_SIZE = 250000
+)
+
+func client_thread(pipe chan<- string) {
+	dealer, _ := zmq.NewSocket(zmq.DEALER)
+	dealer.Connect("tcp://127.0.0.1:6000")
+
+	dealer.Send("fetch", 0)
+	total := 0  //  Total bytes received
+	chunks := 0 //  Total chunks received
+
+	for {
+		frame, err := dealer.RecvBytes(0)
+		if err != nil {
+			break //  Shutting down, quit
+		}
+		chunks++
+		size := len(frame)
+		total += size
+		if size == 0 {
+			break //  Whole file received
+		}
+	}
+	fmt.Printf("%v chunks received, %v bytes\n", chunks, total)
+	pipe <- "OK"
+}
+
+//  The server thread reads the file from disk in chunks, and sends
+//  each chunk to the client as a separate message. We only have one
+//  test file, so open that once and then serve it out as needed:
+
+func server_thread() {
+	file, err := os.Open("testdata")
+	if err != nil {
+		panic(err)
+	}
+
+	router, _ := zmq.NewSocket(zmq.ROUTER)
+	//  Default HWM is 1000, which will drop messages here
+	//  since we send more than 1,000 chunks of test data,
+	//  so set an infinite HWM as a simple, stupid solution:
+	router.SetHwm(0)
+	router.Bind("tcp://*:6000")
+	for {
+		//  First frame in each message is the sender identity
+		identity, err := router.Recv(0)
+		if err != nil {
+			break //  Shutting down, quit
+		}
+
+		//  Second frame is "fetch" command
+		command, _ := router.Recv(0)
+		if command != "fetch" {
+			panic("command != \"fetch\"")
+		}
+
+		chunk := make([]byte, CHUNK_SIZE)
+		for {
+			n, _ := io.ReadFull(file, chunk)
+			router.SendMessage(identity, chunk[:n])
+			if n == 0 {
+				break //  Always end with a zero-size frame
+			}
+		}
+	}
+	file.Close()
+}
+
+//  The main task starts the client and server threads; it's easier
+//  to test this as a single process with threads, than as multiple
+//  processes:
+
+func main() {
+	pipe := make(chan string)
+
+	//  Start child threads
+	go server_thread()
+	go client_thread(pipe)
+	//  Loop until client tells us it's done
+	<-pipe
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/fileio2.go b/vendor/src/github.com/pebbe/zmq2/examples/fileio2.go
new file mode 100644
index 0000000..7343491
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/fileio2.go
@@ -0,0 +1,97 @@
+//  File Transfer model #2
+//
+//  In which the client requests each chunk individually, thus
+//  eliminating server queue overflows, but at a cost in speed.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"os"
+	"strconv"
+)
+
+const (
+	CHUNK_SIZE = 250000
+)
+
+func client_thread(pipe chan<- string) {
+	dealer, _ := zmq.NewSocket(zmq.DEALER)
+	dealer.Connect("tcp://127.0.0.1:6000")
+
+	total := 0  //  Total bytes received
+	chunks := 0 //  Total chunks received
+
+	for {
+		//  Ask for next chunk
+		dealer.SendMessage("fetch", total, CHUNK_SIZE)
+
+		chunk, err := dealer.RecvBytes(0)
+		if err != nil {
+			break //  Shutting down, quit
+		}
+		chunks++
+		size := len(chunk)
+		total += size
+		if size < CHUNK_SIZE {
+			break //  Last chunk received; exit
+		}
+	}
+	fmt.Printf("%v chunks received, %v bytes\n", chunks, total)
+	pipe <- "OK"
+}
+
+//  The server thread waits for a chunk request from a client,
+//  reads that chunk and sends it back to the client:
+
+func server_thread() {
+	file, err := os.Open("testdata")
+	if err != nil {
+		panic(err)
+	}
+
+	router, _ := zmq.NewSocket(zmq.ROUTER)
+	router.SetHwm(1)
+	router.Bind("tcp://*:6000")
+	for {
+		msg, err := router.RecvMessage(0)
+		if err != nil {
+			break //  Shutting down, quit
+		}
+		//  First frame in each message is the sender identity
+		identity := msg[0]
+
+		//  Second frame is "fetch" command
+		if msg[1] != "fetch" {
+			panic("command != \"fetch\"")
+		}
+
+		//  Third frame is chunk offset in file
+		offset, _ := strconv.ParseInt(msg[2], 10, 64)
+
+		//  Fourth frame is maximum chunk size
+		chunksz, _ := strconv.Atoi(msg[3])
+
+		//  Read chunk of data from file
+		chunk := make([]byte, chunksz)
+		n, _ := file.ReadAt(chunk, offset)
+
+		//  Send resulting chunk to client
+		router.SendMessage(identity, chunk[:n])
+	}
+	file.Close()
+}
+
+//  The main task is just the same as in the first model.
+
+func main() {
+	pipe := make(chan string)
+
+	//  Start child threads
+	go server_thread()
+	go client_thread(pipe)
+	//  Loop until client tells us it's done
+	<-pipe
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/fileio3.go b/vendor/src/github.com/pebbe/zmq2/examples/fileio3.go
new file mode 100644
index 0000000..3323853
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/fileio3.go
@@ -0,0 +1,110 @@
+//  File Transfer model #3
+//
+//  In which the client requests each chunk individually, using
+//  command pipelining to give us a credit-based flow control.
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"os"
+	"strconv"
+)
+
+const (
+	CHUNK_SIZE = 250000
+	PIPELINE   = 10
+)
+
+func client_thread(pipe chan<- string) {
+	dealer, _ := zmq.NewSocket(zmq.DEALER)
+	dealer.Connect("tcp://127.0.0.1:6000")
+
+	//  Up to this many chunks in transit
+	credit := PIPELINE
+
+	total := 0  //  Total bytes received
+	chunks := 0 //  Total chunks received
+	offset := 0 //  Offset of next chunk request
+
+	for {
+		for credit > 0 {
+			//  Ask for next chunk
+			dealer.SendMessage("fetch", offset, CHUNK_SIZE)
+			offset += CHUNK_SIZE
+			credit--
+		}
+		chunk, err := dealer.RecvBytes(0)
+		if err != nil {
+			break //  Shutting down, quit
+		}
+		chunks++
+		credit++
+		size := len(chunk)
+		total += size
+		if size < CHUNK_SIZE {
+			break //  Last chunk received; exit
+		}
+	}
+	fmt.Printf("%v chunks received, %v bytes\n", chunks, total)
+	pipe <- "OK"
+}
+
+//  The rest of the code is exactly the same as in model 2, except
+//  that we set the HWM on the server's ROUTER socket to PIPELINE
+//  to act as a sanity check.
+
+//  The server thread waits for a chunk request from a client,
+//  reads that chunk and sends it back to the client:
+
+func server_thread() {
+	file, err := os.Open("testdata")
+	if err != nil {
+		panic(err)
+	}
+
+	router, _ := zmq.NewSocket(zmq.ROUTER)
+	router.SetHwm(PIPELINE * 2)
+	router.Bind("tcp://*:6000")
+	for {
+		msg, err := router.RecvMessage(0)
+		if err != nil {
+			break //  Shutting down, quit
+		}
+		//  First frame in each message is the sender identity
+		identity := msg[0]
+
+		//  Second frame is "fetch" command
+		if msg[1] != "fetch" {
+			panic("command != \"fetch\"")
+		}
+
+		//  Third frame is chunk offset in file
+		offset, _ := strconv.ParseInt(msg[2], 10, 64)
+
+		//  Fourth frame is maximum chunk size
+		chunksz, _ := strconv.Atoi(msg[3])
+
+		//  Read chunk of data from file
+		chunk := make([]byte, chunksz)
+		n, _ := file.ReadAt(chunk, offset)
+
+		//  Send resulting chunk to client
+		router.SendMessage(identity, chunk[:n])
+	}
+	file.Close()
+}
+
+//  The main task is just the same as in the first model.
+
+func main() {
+	pipe := make(chan string)
+
+	//  Start child threads
+	go server_thread()
+	go client_thread(pipe)
+	//  Loop until client tells us it's done
+	<-pipe
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/flcliapi/flcliapi.go b/vendor/src/github.com/pebbe/zmq2/examples/flcliapi/flcliapi.go
new file mode 100644
index 0000000..abbf2fd
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/flcliapi/flcliapi.go
@@ -0,0 +1,268 @@
+//  flcliapi - Freelance Pattern agent class.
+// Implements the Freelance Protocol at http://rfc.zeromq.org/spec:10.
+package flcliapi
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"strconv"
+	"time"
+)
+
+const (
+	//  If no server replies within this time, abandon request
+	GLOBAL_TIMEOUT = 3000 * time.Millisecond
+	//  PING interval for servers we think are alive
+	PING_INTERVAL = 2000 * time.Millisecond
+	//  Server considered dead if silent for this long
+	SERVER_TTL = 6000 * time.Millisecond
+)
+
+//  This API works in two halves, a common pattern for APIs that need to
+//  run in the background. One half is an front-end object our application
+//  creates and works with; the other half is a back-end "agent" that runs
+//  in a background thread. The front-end talks to the back-end over an
+//  inproc pipe socket:
+
+//  ---------------------------------------------------------------------
+//  Structure of our front-end class
+
+type Flcliapi struct {
+	pipe *zmq.Socket //  Pipe through to flcliapi agent
+}
+
+//  This is the thread that handles our real flcliapi class
+
+//  ---------------------------------------------------------------------
+//  Constructor
+
+func New() (flcliapi *Flcliapi) {
+	flcliapi = &Flcliapi{}
+	flcliapi.pipe, _ = zmq.NewSocket(zmq.PAIR)
+	flcliapi.pipe.Bind("inproc://pipe")
+	go flcliapi_agent()
+	return
+}
+
+//  To implement the connect method, the front-end object sends a multi-part
+//  message to the back-end agent. The first part is a string "CONNECT", and
+//  the second part is the endpoint. It waits 100msec for the connection to
+//  come up, which isn't pretty, but saves us from sending all requests to a
+//  single server, at start-up time:
+
+func (flcliapi *Flcliapi) Connect(endpoint string) {
+	flcliapi.pipe.SendMessage("CONNECT", endpoint)
+	time.Sleep(100 * time.Millisecond) //  Allow connection to come up
+}
+
+//  To implement the request method, the front-end object sends a message
+//  to the back-end, specifying a command "REQUEST" and the request message:
+
+func (flcliapi *Flcliapi) Request(request []string) (reply []string, err error) {
+	flcliapi.pipe.SendMessage("REQUEST", request)
+	reply, err = flcliapi.pipe.RecvMessage(0)
+	if err == nil {
+		status := reply[0]
+		reply = reply[1:]
+		if status == "FAILED" {
+			reply = reply[0:0]
+		}
+	}
+	return
+}
+
+//  Here we see the back-end agent. It runs as an attached thread, talking
+//  to its parent over a pipe socket. It is a fairly complex piece of work
+//  so we'll break it down into pieces. First, the agent manages a set of
+//  servers, using our familiar class approach:
+
+//  ---------------------------------------------------------------------
+//  Simple class for one server we talk to
+
+type server_t struct {
+	endpoint string    //  Server identity/endpoint
+	alive    bool      //  true if known to be alive
+	ping_at  time.Time //  Next ping at this time
+	expires  time.Time //  Expires at this time
+}
+
+func server_new(endpoint string) (server *server_t) {
+	server = &server_t{
+		endpoint: endpoint,
+		alive:    false,
+		ping_at:  time.Now().Add(PING_INTERVAL),
+		expires:  time.Now().Add(SERVER_TTL),
+	}
+	return
+}
+
+func (server *server_t) ping(socket *zmq.Socket) {
+	if time.Now().After(server.ping_at) {
+		socket.SendMessage(server.endpoint, "PING")
+		server.ping_at = time.Now().Add(PING_INTERVAL)
+	}
+}
+
+func (server *server_t) tickless(t time.Time) time.Time {
+	if t.After(server.ping_at) {
+		return server.ping_at
+	}
+	return t
+}
+
+//  We build the agent as a class that's capable of processing messages
+//  coming in from its various sockets:
+
+//  ---------------------------------------------------------------------
+//  Simple class for one background agent
+
+type agent_t struct {
+	pipe     *zmq.Socket          //  Socket to talk back to application
+	router   *zmq.Socket          //  Socket to talk to servers
+	servers  map[string]*server_t //  Servers we've connected to
+	actives  []*server_t          //  Servers we know are alive
+	sequence int                  //  Number of requests ever sent
+	request  []string             //  Current request if any
+	reply    []string             //  Current reply if any
+	expires  time.Time            //  Timeout for request/reply
+}
+
+func agent_new() (agent *agent_t) {
+	agent = &agent_t{
+		servers: make(map[string]*server_t),
+		actives: make([]*server_t, 0),
+		request: make([]string, 0),
+		reply:   make([]string, 0),
+	}
+	agent.pipe, _ = zmq.NewSocket(zmq.PAIR)
+	agent.pipe.Connect("inproc://pipe")
+	agent.router, _ = zmq.NewSocket(zmq.ROUTER)
+	return
+}
+
+//  The control_message method processes one message from our front-end
+//  class (it's going to be CONNECT or REQUEST):
+
+func (agent *agent_t) control_message() {
+	msg, _ := agent.pipe.RecvMessage(0)
+	command := msg[0]
+	msg = msg[1:]
+
+	switch command {
+	case "CONNECT":
+		endpoint := msg[0]
+		fmt.Printf("I: connecting to %s...\n", endpoint)
+		err := agent.router.Connect(endpoint)
+		if err != nil {
+			panic("agent.router.Connect(endpoint) failed")
+		}
+		server := server_new(endpoint)
+		agent.servers[endpoint] = server
+		agent.actives = append(agent.actives, server)
+		server.ping_at = time.Now().Add(PING_INTERVAL)
+		server.expires = time.Now().Add(SERVER_TTL)
+	case "REQUEST":
+		if len(agent.request) > 0 {
+			panic("len(agent.request) > 0") //  Strict request-reply cycle
+		}
+		//  Prefix request with sequence number --(and empty envelope)--
+		agent.request = make([]string, 1, 1+len(msg))
+		agent.sequence++
+		agent.request[0] = fmt.Sprint(agent.sequence)
+		agent.request = append(agent.request, msg...)
+		//  Request expires after global timeout
+		agent.expires = time.Now().Add(GLOBAL_TIMEOUT)
+	}
+}
+
+//  The router_message method processes one message from a connected
+//  server:
+
+func (agent *agent_t) router_message() {
+	reply, _ := agent.router.RecvMessage(0)
+
+	//  Frame 0 is server that replied
+	endpoint := reply[0]
+	reply = reply[1:]
+	server, ok := agent.servers[endpoint]
+	if !ok {
+		panic("No server for endpoint")
+	}
+	if !server.alive {
+		agent.actives = append(agent.actives, server)
+		server.alive = true
+	}
+	server.ping_at = time.Now().Add(PING_INTERVAL)
+	server.expires = time.Now().Add(SERVER_TTL)
+
+	//  Frame 1 may be sequence number for reply
+	sequence, _ := strconv.Atoi(reply[0])
+	reply = reply[1:]
+	if sequence == agent.sequence {
+		agent.pipe.SendMessage("OK", reply)
+		agent.request = agent.request[0:0]
+	}
+}
+
+//  Finally here's the agent task itself, which polls its two sockets
+//  and processes incoming messages:
+
+func flcliapi_agent() {
+
+	agent := agent_new()
+
+	poller := zmq.NewPoller()
+	poller.Add(agent.pipe, zmq.POLLIN)
+	poller.Add(agent.router, zmq.POLLIN)
+	for {
+		//  Calculate tickless timer, up to 1 hour
+		tickless := time.Now().Add(time.Hour)
+		if len(agent.request) > 0 && tickless.After(agent.expires) {
+			tickless = agent.expires
+		}
+		for key := range agent.servers {
+			tickless = agent.servers[key].tickless(tickless)
+		}
+
+		polled, err := poller.Poll(tickless.Sub(time.Now()))
+		if err != nil {
+			break //  Context has been shut down
+		}
+
+		for _, item := range polled {
+			switch item.Socket {
+			case agent.pipe:
+				agent.control_message()
+			case agent.router:
+				agent.router_message()
+			}
+		}
+
+		//  If we're processing a request, dispatch to next server
+		if len(agent.request) > 0 {
+			if time.Now().After(agent.expires) {
+				//  Request expired, kill it
+				agent.pipe.SendMessage("FAILED")
+				agent.request = agent.request[0:0]
+			} else {
+				//  Find server to talk to, remove any expired ones
+				for len(agent.actives) > 0 {
+					server := agent.actives[0]
+					if time.Now().After(server.expires) {
+						agent.actives = agent.actives[1:]
+						server.alive = false
+					} else {
+						agent.router.SendMessage(server.endpoint, agent.request)
+						break
+					}
+				}
+			}
+		}
+		//  --(Disconnect and delete any expired servers)--
+		//  Send heartbeats to idle servers if needed
+		for key := range agent.servers {
+			agent.servers[key].ping(agent.router)
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/flclient1.go b/vendor/src/github.com/pebbe/zmq2/examples/flclient1.go
new file mode 100644
index 0000000..446fb82
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/flclient1.go
@@ -0,0 +1,77 @@
+//
+//  Freelance client - Model 1.
+//  Uses REQ socket to query one or more services
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"errors"
+	"fmt"
+	"os"
+	"time"
+)
+
+const (
+	REQUEST_TIMEOUT = 1000 * time.Millisecond
+	MAX_RETRIES     = 3 //  Before we abandon
+)
+
+func try_request(endpoint string, request []string) (reply []string, err error) {
+	fmt.Printf("I: trying echo service at %s...\n", endpoint)
+	client, _ := zmq.NewSocket(zmq.REQ)
+	client.Connect(endpoint)
+
+	//  Send request, wait safely for reply
+	client.SendMessage(request)
+	poller := zmq.NewPoller()
+	poller.Add(client, zmq.POLLIN)
+	polled, err := poller.Poll(REQUEST_TIMEOUT)
+	reply = []string{}
+	if len(polled) == 1 {
+		reply, err = client.RecvMessage(0)
+	} else {
+		err = errors.New("Time out")
+	}
+	return
+}
+
+//  The client uses a Lazy Pirate strategy if it only has one server to talk
+//  to. If it has 2 or more servers to talk to, it will try each server just
+//  once:
+
+func main() {
+	request := []string{"Hello world"}
+	reply := []string{}
+	var err error
+
+	endpoints := len(os.Args) - 1
+	if endpoints == 0 {
+		fmt.Printf("I: syntax: %s <endpoint> ...\n", os.Args[0])
+	} else if endpoints == 1 {
+		//  For one endpoint, we retry N times
+		for retries := 0; retries < MAX_RETRIES; retries++ {
+			endpoint := os.Args[1]
+			reply, err = try_request(endpoint, request)
+			if err == nil {
+				break //  Successful
+			}
+			fmt.Printf("W: no response from %s, retrying...\n", endpoint)
+		}
+	} else {
+		//  For multiple endpoints, try each at most once
+		for endpoint_nbr := 0; endpoint_nbr < endpoints; endpoint_nbr++ {
+			endpoint := os.Args[endpoint_nbr+1]
+			reply, err = try_request(endpoint, request)
+			if err == nil {
+				break //  Successful
+			}
+			fmt.Println("W: no response from", endpoint)
+		}
+	}
+	if len(reply) > 0 {
+		fmt.Printf("Service is running OK: %q\n", reply)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/flclient2.go b/vendor/src/github.com/pebbe/zmq2/examples/flclient2.go
new file mode 100644
index 0000000..b6241d8
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/flclient2.go
@@ -0,0 +1,118 @@
+//
+//  Freelance client - Model 2.
+//  Uses DEALER socket to blast one or more services
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"errors"
+	"fmt"
+	"os"
+	"strconv"
+	"time"
+)
+
+const (
+
+	//  If not a single service replies within this time, give up
+	GLOBAL_TIMEOUT = 2500 * time.Millisecond
+)
+
+func main() {
+	if len(os.Args) == 1 {
+		fmt.Printf("I: syntax: %s <endpoint> ...\n", os.Args[0])
+		return
+	}
+	//  Create new freelance client object
+	client := new_flclient()
+
+	//  Connect to each endpoint
+	for argn := 1; argn < len(os.Args); argn++ {
+		client.connect(os.Args[argn])
+	}
+
+	//  Send a bunch of name resolution 'requests', measure time
+	start := time.Now()
+	for requests := 10000; requests > 0; requests-- {
+		_, err := client.request("random name")
+		if err != nil {
+			fmt.Println("E: name service not available, aborting")
+			break
+		}
+	}
+	fmt.Println("Average round trip cost:", time.Now().Sub(start))
+}
+
+//  Here is the flclient class implementation. Each instance has
+//  a DEALER socket it uses to talk to the servers, a counter of how many
+//  servers it's connected to, and a request sequence number:
+
+type flclient_t struct {
+	socket   *zmq.Socket //  DEALER socket talking to servers
+	servers  int         //  How many servers we have connected to
+	sequence int         //  Number of requests ever sent
+}
+
+//  --------------------------------------------------------------------
+//  Constructor
+
+func new_flclient() (client *flclient_t) {
+	client = &flclient_t{}
+
+	client.socket, _ = zmq.NewSocket(zmq.DEALER)
+	return
+}
+
+//  --------------------------------------------------------------------
+//  Connect to new server endpoint
+
+func (client *flclient_t) connect(endpoint string) {
+	client.socket.Connect(endpoint)
+	client.servers++
+}
+
+//  The request method does the hard work. It sends a request to all
+//  connected servers in parallel (for this to work, all connections
+//  have to be successful and completed by this time). It then waits
+//  for a single successful reply, and returns that to the caller.
+//  Any other replies are just dropped:
+
+func (client *flclient_t) request(request ...string) (reply []string, err error) {
+	reply = []string{}
+
+	//  Prefix request with sequence number and empty envelope
+	client.sequence++
+
+	//  Blast the request to all connected servers
+	for server := 0; server < client.servers; server++ {
+		client.socket.SendMessage("", client.sequence, request)
+	}
+	//  Wait for a matching reply to arrive from anywhere
+	//  Since we can poll several times, calculate each one
+	endtime := time.Now().Add(GLOBAL_TIMEOUT)
+	poller := zmq.NewPoller()
+	poller.Add(client.socket, zmq.POLLIN)
+	for time.Now().Before(endtime) {
+		polled, err := poller.Poll(endtime.Sub(time.Now()))
+		if err == nil && len(polled) > 0 {
+			//  Reply is [empty][sequence][OK]
+			reply, _ = client.socket.RecvMessage(0)
+			if len(reply) != 3 {
+				panic("len(reply) != 3")
+			}
+			sequence := reply[1]
+			reply = reply[2:]
+			sequence_nbr, _ := strconv.Atoi(sequence)
+			if sequence_nbr == client.sequence {
+				break
+			}
+		}
+	}
+	if len(reply) == 0 {
+		err = errors.New("No reply")
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/flclient3.go b/vendor/src/github.com/pebbe/zmq2/examples/flclient3.go
new file mode 100644
index 0000000..8fb176b
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/flclient3.go
@@ -0,0 +1,35 @@
+//
+//  Freelance client - Model 3.
+//  Uses flcliapi class to encapsulate Freelance pattern
+//
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/flcliapi"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	//  Create new freelance client object
+	client := flcliapi.New()
+
+	//  Connect to several endpoints
+	client.Connect("tcp://localhost:5555")
+	client.Connect("tcp://localhost:5556")
+	client.Connect("tcp://localhost:5557")
+
+	//  Send a bunch of name resolution 'requests', measure time
+	start := time.Now()
+	req := []string{"random name"}
+	for requests := 1000; requests > 0; requests-- {
+		_, err := client.Request(req)
+		if err != nil {
+			fmt.Println("E: name service not available, aborting")
+			break
+		}
+	}
+	fmt.Println("Average round trip cost:", time.Now().Sub(start)/1000)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/flserver1.go b/vendor/src/github.com/pebbe/zmq2/examples/flserver1.go
new file mode 100644
index 0000000..5ae9f39
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/flserver1.go
@@ -0,0 +1,32 @@
+//
+//  Freelance server - Model 1.
+//  Trivial echo service
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"os"
+)
+
+func main() {
+	if len(os.Args) < 2 {
+		fmt.Printf("I: syntax: %s <endpoint>\n", os.Args[0])
+		return
+	}
+	server, _ := zmq.NewSocket(zmq.REP)
+	server.Bind(os.Args[1])
+
+	fmt.Println("I: echo service is ready at", os.Args[1])
+	for {
+		msg, err := server.RecvMessage(0)
+		if err != nil {
+			break //  Interrupted
+		}
+		server.SendMessage(msg)
+	}
+	fmt.Println("W: interrupted")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/flserver2.go b/vendor/src/github.com/pebbe/zmq2/examples/flserver2.go
new file mode 100644
index 0000000..040726d
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/flserver2.go
@@ -0,0 +1,39 @@
+//
+//  Freelance server - Model 2.
+//  Does some work, replies OK, with message sequencing
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"os"
+)
+
+func main() {
+	if len(os.Args) < 2 {
+		fmt.Printf("I: syntax: %s <endpoint>\n", os.Args[0])
+		return
+	}
+	server, _ := zmq.NewSocket(zmq.REP)
+	server.Bind(os.Args[1])
+
+	fmt.Println("I: service is ready at", os.Args[1])
+	for {
+		request, err := server.RecvMessage(0)
+		if err != nil {
+			break //  Interrupted
+		}
+		//  Fail nastily if run against wrong client
+		if len(request) != 2 {
+			panic("len(request) != 2")
+		}
+
+		identity := request[0]
+
+		server.SendMessage(identity, "OK")
+	}
+	fmt.Println("W: interrupted")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/flserver3.go b/vendor/src/github.com/pebbe/zmq2/examples/flserver3.go
new file mode 100644
index 0000000..4489ae4
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/flserver3.go
@@ -0,0 +1,57 @@
+//
+//  Freelance server - Model 3.
+//  Uses an ROUTER/ROUTER socket but just one thread
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"os"
+)
+
+func main() {
+	var verbose bool
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+
+	//  Prepare server socket with predictable identity
+	bind_endpoint := "tcp://*:5555"
+	connect_endpoint := "tcp://localhost:5555"
+	server, _ := zmq.NewSocket(zmq.ROUTER)
+	server.SetIdentity(connect_endpoint)
+	server.Bind(bind_endpoint)
+	fmt.Println("I: service is ready at", bind_endpoint)
+
+	for {
+		request, err := server.RecvMessage(0)
+		if err != nil {
+			break
+		}
+		if verbose {
+			fmt.Printf("%q\n", request)
+		}
+
+		//  Frame 0: identity of client
+		//  Frame 1: PING, or client control frame
+		//  Frame 2: request body
+		identity := request[0]
+		control := request[1]
+		reply := make([]string, 1, 3)
+		if control == "PING" {
+			reply = append(reply, "PONG")
+		} else {
+			reply = append(reply, control)
+			reply = append(reply, "OK")
+		}
+		reply[0] = identity
+		if verbose {
+			fmt.Printf("%q\n", reply)
+		}
+		server.SendMessage(reply)
+	}
+	fmt.Println("W: interrupted")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/hwclient.go b/vendor/src/github.com/pebbe/zmq2/examples/hwclient.go
new file mode 100644
index 0000000..21b31ca
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/hwclient.go
@@ -0,0 +1,32 @@
+//
+//  Hello World client.
+//  Connects REQ socket to tcp://localhost:5555
+//  Sends "Hello" to server, expects "World" back
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+)
+
+func main() {
+	//  Socket to talk to server
+	fmt.Println("Connecting to hello world server...")
+	requester, _ := zmq.NewSocket(zmq.REQ)
+	defer requester.Close()
+	requester.Connect("tcp://localhost:5555")
+
+	for request_nbr := 0; request_nbr != 10; request_nbr++ {
+		// send hello
+		msg := fmt.Sprintf("Hello %d", request_nbr)
+		fmt.Println("Sending ", msg)
+		requester.Send(msg, 0)
+
+		// Wait for reply:
+		reply, _ := requester.Recv(0)
+		fmt.Println("Received ", reply)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/hwserver.go b/vendor/src/github.com/pebbe/zmq2/examples/hwserver.go
new file mode 100644
index 0000000..bc4a3f1
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/hwserver.go
@@ -0,0 +1,34 @@
+//
+//  Hello World server.
+//  Binds REP socket to tcp://*:5555
+//  Expects "Hello" from client, replies with "World"
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	//  Socket to talk to clients
+	responder, _ := zmq.NewSocket(zmq.REP)
+	defer responder.Close()
+	responder.Bind("tcp://*:5555")
+
+	for {
+		//  Wait for next request from client
+		msg, _ := responder.Recv(0)
+		fmt.Println("Received ", msg)
+
+		//  Do some 'work'
+		time.Sleep(time.Second)
+
+		//  Send reply back to client
+		reply := "World"
+		responder.Send(reply, 0)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/identity.go b/vendor/src/github.com/pebbe/zmq2/examples/identity.go
new file mode 100644
index 0000000..884f23a
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/identity.go
@@ -0,0 +1,62 @@
+//
+//  Demonstrate identities as used by the request-reply pattern.
+//  Run this program by itself.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"regexp"
+)
+
+var (
+	all_char = regexp.MustCompile("^[^[:cntrl:]]*$")
+)
+
+func main() {
+	sink, _ := zmq.NewSocket(zmq.ROUTER)
+	defer sink.Close()
+	sink.Bind("inproc://example")
+
+	//  First allow 0MQ to set the identity
+	anonymous, _ := zmq.NewSocket(zmq.REQ)
+	defer anonymous.Close()
+	anonymous.Connect("inproc://example")
+	anonymous.Send("ROUTER uses a generated UUID", 0)
+	dump(sink)
+
+	//  Then set the identity ourselves
+	identified, _ := zmq.NewSocket(zmq.REQ)
+	defer identified.Close()
+	identified.SetIdentity("PEER2")
+	identified.Connect("inproc://example")
+	identified.Send("ROUTER socket uses REQ's socket identity", 0)
+	dump(sink)
+}
+
+func dump(soc *zmq.Socket) {
+	fmt.Println("----------------------------------------")
+	for {
+		//  Process all parts of the message
+		message, _ := soc.Recv(0)
+
+		//  Dump the message as text or binary
+		fmt.Printf("[%03d] ", len(message))
+		if all_char.MatchString(message) {
+			fmt.Print(message)
+		} else {
+			for i := 0; i < len(message); i++ {
+				fmt.Printf("%02X ", message[i])
+			}
+		}
+		fmt.Println()
+
+		more, _ := soc.GetRcvmore()
+		if !more {
+			break
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/interrupt.go b/vendor/src/github.com/pebbe/zmq2/examples/interrupt.go
new file mode 100644
index 0000000..84fcb19
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/interrupt.go
@@ -0,0 +1,57 @@
+// WARNING: This won't build on Windows and Plan9.
+
+//
+//  Handling Ctrl-C cleanly in C.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"log"
+	"os"
+	"os/signal"
+	"syscall"
+	"time"
+)
+
+func main() {
+	//  Socket to talk to server
+	fmt.Println("Connecting to hello world server...")
+	client, _ := zmq.NewSocket(zmq.REQ)
+	defer client.Close()
+	client.Connect("tcp://localhost:5555")
+
+	// Without signal handling, Go will exit on signal, even if the signal was caught by ZeroMQ
+	chSignal := make(chan os.Signal, 1)
+	signal.Notify(chSignal, syscall.SIGHUP, syscall.SIGINT, syscall.SIGQUIT, syscall.SIGTERM)
+
+LOOP:
+	for {
+		client.Send("HELLO", 0)
+		reply, err := client.Recv(0)
+		if err != nil {
+			if zmq.AsErrno(err) == zmq.Errno(syscall.EINTR) {
+				// signal was caught by 0MQ
+				log.Println("Client Recv:", err)
+				break
+			} else {
+				// some error occured
+				log.Panicln(err)
+			}
+		}
+
+		fmt.Println("Client:", reply)
+		time.Sleep(time.Second)
+
+		select {
+		case sig := <-chSignal:
+			// signal was caught by Go
+			log.Println("Signal:", sig)
+			break LOOP
+		default:
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/intface/intface.go b/vendor/src/github.com/pebbe/zmq2/examples/intface/intface.go
new file mode 100644
index 0000000..0788618
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/intface/intface.go
@@ -0,0 +1,254 @@
+//  Interface class for Chapter 8.
+//  This implements an "interface" to our network of nodes.
+package intface
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"github.com/pborman/uuid"
+
+	"bytes"
+	"errors"
+	"net"
+	"time"
+)
+
+//  =====================================================================
+//  Synchronous part, works in our application thread
+
+//  ---------------------------------------------------------------------
+//  Structure of our class
+
+type Intface struct {
+	pipe *zmq.Socket //  Pipe through to agent
+}
+
+//  This is the thread that handles our real interface class
+
+//  Here is the constructor for the interface class.
+//  Note that the class has barely any properties, it is just an excuse
+//  to start the background thread, and a wrapper around zmsg_recv():
+
+func New() (iface *Intface) {
+	iface = &Intface{}
+	var err error
+	iface.pipe, err = zmq.NewSocket(zmq.PAIR)
+	if err != nil {
+		panic(err)
+	}
+	err = iface.pipe.Bind("inproc://iface")
+	if err != nil {
+		panic(err)
+	}
+	go iface.agent()
+	time.Sleep(100 * time.Millisecond)
+	return
+}
+
+//  Here we wait for a message from the interface. This returns
+//  us a []string, or error if interrupted:
+
+func (iface *Intface) Recv() (msg []string, err error) {
+	msg, err = iface.pipe.RecvMessage(0)
+	return
+}
+
+//  =====================================================================
+// //  Asynchronous part, works in the background
+
+//  This structure defines each peer that we discover and track:
+
+type peer_t struct {
+	uuid_bytes  []byte
+	uuid_string string
+	expires_at  time.Time
+}
+
+const (
+	PING_PORT_NUMBER = 9999
+	PING_INTERVAL    = 1000 * time.Millisecond //  Once per second
+	PEER_EXPIRY      = 5000 * time.Millisecond //  Five seconds and it's gone
+)
+
+//  We have a constructor for the peer class:
+
+func new_peer(uuid uuid.UUID) (peer *peer_t) {
+	peer = &peer_t{
+		uuid_bytes:  []byte(uuid),
+		uuid_string: uuid.String(),
+	}
+	return
+}
+
+//  Just resets the peers expiry time; we call this method
+//  whenever we get any activity from a peer.
+
+func (peer *peer_t) is_alive() {
+	peer.expires_at = time.Now().Add(PEER_EXPIRY)
+}
+
+//  This structure holds the context for our agent, so we can
+//  pass that around cleanly to methods which need it:
+
+type agent_t struct {
+	pipe        *zmq.Socket //  Pipe back to application
+	udp         *zmq.Socket
+	conn        *net.UDPConn
+	uuid_bytes  []byte //  Our UUID
+	uuid_string string
+	peers       map[string]*peer_t //  Hash of known peers, fast lookup
+}
+
+//  Now the constructor for our agent. Each interface
+//  has one agent object, which implements its background thread:
+
+func new_agent() (agent *agent_t) {
+
+	// push output from udp into zmq socket
+	bcast := &net.UDPAddr{Port: PING_PORT_NUMBER, IP: net.IPv4bcast}
+	conn, e := net.ListenUDP("udp", bcast)
+	if e != nil {
+		panic(e)
+	}
+	go func() {
+		buffer := make([]byte, 1024)
+		udp, _ := zmq.NewSocket(zmq.PAIR)
+		udp.Bind("inproc://udp")
+		for {
+			if n, _, err := conn.ReadFrom(buffer); err == nil {
+				udp.SendBytes(buffer[:n], 0)
+			}
+		}
+	}()
+	time.Sleep(100 * time.Millisecond)
+
+	pipe, _ := zmq.NewSocket(zmq.PAIR)
+	pipe.Connect("inproc://iface")
+	udp, _ := zmq.NewSocket(zmq.PAIR)
+	udp.Connect("inproc://udp")
+
+	uuid := uuid.NewRandom()
+	agent = &agent_t{
+		pipe:        pipe,
+		udp:         udp,
+		conn:        conn,
+		uuid_bytes:  []byte(uuid),
+		uuid_string: uuid.String(),
+		peers:       make(map[string]*peer_t),
+	}
+
+	return
+}
+
+//  Here we handle the different control messages from the front-end.
+
+func (agent *agent_t) control_message() (err error) {
+	//  Get the whole message off the pipe in one go
+	msg, e := agent.pipe.RecvMessage(0)
+	if e != nil {
+		return e
+	}
+	command := msg[0]
+
+	//  We don't actually implement any control commands yet
+	//  but if we did, this would be where we did it..
+	switch command {
+	case "EXAMPLE":
+	default:
+	}
+
+	return
+}
+
+//  This is how we handle a beacon coming into our UDP socket;
+//  this may be from other peers or an echo of our own broadcast
+//  beacon:
+
+func (agent *agent_t) handle_beacon() (err error) {
+
+	msg, err := agent.udp.RecvMessage(0)
+	if len(msg[0]) != 16 {
+		return errors.New("Not a uuid")
+	}
+
+	//  If we got a UUID and it's not our own beacon, we have a peer
+	uuid := uuid.UUID(msg[0])
+	if bytes.Compare(uuid, agent.uuid_bytes) != 0 {
+		//  Find or create peer via its UUID string
+		uuid_string := uuid.String()
+		peer, ok := agent.peers[uuid_string]
+		if !ok {
+			peer = new_peer(uuid)
+			agent.peers[uuid_string] = peer
+
+			//  Report peer joined the network
+			agent.pipe.SendMessage("JOINED", uuid_string)
+		}
+		//  Any activity from the peer means it's alive
+		peer.is_alive()
+	}
+	return
+}
+
+//  This method checks one peer item for expiry; if the peer hasn't
+//  sent us anything by now, it's 'dead' and we can delete it:
+
+func (agent *agent_t) reap_peer(peer *peer_t) {
+	if time.Now().After(peer.expires_at) {
+		//  Report peer left the network
+		agent.pipe.SendMessage("LEFT", peer.uuid_string)
+		delete(agent.peers, peer.uuid_string)
+	}
+}
+
+//  This is the main loop for the background agent. It uses zmq_poll
+//  to monitor the front-end pipe (commands from the API) and the
+//  back-end UDP handle (beacons):
+
+func (iface *Intface) agent() {
+	//  Create agent instance to pass around
+	agent := new_agent()
+
+	//  Send first beacon immediately
+	ping_at := time.Now()
+
+	poller := zmq.NewPoller()
+	poller.Add(agent.pipe, zmq.POLLIN)
+	poller.Add(agent.udp, zmq.POLLIN)
+
+	bcast := &net.UDPAddr{Port: PING_PORT_NUMBER, IP: net.IPv4bcast}
+	for {
+		timeout := ping_at.Add(time.Millisecond).Sub(time.Now())
+		if timeout < 0 {
+			timeout = 0
+		}
+		polled, err := poller.Poll(timeout)
+		if err != nil {
+			break
+		}
+
+		for _, item := range polled {
+			switch socket := item.Socket; socket {
+			case agent.pipe:
+				//  If we had activity on the pipe, go handle the control
+				//  message. Current code never sends control messages.
+				agent.control_message()
+
+			case agent.udp:
+				//  If we had input on the UDP socket, go process that
+				agent.handle_beacon()
+			}
+		}
+
+		//  If we passed the 1-second mark, broadcast our beacon
+		now := time.Now()
+		if now.After(ping_at) {
+			agent.conn.WriteTo(agent.uuid_bytes, bcast)
+			ping_at = now.Add(PING_INTERVAL)
+		}
+		//  Delete and report any expired peers
+		for _, peer := range agent.peers {
+			agent.reap_peer(peer)
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/kvmsg/kvmsg.go b/vendor/src/github.com/pebbe/zmq2/examples/kvmsg/kvmsg.go
new file mode 100644
index 0000000..8b6d660
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/kvmsg/kvmsg.go
@@ -0,0 +1,262 @@
+//  kvmsg class - key-value message class for example applications
+package kvmsg
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"github.com/pborman/uuid"
+
+	"errors"
+	"fmt"
+	"os"
+	"strings"
+)
+
+//  Message is formatted on wire as 4 frames:
+//  frame 0: key (0MQ string)
+//  frame 1: sequence (8 bytes, network order)
+//  frame 2: uuid (blob, 16 bytes)
+//  frame 3: properties (0MQ string)
+//  frame 4: body (blob)
+const (
+	frame_KEY    = 0
+	frame_SEQ    = 1
+	frame_UUID   = 2
+	frame_PROPS  = 3
+	frame_BODY   = 4
+	kvmsg_FRAMES = 5
+)
+
+//  Structure of our class
+type Kvmsg struct {
+	//  Presence indicators for each frame
+	present []bool
+	//  Corresponding 0MQ message frames, if any
+	frame []string
+	//  List of properties, as name=value strings
+	props []string
+}
+
+//  These two helpers serialize a list of properties to and from a
+//  message frame:
+
+func (kvmsg *Kvmsg) encode_props() {
+	kvmsg.frame[frame_PROPS] = strings.Join(kvmsg.props, "\n") + "\n"
+	kvmsg.present[frame_PROPS] = true
+}
+
+func (kvmsg *Kvmsg) decode_props() {
+	kvmsg.props = strings.Split(kvmsg.frame[frame_PROPS], "\n")
+	if ln := len(kvmsg.props); ln > 0 && kvmsg.props[ln-1] == "" {
+		kvmsg.props = kvmsg.props[:ln-1]
+	}
+}
+
+//  Constructor, takes a sequence number for the new Kvmsg instance.
+func NewKvmsg(sequence int64) (kvmsg *Kvmsg) {
+	kvmsg = &Kvmsg{
+		present: make([]bool, kvmsg_FRAMES),
+		frame:   make([]string, kvmsg_FRAMES),
+		props:   make([]string, 0),
+	}
+	kvmsg.SetSequence(sequence)
+	return
+}
+
+//  The RecvKvmsg function reads a key-value message from socket, and returns a new
+//  Kvmsg instance.
+func RecvKvmsg(socket *zmq.Socket) (kvmsg *Kvmsg, err error) {
+	kvmsg = &Kvmsg{
+		present: make([]bool, kvmsg_FRAMES),
+		frame:   make([]string, kvmsg_FRAMES),
+	}
+	msg, err := socket.RecvMessage(0)
+	if err != nil {
+		return
+	}
+	//fmt.Printf("Recv from %s: %q\n", socket, msg)
+	for i := 0; i < kvmsg_FRAMES && i < len(msg); i++ {
+		kvmsg.frame[i] = msg[i]
+		kvmsg.present[i] = true
+	}
+	kvmsg.decode_props()
+	return
+}
+
+//  Send key-value message to socket; any empty frames are sent as such.
+func (kvmsg *Kvmsg) Send(socket *zmq.Socket) (err error) {
+	//fmt.Printf("Send to %s: %q\n", socket, kvmsg.frame)
+	kvmsg.encode_props()
+	_, err = socket.SendMessage(kvmsg.frame)
+	return
+}
+
+//  The Dup method duplicates a kvmsg instance, returns the new instance.
+func (kvmsg *Kvmsg) Dup() (dup *Kvmsg) {
+	dup = &Kvmsg{
+		present: make([]bool, kvmsg_FRAMES),
+		frame:   make([]string, kvmsg_FRAMES),
+		props:   make([]string, len(kvmsg.props)),
+	}
+	copy(dup.present, kvmsg.present)
+	copy(dup.frame, kvmsg.frame)
+	copy(dup.props, kvmsg.props)
+	return
+}
+
+//  Return key from last read message, if any, else NULL
+func (kvmsg *Kvmsg) GetKey() (key string, err error) {
+	if !kvmsg.present[frame_KEY] {
+		err = errors.New("Key not set")
+		return
+	}
+	key = kvmsg.frame[frame_KEY]
+	return
+}
+
+func (kvmsg *Kvmsg) SetKey(key string) {
+	kvmsg.frame[frame_KEY] = key
+	kvmsg.present[frame_KEY] = true
+}
+
+func (kvmsg *Kvmsg) GetSequence() (sequence int64, err error) {
+	if !kvmsg.present[frame_SEQ] {
+		err = errors.New("Sequence not set")
+		return
+	}
+	source := kvmsg.frame[frame_SEQ]
+	sequence = int64(source[0])<<56 +
+		int64(source[1])<<48 +
+		int64(source[2])<<40 +
+		int64(source[3])<<32 +
+		int64(source[4])<<24 +
+		int64(source[5])<<16 +
+		int64(source[6])<<8 +
+		int64(source[7])
+	return
+}
+
+func (kvmsg *Kvmsg) SetSequence(sequence int64) {
+
+	source := make([]byte, 8)
+	source[0] = byte((sequence >> 56) & 255)
+	source[1] = byte((sequence >> 48) & 255)
+	source[2] = byte((sequence >> 40) & 255)
+	source[3] = byte((sequence >> 32) & 255)
+	source[4] = byte((sequence >> 24) & 255)
+	source[5] = byte((sequence >> 16) & 255)
+	source[6] = byte((sequence >> 8) & 255)
+	source[7] = byte((sequence) & 255)
+
+	kvmsg.frame[frame_SEQ] = string(source)
+	kvmsg.present[frame_SEQ] = true
+}
+
+func (kvmsg *Kvmsg) GetBody() (body string, err error) {
+	if !kvmsg.present[frame_BODY] {
+		err = errors.New("Body not set")
+		return
+	}
+	body = kvmsg.frame[frame_BODY]
+	return
+}
+
+func (kvmsg *Kvmsg) SetBody(body string) {
+	kvmsg.frame[frame_BODY] = body
+	kvmsg.present[frame_BODY] = true
+}
+
+//  The size method returns the body size of the last-read message, if any.
+func (kvmsg *Kvmsg) Size() int {
+	if kvmsg.present[frame_BODY] {
+		return len(kvmsg.frame[frame_BODY])
+	}
+	return 0
+}
+
+func (kvmsg *Kvmsg) GetUuid() (uuid string, err error) {
+	if !kvmsg.present[frame_UUID] {
+		err = errors.New("Uuid not set")
+		return
+	}
+	uuid = kvmsg.frame[frame_UUID]
+	return
+}
+
+//  Sets the UUID to a random generated value
+func (kvmsg *Kvmsg) SetUuid() {
+	kvmsg.frame[frame_UUID] = string(uuid.NewRandom()) // raw 16 bytes
+	kvmsg.present[frame_UUID] = true
+
+}
+
+// Get message property, return error if no such property is defined.
+func (kvmsg *Kvmsg) GetProp(name string) (value string, err error) {
+	if !kvmsg.present[frame_PROPS] {
+		err = errors.New("No properties set")
+		return
+	}
+	f := name + "="
+	for _, prop := range kvmsg.props {
+		if strings.HasPrefix(prop, f) {
+			value = prop[len(f):]
+			return
+		}
+	}
+	err = errors.New("Property not set")
+	return
+}
+
+//  Set message property. Property name cannot contain '='.
+func (kvmsg *Kvmsg) SetProp(name, value string) (err error) {
+	if strings.Index(name, "=") >= 0 {
+		err = errors.New("No '=' allowed in property name")
+		return
+	}
+	p := name + "="
+	for i, prop := range kvmsg.props {
+		if strings.HasPrefix(prop, p) {
+			kvmsg.props = append(kvmsg.props[:i], kvmsg.props[i+1:]...)
+			break
+		}
+	}
+	kvmsg.props = append(kvmsg.props, name+"="+value)
+	kvmsg.present[frame_PROPS] = true
+	return
+}
+
+//  The store method stores the key-value message into a hash map, unless
+//  the key is nil.
+func (kvmsg *Kvmsg) Store(kvmap map[string]*Kvmsg) {
+	if kvmsg.present[frame_KEY] {
+		if kvmsg.present[frame_BODY] && kvmsg.frame[frame_BODY] != "" {
+			kvmap[kvmsg.frame[frame_KEY]] = kvmsg
+		} else {
+			delete(kvmap, kvmsg.frame[frame_KEY])
+		}
+	}
+}
+
+//  The dump method extends the kvsimple implementation with support for
+//  message properties.
+func (kvmsg *Kvmsg) Dump() {
+	size := kvmsg.Size()
+	body, _ := kvmsg.GetBody()
+	seq, _ := kvmsg.GetSequence()
+	key, _ := kvmsg.GetKey()
+	fmt.Fprintf(os.Stderr, "[seq:%v][key:%v][size:%v] ", seq, key, size)
+	p := "["
+	for _, prop := range kvmsg.props {
+		fmt.Fprint(os.Stderr, p, prop)
+		p = ";"
+	}
+	if p == ";" {
+		fmt.Fprint(os.Stderr, "]")
+	}
+	for char_nbr := 0; char_nbr < size; char_nbr++ {
+		fmt.Fprintf(os.Stderr, "%02X", body[char_nbr])
+	}
+	fmt.Fprintln(os.Stderr)
+}
+
+// The test function is in kvmsg_test.go
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/kvmsg/kvmsg_test.go b/vendor/src/github.com/pebbe/zmq2/examples/kvmsg/kvmsg_test.go
new file mode 100644
index 0000000..6e71889
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/kvmsg/kvmsg_test.go
@@ -0,0 +1,108 @@
+package kvmsg
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"os"
+	"testing"
+)
+
+//  The test is the same as in kvsimple with added support
+//  for the uuid and property features of kvmsg
+
+func TestKvmsg(t *testing.T) {
+
+	//  Prepare our context and sockets
+	output, err := zmq.NewSocket(zmq.DEALER)
+	if err != nil {
+		t.Error(err)
+	}
+
+	err = output.Bind("ipc://kvmsg_selftest.ipc")
+	if err != nil {
+		t.Error(err)
+	}
+
+	input, err := zmq.NewSocket(zmq.DEALER)
+	if err != nil {
+		t.Error(err)
+	}
+
+	err = input.Connect("ipc://kvmsg_selftest.ipc")
+	if err != nil {
+		t.Error(err)
+	}
+
+	kvmap := make(map[string]*Kvmsg)
+
+	//  Test send and receive of simple message
+	kvmsg := NewKvmsg(1)
+	kvmsg.SetKey("key")
+	kvmsg.SetUuid()
+	kvmsg.SetBody("body")
+	kvmsg.Dump()
+	err = kvmsg.Send(output)
+
+	kvmsg.Store(kvmap)
+	if err != nil {
+		t.Error(err)
+	}
+
+	kvmsg, err = RecvKvmsg(input)
+	if err != nil {
+		t.Error(err)
+	}
+	kvmsg.Dump()
+	key, err := kvmsg.GetKey()
+	if err != nil {
+		t.Error(err)
+	}
+	if key != "key" {
+		t.Error("Expected \"key\", got \"" + key + "\"")
+	}
+	kvmsg.Store(kvmap)
+
+	//  Test send and receive of message with properties
+	kvmsg = NewKvmsg(2)
+	err = kvmsg.SetProp("prop1", "value1")
+	if err != nil {
+		t.Error(err)
+	}
+	kvmsg.SetProp("prop2", "value1")
+	kvmsg.SetProp("prop2", "value2")
+	kvmsg.SetKey("key")
+	kvmsg.SetUuid()
+	kvmsg.SetBody("body")
+	if val, err := kvmsg.GetProp("prop2"); err != nil || val != "value2" {
+		if err != nil {
+			t.Error(err)
+		}
+		t.Error("Expected \"prop2\" = \"value2\", got \"" + val + "\"")
+	}
+	kvmsg.Dump()
+	err = kvmsg.Send(output)
+
+	kvmsg, err = RecvKvmsg(input)
+	if err != nil {
+		t.Error(err)
+	}
+	kvmsg.Dump()
+	key, err = kvmsg.GetKey()
+	if err != nil {
+		t.Error(err)
+	}
+	if key != "key" {
+		t.Error("Expected \"key\", got \"" + key + "\"")
+	}
+	prop, err := kvmsg.GetProp("prop2")
+	if err != nil {
+		t.Error(err)
+	}
+	if prop != "value2" {
+		t.Error("Expected property \"value2\", got \"" + key + "\"")
+	}
+
+	input.Close()
+	output.Close()
+	os.Remove("kvmsg_selftest.ipc")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/kvsimple/kvsimple.go b/vendor/src/github.com/pebbe/zmq2/examples/kvsimple/kvsimple.go
new file mode 100644
index 0000000..39f9f78
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/kvsimple/kvsimple.go
@@ -0,0 +1,157 @@
+//  kvsimple - simple key-value message class for example applications.
+//
+//  This is a very much unlike typical Go.
+package kvsimple
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"errors"
+	"fmt"
+	"os"
+)
+
+const (
+	frame_KEY    = 0
+	frame_SEQ    = 1
+	frame_BODY   = 2
+	kvmsg_FRAMES = 3
+)
+
+//  The Kvmsg type holds a single key-value message consisting of a
+//  list of 0 or more frames.
+type Kvmsg struct {
+	//  Presence indicators for each frame
+	present []bool
+	//  Corresponding 0MQ message frames, if any
+	frame []string
+}
+
+//  Constructor, takes a sequence number for the new Kvmsg instance.
+func NewKvmsg(sequence int64) (kvmsg *Kvmsg) {
+	kvmsg = &Kvmsg{
+		present: make([]bool, kvmsg_FRAMES),
+		frame:   make([]string, kvmsg_FRAMES),
+	}
+	kvmsg.SetSequence(sequence)
+	return
+}
+
+//  The RecvKvmsg function reads a key-value message from socket, and returns a new
+//  Kvmsg instance.
+func RecvKvmsg(socket *zmq.Socket) (kvmsg *Kvmsg, err error) {
+	kvmsg = &Kvmsg{
+		present: make([]bool, kvmsg_FRAMES),
+		frame:   make([]string, kvmsg_FRAMES),
+	}
+	msg, err := socket.RecvMessage(0)
+	if err != nil {
+		return
+	}
+	//fmt.Printf("Recv from %s: %q\n", socket, msg)
+	for i := 0; i < kvmsg_FRAMES && i < len(msg); i++ {
+		kvmsg.frame[i] = msg[i]
+		kvmsg.present[i] = true
+	}
+	return
+}
+
+//  The send method sends a multi-frame key-value message to a socket.
+func (kvmsg *Kvmsg) Send(socket *zmq.Socket) (err error) {
+	//fmt.Printf("Send to %s: %q\n", socket, kvmsg.frame)
+	_, err = socket.SendMessage(kvmsg.frame)
+	return
+}
+
+func (kvmsg *Kvmsg) GetKey() (key string, err error) {
+	if !kvmsg.present[frame_KEY] {
+		err = errors.New("Key not set")
+		return
+	}
+	key = kvmsg.frame[frame_KEY]
+	return
+}
+
+func (kvmsg *Kvmsg) SetKey(key string) {
+	kvmsg.frame[frame_KEY] = key
+	kvmsg.present[frame_KEY] = true
+}
+
+func (kvmsg *Kvmsg) GetSequence() (sequence int64, err error) {
+	if !kvmsg.present[frame_SEQ] {
+		err = errors.New("Sequence not set")
+		return
+	}
+	source := kvmsg.frame[frame_SEQ]
+	sequence = int64(source[0])<<56 +
+		int64(source[1])<<48 +
+		int64(source[2])<<40 +
+		int64(source[3])<<32 +
+		int64(source[4])<<24 +
+		int64(source[5])<<16 +
+		int64(source[6])<<8 +
+		int64(source[7])
+	return
+}
+
+func (kvmsg *Kvmsg) SetSequence(sequence int64) {
+
+	source := make([]byte, 8)
+	source[0] = byte((sequence >> 56) & 255)
+	source[1] = byte((sequence >> 48) & 255)
+	source[2] = byte((sequence >> 40) & 255)
+	source[3] = byte((sequence >> 32) & 255)
+	source[4] = byte((sequence >> 24) & 255)
+	source[5] = byte((sequence >> 16) & 255)
+	source[6] = byte((sequence >> 8) & 255)
+	source[7] = byte((sequence) & 255)
+
+	kvmsg.frame[frame_SEQ] = string(source)
+	kvmsg.present[frame_SEQ] = true
+}
+
+func (kvmsg *Kvmsg) GetBody() (body string, err error) {
+	if !kvmsg.present[frame_BODY] {
+		err = errors.New("Body not set")
+		return
+	}
+	body = kvmsg.frame[frame_BODY]
+	return
+}
+
+func (kvmsg *Kvmsg) SetBody(body string) {
+	kvmsg.frame[frame_BODY] = body
+	kvmsg.present[frame_BODY] = true
+}
+
+//  The size method returns the body size of the last-read message, if any.
+func (kvmsg *Kvmsg) Size() int {
+	if kvmsg.present[frame_BODY] {
+		return len(kvmsg.frame[frame_BODY])
+	}
+	return 0
+}
+
+//  The store method stores the key-value message into a hash map, unless
+//  the key is nil.
+func (kvmsg *Kvmsg) Store(kvmap map[string]*Kvmsg) {
+	if kvmsg.present[frame_KEY] {
+		kvmap[kvmsg.frame[frame_KEY]] = kvmsg
+	}
+}
+
+//  The dump method prints the key-value message to stderr,
+//  for debugging and tracing.
+func (kvmsg *Kvmsg) Dump() {
+	size := kvmsg.Size()
+	body, _ := kvmsg.GetBody()
+	seq, _ := kvmsg.GetSequence()
+	key, _ := kvmsg.GetKey()
+	fmt.Fprintf(os.Stderr, "[seq:%v][key:%v][size:%v]", seq, key, size)
+	for char_nbr := 0; char_nbr < size; char_nbr++ {
+		fmt.Fprintf(os.Stderr, "%02X", body[char_nbr])
+	}
+	fmt.Fprintln(os.Stderr)
+}
+
+// The test function is in kvsimple_test.go
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/kvsimple/kvsimple_test.go b/vendor/src/github.com/pebbe/zmq2/examples/kvsimple/kvsimple_test.go
new file mode 100644
index 0000000..daa06af
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/kvsimple/kvsimple_test.go
@@ -0,0 +1,64 @@
+package kvsimple
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"os"
+	"testing"
+)
+
+func TestKvmsg(t *testing.T) {
+
+	//  Prepare our context and sockets
+	output, err := zmq.NewSocket(zmq.DEALER)
+	if err != nil {
+		t.Error(err)
+	}
+
+	err = output.Bind("ipc://kvmsg_selftest.ipc")
+	if err != nil {
+		t.Error(err)
+	}
+
+	input, err := zmq.NewSocket(zmq.DEALER)
+	if err != nil {
+		t.Error(err)
+	}
+
+	err = input.Connect("ipc://kvmsg_selftest.ipc")
+	if err != nil {
+		t.Error(err)
+	}
+
+	kvmap := make(map[string]*Kvmsg)
+
+	//  Test send and receive of simple message
+	kvmsg := NewKvmsg(1)
+	kvmsg.SetKey("key")
+	kvmsg.SetBody("body")
+	kvmsg.Dump()
+	err = kvmsg.Send(output)
+
+	kvmsg.Store(kvmap)
+	if err != nil {
+		t.Error(err)
+	}
+
+	kvmsg, err = RecvKvmsg(input)
+	if err != nil {
+		t.Error(err)
+	}
+	kvmsg.Dump()
+	key, err := kvmsg.GetKey()
+	if err != nil {
+		t.Error(err)
+	}
+	if key != "key" {
+		t.Error("Expected \"key\", got \"" + key + "\"")
+	}
+	kvmsg.Store(kvmap)
+
+	input.Close()
+	output.Close()
+	os.Remove("kvmsg_selftest.ipc")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/lbbroker.go b/vendor/src/github.com/pebbe/zmq2/examples/lbbroker.go
new file mode 100644
index 0000000..193e868
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/lbbroker.go
@@ -0,0 +1,188 @@
+//
+//  Load-balancing broker.
+//  Clients and workers are shown here in-process
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	//"math/rand"
+	"time"
+)
+
+const (
+	NBR_CLIENTS = 10
+	NBR_WORKERS = 3
+)
+
+//  Basic request-reply client using REQ socket
+//  Since Go Send and Recv can handle 0MQ binary identities we
+//  don't need printable text identity to allow routing.
+
+func client_task() {
+	client, _ := zmq.NewSocket(zmq.REQ)
+	defer client.Close()
+	// set_id(client) //  Set a printable identity
+	client.Connect("ipc://frontend.ipc")
+
+	//  Send request, get reply
+	client.Send("HELLO", 0)
+	reply, _ := client.Recv(0)
+	fmt.Println("Client:", reply)
+}
+
+//  While this example runs in a single process, that is just to make
+//  it easier to start and stop the example.
+//  This is the worker task, using a REQ socket to do load-balancing.
+//  Since Go Send and Recv can handle 0MQ binary identities we
+//  don't need printable text identity to allow routing.
+
+func worker_task() {
+	worker, _ := zmq.NewSocket(zmq.REQ)
+	defer worker.Close()
+	// set_id(worker)
+	worker.Connect("ipc://backend.ipc")
+
+	//  Tell broker we're ready for work
+	worker.Send("READY", 0)
+
+	for {
+		//  Read and save all frames until we get an empty frame
+		//  In this example there is only 1 but it could be more
+		identity, _ := worker.Recv(0)
+		empty, _ := worker.Recv(0)
+		if empty != "" {
+			panic(fmt.Sprintf("empty is not \"\": %q", empty))
+		}
+
+		//  Get request, send reply
+		request, _ := worker.Recv(0)
+		fmt.Println("Worker:", request)
+
+		worker.Send(identity, zmq.SNDMORE)
+		worker.Send("", zmq.SNDMORE)
+		worker.Send("OK", 0)
+	}
+}
+
+//  This is the main task. It starts the clients and workers, and then
+//  routes requests between the two layers. Workers signal READY when
+//  they start; after that we treat them as ready when they reply with
+//  a response back to a client. The load-balancing data structure is
+//  just a queue of next available workers.
+
+func main() {
+	//  Prepare our sockets
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	backend, _ := zmq.NewSocket(zmq.ROUTER)
+	defer frontend.Close()
+	defer backend.Close()
+	frontend.Bind("ipc://frontend.ipc")
+	backend.Bind("ipc://backend.ipc")
+
+	client_nbr := 0
+	for ; client_nbr < NBR_CLIENTS; client_nbr++ {
+		go client_task()
+	}
+	for worker_nbr := 0; worker_nbr < NBR_WORKERS; worker_nbr++ {
+		go worker_task()
+	}
+
+	//  Here is the main loop for the least-recently-used queue. It has two
+	//  sockets; a frontend for clients and a backend for workers. It polls
+	//  the backend in all cases, and polls the frontend only when there are
+	//  one or more workers ready. This is a neat way to use 0MQ's own queues
+	//  to hold messages we're not ready to process yet. When we get a client
+	//  reply, we pop the next available worker, and send the request to it,
+	//  including the originating client identity. When a worker replies, we
+	//  re-queue that worker, and we forward the reply to the original client,
+	//  using the reply envelope.
+
+	//  Queue of available workers
+	worker_queue := make([]string, 0, 10)
+
+	poller1 := zmq.NewPoller()
+	poller1.Add(backend, zmq.POLLIN)
+	poller2 := zmq.NewPoller()
+	poller2.Add(backend, zmq.POLLIN)
+	poller2.Add(frontend, zmq.POLLIN)
+
+	for client_nbr > 0 {
+		//  Poll frontend only if we have available workers
+		var sockets []zmq.Polled
+		if len(worker_queue) > 0 {
+			sockets, _ = poller2.Poll(-1)
+		} else {
+			sockets, _ = poller1.Poll(-1)
+		}
+		for _, socket := range sockets {
+			switch socket.Socket {
+			case backend:
+
+				//  Handle worker activity on backend
+				//  Queue worker identity for load-balancing
+				worker_id, _ := backend.Recv(0)
+				if !(len(worker_queue) < NBR_WORKERS) {
+					panic("!(len(worker_queue) < NBR_WORKERS)")
+				}
+				worker_queue = append(worker_queue, worker_id)
+
+				//  Second frame is empty
+				empty, _ := backend.Recv(0)
+				if empty != "" {
+					panic(fmt.Sprintf("empty is not \"\": %q", empty))
+				}
+
+				//  Third frame is READY or else a client reply identity
+				client_id, _ := backend.Recv(0)
+
+				//  If client reply, send rest back to frontend
+				if client_id != "READY" {
+					empty, _ := backend.Recv(0)
+					if empty != "" {
+						panic(fmt.Sprintf("empty is not \"\": %q", empty))
+					}
+					reply, _ := backend.Recv(0)
+					frontend.Send(client_id, zmq.SNDMORE)
+					frontend.Send("", zmq.SNDMORE)
+					frontend.Send(reply, 0)
+					client_nbr--
+				}
+
+			case frontend:
+				//  Here is how we handle a client request:
+
+				//  Now get next client request, route to last-used worker
+				//  Client request is [identity][empty][request]
+				client_id, _ := frontend.Recv(0)
+				empty, _ := frontend.Recv(0)
+				if empty != "" {
+					panic(fmt.Sprintf("empty is not \"\": %q", empty))
+				}
+				request, _ := frontend.Recv(0)
+
+				backend.Send(worker_queue[0], zmq.SNDMORE)
+				backend.Send("", zmq.SNDMORE)
+				backend.Send(client_id, zmq.SNDMORE)
+				backend.Send("", zmq.SNDMORE)
+				backend.Send(request, 0)
+
+				//  Dequeue and drop the next worker identity
+				worker_queue = worker_queue[1:]
+
+			}
+		}
+	}
+
+	time.Sleep(100 * time.Millisecond)
+}
+
+/*
+func set_id(soc *zmq.Socket) {
+	identity := fmt.Sprintf("%04X-%04X", rand.Intn(0x10000), rand.Intn(0x10000))
+	soc.SetIdentity(identity)
+}
+*/
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/lbbroker2.go b/vendor/src/github.com/pebbe/zmq2/examples/lbbroker2.go
new file mode 100644
index 0000000..84a5999
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/lbbroker2.go
@@ -0,0 +1,147 @@
+//
+//  Load-balancing broker.
+//  Demonstrates use of higher level functions.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"strings"
+	"time"
+)
+
+const (
+	NBR_CLIENTS  = 10
+	NBR_WORKERS  = 3
+	WORKER_READY = "\001" //  Signals worker is ready
+)
+
+//  Basic request-reply client using REQ socket
+//
+func client_task() {
+	client, _ := zmq.NewSocket(zmq.REQ)
+	defer client.Close()
+	client.Connect("ipc://frontend.ipc")
+
+	//  Send request, get reply
+	for {
+		client.SendMessage("HELLO")
+		reply, _ := client.RecvMessage(0)
+		if len(reply) == 0 {
+			break
+		}
+		fmt.Println("Client:", strings.Join(reply, "\n\t"))
+		time.Sleep(time.Second)
+	}
+}
+
+//  Worker using REQ socket to do load-balancing
+//
+func worker_task() {
+	worker, _ := zmq.NewSocket(zmq.REQ)
+	defer worker.Close()
+	worker.Connect("ipc://backend.ipc")
+
+	//  Tell broker we're ready for work
+	worker.SendMessage(WORKER_READY)
+
+	//  Process messages as they arrive
+	for {
+		msg, e := worker.RecvMessage(0)
+		if e != nil {
+			break //  Interrupted ??
+		}
+		msg[len(msg)-1] = "OK"
+		worker.SendMessage(msg)
+	}
+}
+
+//  Now we come to the main task. This has the identical functionality to
+//  the previous lbbroker example but uses higher level functions to read
+//  and send messages:
+
+func main() {
+	//  Prepare our sockets
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	backend, _ := zmq.NewSocket(zmq.ROUTER)
+	defer frontend.Close()
+	defer backend.Close()
+	frontend.Bind("ipc://frontend.ipc")
+	backend.Bind("ipc://backend.ipc")
+
+	for client_nbr := 0; client_nbr < NBR_CLIENTS; client_nbr++ {
+		go client_task()
+	}
+	for worker_nbr := 0; worker_nbr < NBR_WORKERS; worker_nbr++ {
+		go worker_task()
+	}
+
+	//  Queue of available workers
+	workers := make([]string, 0, 10)
+
+	poller1 := zmq.NewPoller()
+	poller1.Add(backend, zmq.POLLIN)
+	poller2 := zmq.NewPoller()
+	poller2.Add(backend, zmq.POLLIN)
+	poller2.Add(frontend, zmq.POLLIN)
+
+LOOP:
+	for {
+		//  Poll frontend only if we have available workers
+		var sockets []zmq.Polled
+		var err error
+		if len(workers) > 0 {
+			sockets, err = poller2.Poll(-1)
+		} else {
+			sockets, err = poller1.Poll(-1)
+		}
+		if err != nil {
+			break //  Interrupted
+		}
+		for _, socket := range sockets {
+			switch socket.Socket {
+			case backend:
+				//  Handle worker activity on backend
+
+				//  Use worker identity for load-balancing
+				msg, err := backend.RecvMessage(0)
+				if err != nil {
+					break LOOP //  Interrupted
+				}
+				identity, msg := unwrap(msg)
+				workers = append(workers, identity)
+
+				//  Forward message to client if it's not a READY
+				if msg[0] != WORKER_READY {
+					frontend.SendMessage(msg)
+				}
+
+			case frontend:
+				//  Get client request, route to first available worker
+				msg, err := frontend.RecvMessage(0)
+				if err == nil {
+					backend.SendMessage(workers[0], "", msg)
+					workers = workers[1:]
+				}
+			}
+		}
+	}
+
+	time.Sleep(100 * time.Millisecond)
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/lbbroker3.go b/vendor/src/github.com/pebbe/zmq2/examples/lbbroker3.go
new file mode 100644
index 0000000..2815fe5
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/lbbroker3.go
@@ -0,0 +1,157 @@
+//
+//  Load-balancing broker.
+//  Demonstrates use of Reactor, and other higher level functions.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"strings"
+	"time"
+)
+
+const (
+	NBR_CLIENTS  = 10
+	NBR_WORKERS  = 3
+	WORKER_READY = "\001" //  Signals worker is ready
+)
+
+//  Basic request-reply client using REQ socket
+//
+func client_task() {
+	client, _ := zmq.NewSocket(zmq.REQ)
+	defer client.Close()
+	client.Connect("ipc://frontend.ipc")
+
+	//  Send request, get reply
+	for {
+		client.SendMessage("HELLO")
+		reply, _ := client.RecvMessage(0)
+		if len(reply) == 0 {
+			break
+		}
+		fmt.Println("Client:", strings.Join(reply, "\n\t"))
+		time.Sleep(time.Second)
+	}
+}
+
+//  Worker using REQ socket to do load-balancing
+//
+func worker_task() {
+	worker, _ := zmq.NewSocket(zmq.REQ)
+	defer worker.Close()
+	worker.Connect("ipc://backend.ipc")
+
+	//  Tell broker we're ready for work
+	worker.SendMessage(WORKER_READY)
+
+	//  Process messages as they arrive
+	for {
+		msg, e := worker.RecvMessage(0)
+		if e != nil {
+			break //  Interrupted
+		}
+		msg[len(msg)-1] = "OK"
+		worker.SendMessage(msg)
+	}
+}
+
+//  Our load-balancer structure, passed to reactor handlers
+type lbbroker_t struct {
+	frontend *zmq.Socket //  Listen to clients
+	backend  *zmq.Socket //  Listen to workers
+	workers  []string    //  List of ready workers
+	reactor  *zmq.Reactor
+}
+
+//  In the reactor design, each time a message arrives on a socket, the
+//  reactor passes it to a handler function. We have two handlers; one
+//  for the frontend, one for the backend:
+
+//  Handle input from client, on frontend
+func handle_frontend(lbbroker *lbbroker_t) error {
+
+	//  Get client request, route to first available worker
+	msg, err := lbbroker.frontend.RecvMessage(0)
+	if err != nil {
+		return err
+	}
+	lbbroker.backend.SendMessage(lbbroker.workers[0], "", msg)
+	lbbroker.workers = lbbroker.workers[1:]
+
+	//  Cancel reader on frontend if we went from 1 to 0 workers
+	if len(lbbroker.workers) == 0 {
+		lbbroker.reactor.RemoveSocket(lbbroker.frontend)
+	}
+	return nil
+}
+
+//  Handle input from worker, on backend
+func handle_backend(lbbroker *lbbroker_t) error {
+	//  Use worker identity for load-balancing
+	msg, err := lbbroker.backend.RecvMessage(0)
+	if err != nil {
+		return err
+	}
+	identity, msg := unwrap(msg)
+	lbbroker.workers = append(lbbroker.workers, identity)
+
+	//  Enable reader on frontend if we went from 0 to 1 workers
+	if len(lbbroker.workers) == 1 {
+		lbbroker.reactor.AddSocket(lbbroker.frontend, zmq.POLLIN,
+			func(e zmq.State) error { return handle_frontend(lbbroker) })
+	}
+
+	//  Forward message to client if it's not a READY
+	if msg[0] != WORKER_READY {
+		lbbroker.frontend.SendMessage(msg)
+	}
+
+	return nil
+}
+
+//  Now we come to the main task. This has the identical functionality to
+//  the previous lbbroker example but uses higher level functions to read
+//  and send messages:
+
+func main() {
+	lbbroker := &lbbroker_t{}
+	lbbroker.frontend, _ = zmq.NewSocket(zmq.ROUTER)
+	lbbroker.backend, _ = zmq.NewSocket(zmq.ROUTER)
+	defer lbbroker.frontend.Close()
+	defer lbbroker.backend.Close()
+	lbbroker.frontend.Bind("ipc://frontend.ipc")
+	lbbroker.backend.Bind("ipc://backend.ipc")
+
+	for client_nbr := 0; client_nbr < NBR_CLIENTS; client_nbr++ {
+		go client_task()
+	}
+	for worker_nbr := 0; worker_nbr < NBR_WORKERS; worker_nbr++ {
+		go worker_task()
+	}
+
+	//  Queue of available workers
+	lbbroker.workers = make([]string, 0, 10)
+
+	//  Prepare reactor and fire it up
+	lbbroker.reactor = zmq.NewReactor()
+	lbbroker.reactor.AddSocket(lbbroker.backend, zmq.POLLIN,
+		func(e zmq.State) error { return handle_backend(lbbroker) })
+	lbbroker.reactor.Run(-1)
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/lpclient.go b/vendor/src/github.com/pebbe/zmq2/examples/lpclient.go
new file mode 100644
index 0000000..62c5f5b
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/lpclient.go
@@ -0,0 +1,88 @@
+//
+//  Lazy Pirate client.
+//  Use zmq_poll to do a safe request-reply
+//  To run, start lpserver and then randomly kill/restart it
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"strconv"
+	"time"
+)
+
+const (
+	REQUEST_TIMEOUT = 2500 * time.Millisecond //  msecs, (> 1000!)
+	REQUEST_RETRIES = 3                       //  Before we abandon
+	SERVER_ENDPOINT = "tcp://localhost:5555"
+)
+
+func main() {
+	fmt.Println("I: connecting to server...")
+	client, err := zmq.NewSocket(zmq.REQ)
+	if err != nil {
+		panic(err)
+	}
+	client.Connect(SERVER_ENDPOINT)
+
+	poller := zmq.NewPoller()
+	poller.Add(client, zmq.POLLIN)
+
+	sequence := 0
+	retries_left := REQUEST_RETRIES
+	for retries_left > 0 {
+		//  We send a request, then we work to get a reply
+		sequence++
+		client.SendMessage(sequence)
+
+		for expect_reply := true; expect_reply; {
+			//  Poll socket for a reply, with timeout
+			sockets, err := poller.Poll(REQUEST_TIMEOUT)
+			if err != nil {
+				break //  Interrupted
+			}
+
+			//  Here we process a server reply and exit our loop if the
+			//  reply is valid. If we didn't a reply we close the client
+			//  socket and resend the request. We try a number of times
+			//  before finally abandoning:
+
+			if len(sockets) > 0 {
+				//  We got a reply from the server, must match sequence
+				reply, err := client.RecvMessage(0)
+				if err != nil {
+					break //  Interrupted
+				}
+				seq, _ := strconv.Atoi(reply[0])
+				if seq == sequence {
+					fmt.Printf("I: server replied OK (%s)\n", reply[0])
+					retries_left = REQUEST_RETRIES
+					expect_reply = false
+				} else {
+					fmt.Printf("E: malformed reply from server: %s\n", reply)
+				}
+			} else {
+				retries_left--
+				if retries_left == 0 {
+					fmt.Println("E: server seems to be offline, abandoning")
+					break
+				} else {
+					fmt.Println("W: no response from server, retrying...")
+					//  Old socket is confused; close it and open a new one
+					client.Close()
+					client, _ = zmq.NewSocket(zmq.REQ)
+					client.Connect(SERVER_ENDPOINT)
+					// Recreate poller for new client
+					poller = zmq.NewPoller()
+					poller.Add(client, zmq.POLLIN)
+					//  Send request again, on new socket
+					client.SendMessage(sequence)
+				}
+			}
+		}
+	}
+	client.Close()
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/lpserver.go b/vendor/src/github.com/pebbe/zmq2/examples/lpserver.go
new file mode 100644
index 0000000..0a5cfb4
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/lpserver.go
@@ -0,0 +1,42 @@
+//
+//  Lazy Pirate server.
+//  Binds REQ socket to tcp://*:5555
+//  Like hwserver except:
+//   - echoes request as-is
+//   - randomly runs slowly, or exits to simulate a crash.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+func main() {
+	rand.Seed(time.Now().UnixNano())
+
+	server, _ := zmq.NewSocket(zmq.REP)
+	defer server.Close()
+	server.Bind("tcp://*:5555")
+
+	for cycles := 0; true; {
+		request, _ := server.RecvMessage(0)
+		cycles++
+
+		//  Simulate various problems, after a few cycles
+		if cycles > 3 && rand.Intn(3) == 0 {
+			fmt.Println("I: simulating a crash")
+			break
+		} else if cycles > 3 && rand.Intn(3) == 0 {
+			fmt.Println("I: simulating CPU overload")
+			time.Sleep(2 * time.Second)
+		}
+		fmt.Printf("I: normal request (%s)\n", request)
+		time.Sleep(time.Second) //  Do some heavy work
+		server.SendMessage(request)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/lvcache.go b/vendor/src/github.com/pebbe/zmq2/examples/lvcache.go
new file mode 100644
index 0000000..276db19
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/lvcache.go
@@ -0,0 +1,69 @@
+//
+//  Last value cache
+//  Uses XPUB subscription messages to re-send data
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	frontend, _ := zmq.NewSocket(zmq.SUB)
+	frontend.Bind("tcp://*:5557")
+	backend, _ := zmq.NewSocket(zmq.XPUB)
+	backend.Bind("tcp://*:5558")
+
+	//  Subscribe to every single topic from publisher
+	frontend.SetSubscribe("")
+
+	//  Store last instance of each topic in a cache
+	cache := make(map[string]string)
+
+	//  We route topic updates from frontend to backend, and
+	//  we handle subscriptions by sending whatever we cached,
+	//  if anything:
+	poller := zmq.NewPoller()
+	poller.Add(frontend, zmq.POLLIN)
+	poller.Add(backend, zmq.POLLIN)
+LOOP:
+	for {
+		polled, err := poller.Poll(1000 * time.Millisecond)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		for _, item := range polled {
+			switch socket := item.Socket; socket {
+			case frontend:
+				//  Any new topic data we cache and then forward
+				msg, err := frontend.RecvMessage(0)
+				if err != nil {
+					break LOOP
+				}
+				cache[msg[0]] = msg[1]
+				backend.SendMessage(msg)
+			case backend:
+				//  When we get a new subscription we pull data from the cache:
+				msg, err := backend.RecvMessage(0)
+				if err != nil {
+					break LOOP
+				}
+				frame := msg[0]
+				//  Event is one byte 0=unsub or 1=sub, followed by topic
+				if frame[0] == 1 {
+					topic := frame[1:]
+					fmt.Println("Sending cached topic", topic)
+					previous, ok := cache[topic]
+					if ok {
+						backend.SendMessage(topic, previous)
+					}
+				}
+			}
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdapi/const.go b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/const.go
new file mode 100644
index 0000000..05fd08a
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/const.go
@@ -0,0 +1,30 @@
+// Majordomo Protocol Client and Worker API.
+// Implements the MDP/Worker spec at http://rfc.zeromq.org/spec:7.
+package mdapi
+
+const (
+	//  This is the version of MDP/Client we implement
+	MDPC_CLIENT = "MDPC01"
+
+	//  This is the version of MDP/Worker we implement
+	MDPW_WORKER = "MDPW01"
+)
+
+const (
+	//  MDP/Server commands, as strings
+	MDPW_READY = string(iota + 1)
+	MDPW_REQUEST
+	MDPW_REPLY
+	MDPW_HEARTBEAT
+	MDPW_DISCONNECT
+)
+
+var (
+	MDPS_COMMANDS = map[string]string{
+		MDPW_READY:      "READY",
+		MDPW_REQUEST:    "REQUEST",
+		MDPW_REPLY:      "REPLY",
+		MDPW_HEARTBEAT:  "HEARTBEAT",
+		MDPW_DISCONNECT: "DISCONNECT",
+	}
+)
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdcliapi.go b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdcliapi.go
new file mode 100644
index 0000000..60beaf8
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdcliapi.go
@@ -0,0 +1,173 @@
+// Majordomo Protocol Client API.
+// Implements the MDP/Worker spec at http://rfc.zeromq.org/spec:7.
+
+package mdapi
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"errors"
+	"log"
+	"runtime"
+	"time"
+)
+
+//  Structure of our class
+//  We access these properties only via class methods
+
+// Majordomo Protocol Client API.
+type Mdcli struct {
+	broker  string
+	client  *zmq.Socket   //  Socket to broker
+	verbose bool          //  Print activity to stdout
+	timeout time.Duration //  Request timeout
+	retries int           //  Request retries
+	poller  *zmq.Poller
+}
+
+//  ---------------------------------------------------------------------
+
+//  Connect or reconnect to broker.
+func (mdcli *Mdcli) ConnectToBroker() (err error) {
+	if mdcli.client != nil {
+		mdcli.client.Close()
+		mdcli.client = nil
+	}
+	mdcli.client, err = zmq.NewSocket(zmq.REQ)
+	if err != nil {
+		if mdcli.verbose {
+			log.Println("E: ConnectToBroker() creating socket failed")
+		}
+		return
+	}
+	mdcli.poller = zmq.NewPoller()
+	mdcli.poller.Add(mdcli.client, zmq.POLLIN)
+
+	if mdcli.verbose {
+		log.Printf("I: connecting to broker at %s...", mdcli.broker)
+	}
+	err = mdcli.client.Connect(mdcli.broker)
+	if err != nil && mdcli.verbose {
+		log.Println("E: ConnectToBroker() failed to connect to broker", mdcli.broker)
+	}
+
+	return
+}
+
+//  Here we have the constructor and destructor for our mdcli class:
+
+//  ---------------------------------------------------------------------
+//  Constructor
+
+func NewMdcli(broker string, verbose bool) (mdcli *Mdcli, err error) {
+
+	mdcli = &Mdcli{
+		broker:  broker,
+		verbose: verbose,
+		timeout: time.Duration(2500 * time.Millisecond),
+		retries: 3, //  Before we abandon
+	}
+	err = mdcli.ConnectToBroker()
+	runtime.SetFinalizer(mdcli, (*Mdcli).Close)
+	return
+}
+
+//  ---------------------------------------------------------------------
+//  Destructor
+
+func (mdcli *Mdcli) Close() (err error) {
+	if mdcli.client != nil {
+		err = mdcli.client.Close()
+		mdcli.client = nil
+	}
+	return
+}
+
+//  These are the class methods. We can set the request timeout and number
+//  of retry attempts, before sending requests:
+
+//  ---------------------------------------------------------------------
+
+//  Set request timeout.
+func (mdcli *Mdcli) SetTimeout(timeout time.Duration) {
+	mdcli.timeout = timeout
+}
+
+//  ---------------------------------------------------------------------
+
+//  Set request retries.
+func (mdcli *Mdcli) SetRetries(retries int) {
+	mdcli.retries = retries
+}
+
+//  Here is the send method. It sends a request to the broker and gets a
+//  reply even if it has to retry several times. It returns the reply
+//  message, or error if there was no reply after multiple attempts:
+func (mdcli *Mdcli) Send(service string, request ...string) (reply []string, err error) {
+	//  Prefix request with protocol frames
+	//  Frame 1: "MDPCxy" (six bytes, MDP/Client x.y)
+	//  Frame 2: Service name (printable string)
+
+	req := make([]string, 2, len(request)+2)
+	req = append(req, request...)
+	req[1] = service
+	req[0] = MDPC_CLIENT
+	if mdcli.verbose {
+		log.Printf("I: send request to '%s' service: %q\n", service, req)
+	}
+	for retries_left := mdcli.retries; retries_left > 0; retries_left-- {
+		_, err = mdcli.client.SendMessage(req)
+		if err != nil {
+			break
+		}
+
+		//  On any blocking call, libzmq will return -1 if there was
+		//  an error; we could in theory check for different error codes
+		//  but in practice it's OK to assume it was EINTR (Ctrl-C):
+
+		var polled []zmq.Polled
+		polled, err = mdcli.poller.Poll(mdcli.timeout)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		//  If we got a reply, process it
+		if len(polled) > 0 {
+			var msg []string
+			msg, err = mdcli.client.RecvMessage(0)
+			if err != nil {
+				break
+			}
+			if mdcli.verbose {
+				log.Printf("I: received reply: %q\n", msg)
+			}
+			//  We would handle malformed replies better in real code
+			if len(msg) < 3 {
+				panic("len(msg) < 3")
+			}
+
+			if msg[0] != MDPC_CLIENT {
+				panic("msg[0] != MDPC_CLIENT")
+			}
+
+			if msg[1] != service {
+				panic("msg[1] != service")
+			}
+
+			reply = msg[2:]
+			return //  Success
+		} else {
+			if mdcli.verbose {
+				log.Println("W: no reply, reconnecting...")
+			}
+			mdcli.ConnectToBroker()
+		}
+	}
+	if err == nil {
+		err = errors.New("permanent error")
+	}
+	if mdcli.verbose {
+		log.Println("W: permanent error, abandoning")
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdcliapi2.go b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdcliapi2.go
new file mode 100644
index 0000000..0c86dc0
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdcliapi2.go
@@ -0,0 +1,171 @@
+// Majordomo Protocol Client API.
+// Implements the MDP/Worker spec at http://rfc.zeromq.org/spec:7.
+
+package mdapi
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"errors"
+	"log"
+	"runtime"
+	"time"
+)
+
+var (
+	errPermanent = errors.New("permanent error, abandoning request")
+)
+
+//  Structure of our class
+//  We access these properties only via class methods
+
+// Majordomo Protocol Client API.
+type Mdcli2 struct {
+	broker  string
+	client  *zmq.Socket   //  Socket to broker
+	verbose bool          //  Print activity to stdout
+	timeout time.Duration //  Request timeout
+	poller  *zmq.Poller
+}
+
+//  ---------------------------------------------------------------------
+
+//  Connect or reconnect to broker. In this asynchronous class we use a
+//  DEALER socket instead of a REQ socket; this lets us send any number
+//  of requests without waiting for a reply.
+func (mdcli2 *Mdcli2) ConnectToBroker() (err error) {
+	if mdcli2.client != nil {
+		mdcli2.client.Close()
+		mdcli2.client = nil
+	}
+	mdcli2.client, err = zmq.NewSocket(zmq.DEALER)
+	if err != nil {
+		if mdcli2.verbose {
+			log.Println("E: ConnectToBroker() creating socket failed")
+		}
+		return
+	}
+	mdcli2.poller = zmq.NewPoller()
+	mdcli2.poller.Add(mdcli2.client, zmq.POLLIN)
+
+	if mdcli2.verbose {
+		log.Printf("I: connecting to broker at %s...", mdcli2.broker)
+	}
+	err = mdcli2.client.Connect(mdcli2.broker)
+	if err != nil && mdcli2.verbose {
+		log.Println("E: ConnectToBroker() failed to connect to broker", mdcli2.broker)
+	}
+
+	return
+}
+
+//  Here we have the constructor and destructor for our mdcli2 class:
+
+//  The constructor and destructor are the same as in mdcliapi, except
+//  we don't do retries, so there's no retries property.
+//  ---------------------------------------------------------------------
+//  Constructor
+
+func NewMdcli2(broker string, verbose bool) (mdcli2 *Mdcli2, err error) {
+
+	mdcli2 = &Mdcli2{
+		broker:  broker,
+		verbose: verbose,
+		timeout: time.Duration(2500 * time.Millisecond),
+	}
+	err = mdcli2.ConnectToBroker()
+	runtime.SetFinalizer(mdcli2, (*Mdcli2).Close)
+	return
+}
+
+//  ---------------------------------------------------------------------
+//  Destructor
+
+func (mdcli2 *Mdcli2) Close() (err error) {
+	if mdcli2.client != nil {
+		err = mdcli2.client.Close()
+		mdcli2.client = nil
+	}
+	return
+}
+
+//  ---------------------------------------------------------------------
+
+//  Set request timeout.
+func (mdcli2 *Mdcli2) SetTimeout(timeout time.Duration) {
+	mdcli2.timeout = timeout
+}
+
+//  The send method now just sends one message, without waiting for a
+//  reply. Since we're using a DEALER socket we have to send an empty
+//  frame at the start, to create the same envelope that the REQ socket
+//  would normally make for us:
+func (mdcli2 *Mdcli2) Send(service string, request ...string) (err error) {
+	//  Prefix request with protocol frames
+	//  Frame 0: empty (REQ emulation)
+	//  Frame 1: "MDPCxy" (six bytes, MDP/Client x.y)
+	//  Frame 2: Service name (printable string)
+
+	req := make([]string, 3, len(request)+3)
+	req = append(req, request...)
+	req[2] = service
+	req[1] = MDPC_CLIENT
+	req[0] = ""
+	if mdcli2.verbose {
+		log.Printf("I: send request to '%s' service: %q\n", service, req)
+	}
+	_, err = mdcli2.client.SendMessage(req)
+	return
+}
+
+//  The recv method waits for a reply message and returns that to the
+//  caller.
+//  ---------------------------------------------------------------------
+//  Returns the reply message or NULL if there was no reply. Does not
+//  attempt to recover from a broker failure, this is not possible
+//  without storing all unanswered requests and resending them all...
+
+func (mdcli2 *Mdcli2) Recv() (msg []string, err error) {
+
+	msg = []string{}
+
+	//  Poll socket for a reply, with timeout
+	polled, err := mdcli2.poller.Poll(mdcli2.timeout)
+	if err != nil {
+		return //  Interrupted
+	}
+
+	//  If we got a reply, process it
+	if len(polled) > 0 {
+		msg, err = mdcli2.client.RecvMessage(0)
+		if err != nil {
+			log.Println("W: interrupt received, killing client...")
+			return
+		}
+
+		if mdcli2.verbose {
+			log.Printf("I: received reply: %q\n", msg)
+		}
+		//  Don't try to handle errors, just assert noisily
+		if len(msg) < 4 {
+			panic("len(msg) < 4")
+		}
+
+		if msg[0] != "" {
+			panic("msg[0] != \"\"")
+		}
+
+		if msg[1] != MDPC_CLIENT {
+			panic("msg[1] != MDPC_CLIENT")
+		}
+
+		msg = msg[3:]
+		return //  Success
+	}
+
+	err = errPermanent
+	if mdcli2.verbose {
+		log.Println(err)
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdwrkapi.go b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdwrkapi.go
new file mode 100644
index 0000000..4b394f0
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdapi/mdwrkapi.go
@@ -0,0 +1,248 @@
+// Majordomo Protocol Worker API.
+// Implements the MDP/Worker spec at http://rfc.zeromq.org/spec:7.
+
+package mdapi
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"log"
+	"runtime"
+	"time"
+)
+
+const (
+	heartbeat_liveness = 3 //  3-5 is reasonable
+)
+
+//  This is the structure of a worker API instance. We use a pseudo-OO
+//  approach in a lot of the C examples, as well as the CZMQ binding:
+
+//  Structure of our class
+//  We access these properties only via class methods
+
+// Majordomo Protocol Worker API.
+type Mdwrk struct {
+	broker  string
+	service string
+	worker  *zmq.Socket //  Socket to broker
+	poller  *zmq.Poller
+	verbose bool //  Print activity to stdout
+
+	//  Heartbeat management
+	heartbeat_at time.Time     //  When to send HEARTBEAT
+	liveness     int           //  How many attempts left
+	heartbeat    time.Duration //  Heartbeat delay, msecs
+	reconnect    time.Duration //  Reconnect delay, msecs
+
+	expect_reply bool   //  False only at start
+	reply_to     string //  Return identity, if any
+}
+
+//  We have two utility functions; to send a message to the broker and
+//  to (re-)connect to the broker:
+
+//  ---------------------------------------------------------------------
+
+//  Send message to broker.
+func (mdwrk *Mdwrk) SendToBroker(command string, option string, msg []string) (err error) {
+
+	n := 3
+	if option != "" {
+		n++
+	}
+	m := make([]string, n, n+len(msg))
+	m = append(m, msg...)
+
+	//  Stack protocol envelope to start of message
+	if option != "" {
+		m[3] = option
+	}
+	m[2] = command
+	m[1] = MDPW_WORKER
+	m[0] = ""
+
+	if mdwrk.verbose {
+		log.Printf("I: sending %s to broker %q\n", MDPS_COMMANDS[command], m)
+	}
+	_, err = mdwrk.worker.SendMessage(m)
+	return
+}
+
+//  ---------------------------------------------------------------------
+
+//  Connect or reconnect to broker.
+func (mdwrk *Mdwrk) ConnectToBroker() (err error) {
+	if mdwrk.worker != nil {
+		mdwrk.worker.Close()
+		mdwrk.worker = nil
+	}
+	mdwrk.worker, err = zmq.NewSocket(zmq.DEALER)
+	err = mdwrk.worker.Connect(mdwrk.broker)
+	if mdwrk.verbose {
+		log.Printf("I: connecting to broker at %s...\n", mdwrk.broker)
+	}
+	mdwrk.poller = zmq.NewPoller()
+	mdwrk.poller.Add(mdwrk.worker, zmq.POLLIN)
+
+	//  Register service with broker
+	err = mdwrk.SendToBroker(MDPW_READY, mdwrk.service, []string{})
+
+	//  If liveness hits zero, queue is considered disconnected
+	mdwrk.liveness = heartbeat_liveness
+	mdwrk.heartbeat_at = time.Now().Add(mdwrk.heartbeat)
+
+	return
+}
+
+//  Here we have the constructor and destructor for our mdwrk class:
+
+//  ---------------------------------------------------------------------
+//  Constructor
+
+func NewMdwrk(broker, service string, verbose bool) (mdwrk *Mdwrk, err error) {
+
+	mdwrk = &Mdwrk{
+		broker:    broker,
+		service:   service,
+		verbose:   verbose,
+		heartbeat: 2500 * time.Millisecond,
+		reconnect: 2500 * time.Millisecond,
+	}
+
+	err = mdwrk.ConnectToBroker()
+
+	runtime.SetFinalizer(mdwrk, (*Mdwrk).Close)
+
+	return
+}
+
+//  ---------------------------------------------------------------------
+//  Destructor
+
+func (mdwrk *Mdwrk) Close() {
+	if mdwrk.worker != nil {
+		mdwrk.worker.Close()
+		mdwrk.worker = nil
+	}
+}
+
+//  We provide two methods to configure the worker API. You can set the
+//  heartbeat interval and retries to match the expected network performance.
+
+//  ---------------------------------------------------------------------
+
+//  Set heartbeat delay.
+func (mdwrk *Mdwrk) SetHeartbeat(heartbeat time.Duration) {
+	mdwrk.heartbeat = heartbeat
+}
+
+//  ---------------------------------------------------------------------
+
+//  Set reconnect delay.
+func (mdwrk *Mdwrk) SetReconnect(reconnect time.Duration) {
+	mdwrk.reconnect = reconnect
+}
+
+//  This is the recv method; it's a little misnamed since it first sends
+//  any reply and then waits for a new request. If you have a better name
+//  for this, let me know:
+
+//  ---------------------------------------------------------------------
+
+//  Send reply, if any, to broker and wait for next request.
+func (mdwrk *Mdwrk) Recv(reply []string) (msg []string, err error) {
+	//  Format and send the reply if we were provided one
+	if len(reply) == 0 && mdwrk.expect_reply {
+		panic("No reply, expected")
+	}
+	if len(reply) > 0 {
+		if mdwrk.reply_to == "" {
+			panic("mdwrk.reply_to == \"\"")
+		}
+		m := make([]string, 2, 2+len(reply))
+		m = append(m, reply...)
+		m[0] = mdwrk.reply_to
+		m[1] = ""
+		err = mdwrk.SendToBroker(MDPW_REPLY, "", m)
+	}
+	mdwrk.expect_reply = true
+
+	for {
+		var polled []zmq.Polled
+		polled, err = mdwrk.poller.Poll(mdwrk.heartbeat)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		if len(polled) > 0 {
+			msg, err = mdwrk.worker.RecvMessage(0)
+			if err != nil {
+				break //  Interrupted
+			}
+			if mdwrk.verbose {
+				log.Printf("I: received message from broker: %q\n", msg)
+			}
+			mdwrk.liveness = heartbeat_liveness
+
+			//  Don't try to handle errors, just assert noisily
+			if len(msg) < 3 {
+				panic("len(msg) < 3")
+			}
+
+			if msg[0] != "" {
+				panic("msg[0] != \"\"")
+			}
+
+			if msg[1] != MDPW_WORKER {
+				panic("msg[1] != MDPW_WORKER")
+			}
+
+			command := msg[2]
+			msg = msg[3:]
+			switch command {
+			case MDPW_REQUEST:
+				//  We should pop and save as many addresses as there are
+				//  up to a null part, but for now, just save one...
+				mdwrk.reply_to, msg = unwrap(msg)
+				//  Here is where we actually have a message to process; we
+				//  return it to the caller application:
+				return //  We have a request to process
+			case MDPW_HEARTBEAT:
+				//  Do nothing for heartbeats
+			case MDPW_DISCONNECT:
+				mdwrk.ConnectToBroker()
+			default:
+				log.Printf("E: invalid input message %q\n", msg)
+			}
+		} else {
+			mdwrk.liveness--
+			if mdwrk.liveness == 0 {
+				if mdwrk.verbose {
+					log.Println("W: disconnected from broker - retrying...")
+				}
+				time.Sleep(mdwrk.reconnect)
+				mdwrk.ConnectToBroker()
+			}
+		}
+		//  Send HEARTBEAT if it's time
+		if time.Now().After(mdwrk.heartbeat_at) {
+			mdwrk.SendToBroker(MDPW_HEARTBEAT, "", []string{})
+			mdwrk.heartbeat_at = time.Now().Add(mdwrk.heartbeat)
+		}
+	}
+	return
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdbroker.go b/vendor/src/github.com/pebbe/zmq2/examples/mdbroker.go
new file mode 100644
index 0000000..1fd2bf1
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdbroker.go
@@ -0,0 +1,423 @@
+//
+//  Majordomo Protocol broker.
+//  A minimal Go implementation of the Majordomo Protocol as defined in
+//  http://rfc.zeromq.org/spec:7 and http://rfc.zeromq.org/spec:8.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+	"github.com/pebbe/zmq2/examples/mdapi"
+
+	"fmt"
+	"log"
+	"os"
+	"runtime"
+	"time"
+)
+
+const (
+	//  We'd normally pull these from config data
+
+	HEARTBEAT_LIVENESS = 3                       //  3-5 is reasonable
+	HEARTBEAT_INTERVAL = 2500 * time.Millisecond //  msecs
+	HEARTBEAT_EXPIRY   = HEARTBEAT_INTERVAL * HEARTBEAT_LIVENESS
+)
+
+//  The broker class defines a single broker instance:
+
+type Broker struct {
+	socket       *zmq.Socket         //  Socket for clients & workers
+	verbose      bool                //  Print activity to stdout
+	endpoint     string              //  Broker binds to this endpoint
+	services     map[string]*Service //  Hash of known services
+	workers      map[string]*Worker  //  Hash of known workers
+	waiting      []*Worker           //  List of waiting workers
+	heartbeat_at time.Time           //  When to send HEARTBEAT
+}
+
+//  The service class defines a single service instance:
+
+type Service struct {
+	broker   *Broker    //  Broker instance
+	name     string     //  Service name
+	requests [][]string //  List of client requests
+	waiting  []*Worker  //  List of waiting workers
+}
+
+//  The worker class defines a single worker, idle or active:
+
+type Worker struct {
+	broker    *Broker   //  Broker instance
+	id_string string    //  Identity of worker as string
+	identity  string    //  Identity frame for routing
+	service   *Service  //  Owning service, if known
+	expiry    time.Time //  Expires at unless heartbeat
+}
+
+//  Here are the constructor and destructor for the broker:
+
+func NewBroker(verbose bool) (broker *Broker, err error) {
+
+	//  Initialize broker state
+	broker = &Broker{
+		verbose:      verbose,
+		services:     make(map[string]*Service),
+		workers:      make(map[string]*Worker),
+		waiting:      make([]*Worker, 0),
+		heartbeat_at: time.Now().Add(HEARTBEAT_INTERVAL),
+	}
+	broker.socket, err = zmq.NewSocket(zmq.ROUTER)
+	runtime.SetFinalizer(broker, (*Broker).Close)
+	return
+}
+
+func (broker *Broker) Close() (err error) {
+	if broker.socket != nil {
+		err = broker.socket.Close()
+		broker.socket = nil
+	}
+	return
+}
+
+//  The bind method binds the broker instance to an endpoint. We can call
+//  this multiple times. Note that MDP uses a single socket for both clients
+//  and workers:
+
+func (broker *Broker) Bind(endpoint string) (err error) {
+	err = broker.socket.Bind(endpoint)
+	if err != nil {
+		log.Println("E: MDP broker/0.2.0 failed to bind at", endpoint)
+		return
+	}
+	log.Println("I: MDP broker/0.2.0 is active at", endpoint)
+	return
+}
+
+//  The WorkerMsg method processes one READY, REPLY, HEARTBEAT or
+//  DISCONNECT message sent to the broker by a worker:
+
+func (broker *Broker) WorkerMsg(sender string, msg []string) {
+	//  At least, command
+	if len(msg) == 0 {
+		panic("len(msg) == 0")
+	}
+
+	command, msg := popStr(msg)
+	id_string := fmt.Sprintf("%q", sender)
+	_, worker_ready := broker.workers[id_string]
+	worker := broker.WorkerRequire(sender)
+
+	switch command {
+	case mdapi.MDPW_READY:
+		if worker_ready { //  Not first command in session
+			worker.Delete(true)
+		} else if len(sender) >= 4 /*  Reserved service name */ && sender[:4] == "mmi." {
+			worker.Delete(true)
+		} else {
+			//  Attach worker to service and mark as idle
+			worker.service = broker.ServiceRequire(msg[0])
+			worker.Waiting()
+		}
+	case mdapi.MDPW_REPLY:
+		if worker_ready {
+			//  Remove & save client return envelope and insert the
+			//  protocol header and service name, then rewrap envelope.
+			client, msg := unwrap(msg)
+			broker.socket.SendMessage(client, "", mdapi.MDPC_CLIENT, worker.service.name, msg)
+			worker.Waiting()
+		} else {
+			worker.Delete(true)
+		}
+	case mdapi.MDPW_HEARTBEAT:
+		if worker_ready {
+			worker.expiry = time.Now().Add(HEARTBEAT_EXPIRY)
+		} else {
+			worker.Delete(true)
+		}
+	case mdapi.MDPW_DISCONNECT:
+		worker.Delete(false)
+	default:
+		log.Printf("E: invalid input message %q\n", msg)
+	}
+}
+
+//  Process a request coming from a client. We implement MMI requests
+//  directly here (at present, we implement only the mmi.service request):
+
+func (broker *Broker) ClientMsg(sender string, msg []string) {
+	//  Service name + body
+	if len(msg) < 2 {
+		panic("len(msg) < 2")
+	}
+
+	service_frame, msg := popStr(msg)
+	service := broker.ServiceRequire(service_frame)
+
+	//  Set reply return identity to client sender
+	m := []string{sender, ""}
+	msg = append(m, msg...)
+
+	//  If we got a MMI service request, process that internally
+	if len(service_frame) >= 4 && service_frame[:4] == "mmi." {
+		var return_code string
+		if service_frame == "mmi.service" {
+			name := msg[len(msg)-1]
+			service, ok := broker.services[name]
+			if ok && len(service.waiting) > 0 {
+				return_code = "200"
+			} else {
+				return_code = "404"
+			}
+		} else {
+			return_code = "501"
+		}
+
+		msg[len(msg)-1] = return_code
+
+		//  Remove & save client return envelope and insert the
+		//  protocol header and service name, then rewrap envelope.
+		client, msg := unwrap(msg)
+		broker.socket.SendMessage(client, "", mdapi.MDPC_CLIENT, service_frame, msg)
+	} else {
+		//  Else dispatch the message to the requested service
+		service.Dispatch(msg)
+	}
+}
+
+//  The purge method deletes any idle workers that haven't pinged us in a
+//  while. We hold workers from oldest to most recent, so we can stop
+//  scanning whenever we find a live worker. This means we'll mainly stop
+//  at the first worker, which is essential when we have large numbers of
+//  workers (since we call this method in our critical path):
+
+func (broker *Broker) Purge() {
+	now := time.Now()
+	for len(broker.waiting) > 0 {
+		if broker.waiting[0].expiry.After(now) {
+			break //  Worker is alive, we're done here
+		}
+		if broker.verbose {
+			log.Println("I: deleting expired worker:", broker.waiting[0].id_string)
+		}
+		broker.waiting[0].Delete(false)
+	}
+}
+
+//  Here is the implementation of the methods that work on a service:
+
+//  Lazy constructor that locates a service by name, or creates a new
+//  service if there is no service already with that name.
+
+func (broker *Broker) ServiceRequire(service_frame string) (service *Service) {
+	name := service_frame
+	service, ok := broker.services[name]
+	if !ok {
+		service = &Service{
+			broker:   broker,
+			name:     name,
+			requests: make([][]string, 0),
+			waiting:  make([]*Worker, 0),
+		}
+		broker.services[name] = service
+		if broker.verbose {
+			log.Println("I: added service:", name)
+		}
+	}
+	return
+}
+
+//  The dispatch method sends requests to waiting workers:
+
+func (service *Service) Dispatch(msg []string) {
+
+	if len(msg) > 0 {
+		//  Queue message if any
+		service.requests = append(service.requests, msg)
+	}
+
+	service.broker.Purge()
+	for len(service.waiting) > 0 && len(service.requests) > 0 {
+		var worker *Worker
+		worker, service.waiting = popWorker(service.waiting)
+		service.broker.waiting = delWorker(service.broker.waiting, worker)
+		msg, service.requests = popMsg(service.requests)
+		worker.Send(mdapi.MDPW_REQUEST, "", msg)
+	}
+}
+
+//  Here is the implementation of the methods that work on a worker:
+
+//  Lazy constructor that locates a worker by identity, or creates a new
+//  worker if there is no worker already with that identity.
+
+func (broker *Broker) WorkerRequire(identity string) (worker *Worker) {
+
+	//  broker.workers is keyed off worker identity
+	id_string := fmt.Sprintf("%q", identity)
+	worker, ok := broker.workers[id_string]
+	if !ok {
+		worker = &Worker{
+			broker:    broker,
+			id_string: id_string,
+			identity:  identity,
+		}
+		broker.workers[id_string] = worker
+		if broker.verbose {
+			log.Printf("I: registering new worker: %s\n", id_string)
+		}
+	}
+	return
+}
+
+//  The delete method deletes the current worker.
+
+func (worker *Worker) Delete(disconnect bool) {
+	if disconnect {
+		worker.Send(mdapi.MDPW_DISCONNECT, "", []string{})
+	}
+
+	if worker.service != nil {
+		worker.service.waiting = delWorker(worker.service.waiting, worker)
+	}
+	worker.broker.waiting = delWorker(worker.broker.waiting, worker)
+	delete(worker.broker.workers, worker.id_string)
+}
+
+//  The send method formats and sends a command to a worker. The caller may
+//  also provide a command option, and a message payload:
+
+func (worker *Worker) Send(command, option string, msg []string) (err error) {
+	n := 4
+	if option != "" {
+		n++
+	}
+	m := make([]string, n, n+len(msg))
+	m = append(m, msg...)
+
+	//  Stack protocol envelope to start of message
+	if option != "" {
+		m[4] = option
+	}
+	m[3] = command
+	m[2] = mdapi.MDPW_WORKER
+
+	//  Stack routing envelope to start of message
+	m[1] = ""
+	m[0] = worker.identity
+
+	if worker.broker.verbose {
+		log.Printf("I: sending %s to worker %q\n", mdapi.MDPS_COMMANDS[command], m)
+	}
+	_, err = worker.broker.socket.SendMessage(m)
+	return
+}
+
+//  This worker is now waiting for work
+
+func (worker *Worker) Waiting() {
+	//  Queue to broker and service waiting lists
+	worker.broker.waiting = append(worker.broker.waiting, worker)
+	worker.service.waiting = append(worker.service.waiting, worker)
+	worker.expiry = time.Now().Add(HEARTBEAT_EXPIRY)
+	worker.service.Dispatch([]string{})
+}
+
+//  Finally here is the main task. We create a new broker instance and
+//  then processes messages on the broker socket:
+
+func main() {
+	verbose := false
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+
+	broker, _ := NewBroker(verbose)
+	broker.Bind("tcp://*:5555")
+
+	poller := zmq.NewPoller()
+	poller.Add(broker.socket, zmq.POLLIN)
+
+	//  Get and process messages forever or until interrupted
+	for {
+		polled, err := poller.Poll(HEARTBEAT_INTERVAL)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		//  Process next input message, if any
+		if len(polled) > 0 {
+			msg, err := broker.socket.RecvMessage(0)
+			if err != nil {
+				break //  Interrupted
+			}
+			if broker.verbose {
+				log.Printf("I: received message: %q\n", msg)
+			}
+			sender, msg := popStr(msg)
+			_, msg = popStr(msg)
+			header, msg := popStr(msg)
+
+			switch header {
+			case mdapi.MDPC_CLIENT:
+				broker.ClientMsg(sender, msg)
+			case mdapi.MDPW_WORKER:
+				broker.WorkerMsg(sender, msg)
+			default:
+				log.Printf("E: invalid message: %q\n", msg)
+			}
+		}
+		//  Disconnect and delete any expired workers
+		//  Send heartbeats to idle workers if needed
+		if time.Now().After(broker.heartbeat_at) {
+			broker.Purge()
+			for _, worker := range broker.waiting {
+				worker.Send(mdapi.MDPW_HEARTBEAT, "", []string{})
+			}
+			broker.heartbeat_at = time.Now().Add(HEARTBEAT_INTERVAL)
+		}
+	}
+	log.Println("W: interrupt received, shutting down...")
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
+
+func popStr(ss []string) (s string, ss2 []string) {
+	s = ss[0]
+	ss2 = ss[1:]
+	return
+}
+
+func popMsg(msgs [][]string) (msg []string, msgs2 [][]string) {
+	msg = msgs[0]
+	msgs2 = msgs[1:]
+	return
+}
+
+func popWorker(workers []*Worker) (worker *Worker, workers2 []*Worker) {
+	worker = workers[0]
+	workers2 = workers[1:]
+	return
+}
+
+func delWorker(workers []*Worker, worker *Worker) []*Worker {
+	for i := 0; i < len(workers); i++ {
+		if workers[i] == worker {
+			workers = append(workers[:i], workers[i+1:]...)
+			i--
+		}
+	}
+	return workers
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdclient.go b/vendor/src/github.com/pebbe/zmq2/examples/mdclient.go
new file mode 100644
index 0000000..348ea2d
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdclient.go
@@ -0,0 +1,32 @@
+//
+//  Majordomo Protocol client example.
+//  Uses the mdcli API to hide all MDP aspects
+//
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/mdapi"
+
+	"fmt"
+	"log"
+	"os"
+)
+
+func main() {
+	var verbose bool
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+	session, _ := mdapi.NewMdcli("tcp://localhost:5555", verbose)
+
+	count := 0
+	for ; count < 100000; count++ {
+		_, err := session.Send("echo", "Hello world")
+		if err != nil {
+			log.Println(err)
+			break //  Interrupt or failure
+		}
+	}
+	fmt.Printf("%d requests/replies processed\n", count)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdclient2.go b/vendor/src/github.com/pebbe/zmq2/examples/mdclient2.go
new file mode 100644
index 0000000..9a52e72
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdclient2.go
@@ -0,0 +1,39 @@
+//
+//  Majordomo Protocol client example - asynchronous.
+//  Uses the mdcli API to hide all MDP aspects
+//
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/mdapi"
+
+	"fmt"
+	"log"
+	"os"
+)
+
+func main() {
+	var verbose bool
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+	session, _ := mdapi.NewMdcli2("tcp://localhost:5555", verbose)
+
+	var count int
+	for count = 0; count < 100000; count++ {
+		err := session.Send("echo", "Hello world")
+		if err != nil {
+			log.Println("Send:", err)
+			break
+		}
+	}
+	for count = 0; count < 100000; count++ {
+		_, err := session.Recv()
+		if err != nil {
+			log.Println("Recv:", err)
+			break
+		}
+	}
+	fmt.Printf("%d replies received\n", count)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mdworker.go b/vendor/src/github.com/pebbe/zmq2/examples/mdworker.go
new file mode 100644
index 0000000..42b6ca7
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mdworker.go
@@ -0,0 +1,32 @@
+//
+//  Majordomo Protocol worker example.
+//  Uses the mdwrk API to hide all MDP aspects
+//
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/mdapi"
+
+	"log"
+	"os"
+)
+
+func main() {
+	var verbose bool
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+	session, _ := mdapi.NewMdwrk("tcp://localhost:5555", "echo", verbose)
+
+	var err error
+	var request, reply []string
+	for {
+		request, err = session.Recv(reply)
+		if err != nil {
+			break //  Worker was interrupted
+		}
+		reply = request //  Echo is complex... :-)
+	}
+	log.Println(err)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mmiecho.go b/vendor/src/github.com/pebbe/zmq2/examples/mmiecho.go
new file mode 100644
index 0000000..7f1beed
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mmiecho.go
@@ -0,0 +1,32 @@
+//
+//  MMI echo query example.
+//
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/mdapi"
+
+	"fmt"
+	"os"
+)
+
+func main() {
+	var verbose bool
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+	session, _ := mdapi.NewMdcli("tcp://localhost:5555", verbose)
+
+	//  This is the service we want to look up
+	request := "echo"
+
+	//  This is the service we send our request to
+	reply, err := session.Send("mmi.service", request)
+
+	if err == nil {
+		fmt.Println("Lookup echo service:", reply[0])
+	} else {
+		fmt.Println("E: no response from broker, make sure it's running")
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/msgqueue.go b/vendor/src/github.com/pebbe/zmq2/examples/msgqueue.go
new file mode 100644
index 0000000..4cd56f4
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/msgqueue.go
@@ -0,0 +1,36 @@
+//
+//  Simple message queuing broker.
+//  Same as request-reply broker but using QUEUE device
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"log"
+)
+
+func main() {
+	var err error
+
+	//  Socket facing clients
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	defer frontend.Close()
+	err = frontend.Bind("tcp://*:5559")
+	if err != nil {
+		log.Fatalln("Binding frontend:", err)
+	}
+
+	//  Socket facing services
+	backend, _ := zmq.NewSocket(zmq.DEALER)
+	defer backend.Close()
+	err = backend.Bind("tcp://*:5560")
+	if err != nil {
+		log.Fatalln("Binding backend:", err)
+	}
+
+	//  Start the proxy
+	err = zmq.Proxy(frontend, backend, nil)
+	log.Fatalln("Proxy interrupted:", err)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mspoller.go b/vendor/src/github.com/pebbe/zmq2/examples/mspoller.go
new file mode 100644
index 0000000..3e9e071
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mspoller.go
@@ -0,0 +1,47 @@
+//
+//  Reading from multiple sockets.
+//  This version uses zmq.Poll()
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+)
+
+func main() {
+
+	//  Connect to task ventilator
+	receiver, _ := zmq.NewSocket(zmq.PULL)
+	defer receiver.Close()
+	receiver.Connect("tcp://localhost:5557")
+
+	//  Connect to weather server
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	defer subscriber.Close()
+	subscriber.Connect("tcp://localhost:5556")
+	subscriber.SetSubscribe("10001 ")
+
+	//  Initialize poll set
+	poller := zmq.NewPoller()
+	poller.Add(receiver, zmq.POLLIN)
+	poller.Add(subscriber, zmq.POLLIN)
+	//  Process messages from both sockets
+	for {
+		sockets, _ := poller.Poll(-1)
+		for _, socket := range sockets {
+			switch s := socket.Socket; s {
+			case receiver:
+				task, _ := s.Recv(0)
+				//  Process task
+				fmt.Println("Got task:", task)
+			case subscriber:
+				update, _ := s.Recv(0)
+				//  Process weather update
+				fmt.Println("Got weather update:", update)
+			}
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/msreader.go b/vendor/src/github.com/pebbe/zmq2/examples/msreader.go
new file mode 100644
index 0000000..d1099ac
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/msreader.go
@@ -0,0 +1,55 @@
+//
+//  Reading from multiple sockets.
+//  This version uses a simple recv loop
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+
+	//  Connect to task ventilator
+	receiver, _ := zmq.NewSocket(zmq.PULL)
+	defer receiver.Close()
+	receiver.Connect("tcp://localhost:5557")
+
+	//  Connect to weather server
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	defer subscriber.Close()
+	subscriber.Connect("tcp://localhost:5556")
+	subscriber.SetSubscribe("10001 ")
+
+	//  Process messages from both sockets
+	//  We prioritize traffic from the task ventilator
+	for {
+
+		//  Process any waiting tasks
+		for {
+			task, err := receiver.Recv(zmq.NOBLOCK)
+			if err != nil {
+				break
+			}
+			//  process task
+			fmt.Println("Got task:", task)
+		}
+
+		//  Process any waiting weather updates
+		for {
+			udate, err := subscriber.Recv(zmq.NOBLOCK)
+			if err != nil {
+				break
+			}
+			//  process weather update
+			fmt.Println("Got weather update:", udate)
+		}
+
+		//  No activity, so sleep for 1 msec
+		time.Sleep(time.Millisecond)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mtrelay.go b/vendor/src/github.com/pebbe/zmq2/examples/mtrelay.go
new file mode 100644
index 0000000..c94784e
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mtrelay.go
@@ -0,0 +1,52 @@
+//
+//  Multithreaded relay.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+)
+
+func step1() {
+	//  Connect to step2 and tell it we're ready
+	xmitter, _ := zmq.NewSocket(zmq.PAIR)
+	defer xmitter.Close()
+	xmitter.Connect("inproc://step2")
+	fmt.Println("Step 1 ready, signaling step 2")
+	xmitter.Send("READY", 0)
+}
+
+func step2() {
+	//  Bind inproc socket before starting step1
+	receiver, _ := zmq.NewSocket(zmq.PAIR)
+	defer receiver.Close()
+	receiver.Bind("inproc://step2")
+	go step1()
+
+	//  Wait for signal and pass it on
+	receiver.Recv(0)
+
+	//  Connect to step3 and tell it we're ready
+	xmitter, _ := zmq.NewSocket(zmq.PAIR)
+	defer xmitter.Close()
+	xmitter.Connect("inproc://step3")
+	fmt.Println("Step 2 ready, signaling step 3")
+	xmitter.Send("READY", 0)
+}
+
+func main() {
+
+	//  Bind inproc socket before starting step2
+	receiver, _ := zmq.NewSocket(zmq.PAIR)
+	defer receiver.Close()
+	receiver.Bind("inproc://step3")
+	go step2()
+
+	//  Wait for signal
+	receiver.Recv(0)
+
+	fmt.Println("Test successful!")
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/mtserver.go b/vendor/src/github.com/pebbe/zmq2/examples/mtserver.go
new file mode 100644
index 0000000..3f50b2d
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/mtserver.go
@@ -0,0 +1,54 @@
+//
+//  Multithreaded Hello World server.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"log"
+	"time"
+)
+
+func worker_routine() {
+	//  Socket to talk to dispatcher
+	receiver, _ := zmq.NewSocket(zmq.REP)
+	defer receiver.Close()
+	receiver.Connect("inproc://workers")
+
+	for {
+		msg, e := receiver.Recv(0)
+		if e != nil {
+			break
+		}
+		fmt.Println("Received request: [" + msg + "]")
+
+		//  Do some 'work'
+		time.Sleep(time.Second)
+
+		//  Send reply back to client
+		receiver.Send("World", 0)
+	}
+}
+
+func main() {
+	//  Socket to talk to clients
+	clients, _ := zmq.NewSocket(zmq.ROUTER)
+	defer clients.Close()
+	clients.Bind("tcp://*:5555")
+
+	//  Socket to talk to workers
+	workers, _ := zmq.NewSocket(zmq.DEALER)
+	defer workers.Close()
+	workers.Bind("inproc://workers")
+
+	//  Launch pool of worker goroutines
+	for thread_nbr := 0; thread_nbr < 5; thread_nbr++ {
+		go worker_routine()
+	}
+	//  Connect work threads to client threads via a queue proxy
+	err := zmq.Proxy(clients, workers, nil)
+	log.Fatalln("Proxy interrupted:", err)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/pathopub.go b/vendor/src/github.com/pebbe/zmq2/examples/pathopub.go
new file mode 100644
index 0000000..8988c09
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/pathopub.go
@@ -0,0 +1,44 @@
+//
+//  Pathological publisher
+//  Sends out 1,000 topics and then one random update per second
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"os"
+	"time"
+)
+
+func main() {
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	if len(os.Args) == 2 {
+		publisher.Connect(os.Args[1])
+	} else {
+		publisher.Bind("tcp://*:5556")
+	}
+
+	//  Ensure subscriber connection has time to complete
+	time.Sleep(time.Second)
+
+	//  Send out all 1,000 topic messages
+	for topic_nbr := 0; topic_nbr < 1000; topic_nbr++ {
+		_, err := publisher.SendMessage(fmt.Sprintf("%03d", topic_nbr), "Save Roger")
+		if err != nil {
+			fmt.Println(err)
+		}
+	}
+	//  Send one random update per second
+	rand.Seed(time.Now().UnixNano())
+	for {
+		time.Sleep(time.Second)
+		_, err := publisher.SendMessage(fmt.Sprintf("%03d", rand.Intn(1000)), "Off with his head!")
+		if err != nil {
+			fmt.Println(err)
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/pathosub.go b/vendor/src/github.com/pebbe/zmq2/examples/pathosub.go
new file mode 100644
index 0000000..fc45447
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/pathosub.go
@@ -0,0 +1,41 @@
+//
+//  Pathological subscriber
+//  Subscribes to one random topic and prints received messages
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"os"
+	"time"
+)
+
+func main() {
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	if len(os.Args) == 2 {
+		subscriber.Connect(os.Args[1])
+	} else {
+		subscriber.Connect("tcp://localhost:5556")
+	}
+
+	rand.Seed(time.Now().UnixNano())
+	subscription := fmt.Sprintf("%03d", rand.Intn(1000))
+	subscriber.SetSubscribe(subscription)
+
+	for {
+		msg, err := subscriber.RecvMessage(0)
+		if err != nil {
+			break
+		}
+		topic := msg[0]
+		data := msg[1]
+		if topic != subscription {
+			panic("topic != subscription")
+		}
+		fmt.Println(data)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/peering1.go b/vendor/src/github.com/pebbe/zmq2/examples/peering1.go
new file mode 100644
index 0000000..175e2e6
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/peering1.go
@@ -0,0 +1,66 @@
+//
+//  Broker peering simulation (part 1).
+//  Prototypes the state flow
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"os"
+	"time"
+)
+
+func main() {
+	//  First argument is this broker's name
+	//  Other arguments are our peers' names
+	//
+	if len(os.Args) < 2 {
+		fmt.Println("syntax: peering1 me {you}...")
+		os.Exit(1)
+	}
+	self := os.Args[1]
+	fmt.Printf("I: preparing broker at %s...\n", self)
+	rand.Seed(time.Now().UnixNano())
+
+	//  Bind state backend to endpoint
+	statebe, _ := zmq.NewSocket(zmq.PUB)
+	defer statebe.Close()
+	statebe.Bind("ipc://" + self + "-state.ipc")
+
+	//  Connect statefe to all peers
+	statefe, _ := zmq.NewSocket(zmq.SUB)
+	defer statefe.Close()
+	statefe.SetSubscribe("")
+	for _, peer := range os.Args[2:] {
+		fmt.Printf("I: connecting to state backend at '%s'\n", peer)
+		statefe.Connect("ipc://" + peer + "-state.ipc")
+	}
+
+	//  The main loop sends out status messages to peers, and collects
+	//  status messages back from peers. The zmq_poll timeout defines
+	//  our own heartbeat:
+
+	poller := zmq.NewPoller()
+	poller.Add(statefe, zmq.POLLIN)
+	for {
+		//  Poll for activity, or 1 second timeout
+		sockets, err := poller.Poll(time.Second)
+		if err != nil {
+			break
+		}
+
+		//  Handle incoming status messages
+		if len(sockets) == 1 {
+			msg, _ := statefe.RecvMessage(0)
+			peer_name := msg[0]
+			available := msg[1]
+			fmt.Printf("%s - %s workers free\n", peer_name, available)
+		} else {
+			statebe.SendMessage(self, rand.Intn(10))
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/peering2.go b/vendor/src/github.com/pebbe/zmq2/examples/peering2.go
new file mode 100644
index 0000000..f135daf
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/peering2.go
@@ -0,0 +1,264 @@
+//
+//  Broker peering simulation (part 2).
+//  Prototypes the request-reply flow
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"log"
+	"math/rand"
+	"os"
+	"time"
+)
+
+const (
+	NBR_CLIENTS  = 10
+	NBR_WORKERS  = 3
+	WORKER_READY = "**READY**" //  Signals worker is ready
+)
+
+var (
+	peers = make(map[string]bool)
+)
+
+//  The client task does a request-reply dialog using a standard
+//  synchronous REQ socket:
+
+func client_task(name string, i int) {
+	clientname := fmt.Sprintf("Client-%s-%d", name, i)
+
+	client, _ := zmq.NewSocket(zmq.REQ)
+	defer client.Close()
+	client.SetIdentity(clientname)
+	client.Connect("ipc://" + name + "-localfe.ipc")
+
+	for {
+		//  Send request, get reply
+		client.Send("HELLO from "+clientname, 0)
+		reply, err := client.Recv(0)
+		if err != nil {
+			fmt.Println("client_task interrupted", name)
+			break //  Interrupted
+		}
+		fmt.Printf("%s: %s\n", clientname, reply)
+		time.Sleep(time.Duration(500+rand.Intn(1000)) * time.Millisecond)
+	}
+}
+
+//  The worker task plugs into the load-balancer using a REQ
+//  socket:
+
+func worker_task(name string, i int) {
+	workername := fmt.Sprintf("Worker-%s-%d", name, i)
+
+	worker, _ := zmq.NewSocket(zmq.REQ)
+	defer worker.Close()
+	worker.SetIdentity(workername)
+	worker.Connect("ipc://" + name + "-localbe.ipc")
+
+	//  Tell broker we're ready for work
+	worker.SendMessage(WORKER_READY)
+
+	//  Process messages as they arrive
+	for {
+		msg, err := worker.RecvMessage(0)
+		if err != nil {
+			fmt.Println("worker_task interrupted", name)
+			break //  Interrupted
+		}
+
+		i := len(msg) - 1
+		fmt.Printf("%s: %s\n", workername, msg[i])
+		worker.SendMessage(msg[:i], "OK from "+workername)
+	}
+}
+
+//  The main task begins by setting-up its frontend and backend sockets
+//  and then starting its client and worker tasks:
+
+func main() {
+	//  First argument is this broker's name
+	//  Other arguments are our peers' names
+	//
+	if len(os.Args) < 2 {
+		fmt.Println("syntax: peering2 me {you}...")
+		os.Exit(1)
+	}
+	for _, peer := range os.Args[2:] {
+		peers[peer] = true
+	}
+
+	self := os.Args[1]
+	fmt.Println("I: preparing broker at", self)
+	rand.Seed(time.Now().UnixNano())
+
+	//  Bind cloud frontend to endpoint
+	cloudfe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer cloudfe.Close()
+	cloudfe.SetIdentity(self)
+	cloudfe.Bind("ipc://" + self + "-cloud.ipc")
+
+	//  Connect cloud backend to all peers
+	cloudbe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer cloudbe.Close()
+	cloudbe.SetIdentity(self)
+	for _, peer := range os.Args[2:] {
+		fmt.Println("I: connecting to cloud frontend at", peer)
+		cloudbe.Connect("ipc://" + peer + "-cloud.ipc")
+	}
+	//  Prepare local frontend and backend
+	localfe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer localfe.Close()
+	localfe.Bind("ipc://" + self + "-localfe.ipc")
+	localbe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer localbe.Close()
+	localbe.Bind("ipc://" + self + "-localbe.ipc")
+
+	//  Get user to tell us when we can start...
+	fmt.Print("Press Enter when all brokers are started: ")
+	var line string
+	fmt.Scanln(&line)
+
+	//  Start local workers
+	for worker_nbr := 0; worker_nbr < NBR_WORKERS; worker_nbr++ {
+		go worker_task(self, worker_nbr)
+	}
+
+	//  Start local clients
+	for client_nbr := 0; client_nbr < NBR_CLIENTS; client_nbr++ {
+		go client_task(self, client_nbr)
+	}
+
+	//  Here we handle the request-reply flow. We're using load-balancing
+	//  to poll workers at all times, and clients only when there are one or
+	//  more workers available.
+
+	//  Least recently used queue of available workers
+	workers := make([]string, 0)
+
+	backends := zmq.NewPoller()
+	backends.Add(localbe, zmq.POLLIN)
+	backends.Add(cloudbe, zmq.POLLIN)
+	frontends := zmq.NewPoller()
+	frontends.Add(localfe, zmq.POLLIN)
+	frontends.Add(cloudfe, zmq.POLLIN)
+
+	msg := []string{}
+	number_of_peers := len(os.Args) - 2
+
+	for {
+		//  First, route any waiting replies from workers
+		//  If we have no workers anyhow, wait indefinitely
+		timeout := time.Second
+		if len(workers) == 0 {
+			timeout = -1
+		}
+		sockets, err := backends.Poll(timeout)
+		if err != nil {
+			log.Println(err)
+			break //  Interrupted
+		}
+
+		msg = msg[:]
+		if socketInPolled(localbe, sockets) {
+			//  Handle reply from local worker
+			msg, err = localbe.RecvMessage(0)
+			if err != nil {
+				log.Println(err)
+				break //  Interrupted
+			}
+			var identity string
+			identity, msg = unwrap(msg)
+			workers = append(workers, identity)
+
+			//  If it's READY, don't route the message any further
+			if msg[0] == WORKER_READY {
+				msg = msg[0:0]
+			}
+		} else if socketInPolled(cloudbe, sockets) {
+			//  Or handle reply from peer broker
+			msg, err = cloudbe.RecvMessage(0)
+			if err != nil {
+				log.Println(err)
+				break //  Interrupted
+			}
+
+			//  We don't use peer broker identity for anything
+			_, msg = unwrap(msg)
+		}
+
+		if len(msg) > 0 {
+			//  Route reply to cloud if it's addressed to a broker
+			if peers[msg[0]] {
+				cloudfe.SendMessage(msg)
+			} else {
+				localfe.SendMessage(msg)
+			}
+		}
+
+		//  Now we route as many client requests as we have worker capacity
+		//  for. We may reroute requests from our local frontend, but not from
+		//  the cloud frontend. We reroute randomly now, just to test things
+		//  out. In the next version we'll do this properly by calculating
+		//  cloud capacity:
+
+		for len(workers) > 0 {
+			sockets, err := frontends.Poll(0)
+			if err != nil {
+				log.Println(err)
+				break //  Interrupted
+			}
+			var reroutable bool
+			//  We'll do peer brokers first, to prevent starvation
+			if socketInPolled(cloudfe, sockets) {
+				msg, _ = cloudfe.RecvMessage(0)
+				reroutable = false
+			} else if socketInPolled(localfe, sockets) {
+				msg, _ = localfe.RecvMessage(0)
+				reroutable = true
+			} else {
+				break //  No work, go back to backends
+			}
+
+			//  If reroutable, send to cloud 20% of the time
+			//  Here we'd normally use cloud status information
+			//
+			if reroutable && number_of_peers > 0 && rand.Intn(5) == 0 {
+				//  Route to random broker peer
+				random_peer := os.Args[2+rand.Intn(number_of_peers)]
+				cloudbe.SendMessage(random_peer, "", msg)
+			} else {
+				localbe.SendMessage(workers[0], "", msg)
+				workers = workers[1:]
+			}
+		}
+	}
+	fmt.Println("Exit")
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
+
+// Returns true if *Socket is in []Polled
+func socketInPolled(s *zmq.Socket, p []zmq.Polled) bool {
+	for _, pp := range p {
+		if pp.Socket == s {
+			return true
+		}
+	}
+	return false
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/peering3.go b/vendor/src/github.com/pebbe/zmq2/examples/peering3.go
new file mode 100644
index 0000000..cec601e
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/peering3.go
@@ -0,0 +1,335 @@
+//
+//  Broker peering simulation (part 3).
+//  Prototypes the full flow of status and tasks
+//
+
+/*
+
+One of the differences between peering2 and peering3 is that
+peering2 always uses Poll() and then uses a helper function socketInPolled()
+to check if a specific socket returned a result, while peering3 uses PollAll()
+and checks the event state of the socket in a specific index in the list.
+
+*/
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"os"
+	"strconv"
+	"strings"
+	"time"
+)
+
+const (
+	NBR_CLIENTS  = 10
+	NBR_WORKERS  = 5
+	WORKER_READY = "**READY**" //  Signals worker is ready
+)
+
+var (
+	//  Our own name; in practice this would be configured per node
+	self string
+)
+
+//  This is the client task. It issues a burst of requests and then
+//  sleeps for a few seconds. This simulates sporadic activity; when
+//  a number of clients are active at once, the local workers should
+//  be overloaded. The client uses a REQ socket for requests and also
+//  pushes statistics to the monitor socket:
+
+func client_task(i int) {
+	client, _ := zmq.NewSocket(zmq.REQ)
+	defer client.Close()
+	client.Connect("ipc://" + self + "-localfe.ipc")
+	monitor, _ := zmq.NewSocket(zmq.PUSH)
+	defer monitor.Close()
+	monitor.Connect("ipc://" + self + "-monitor.ipc")
+
+	poller := zmq.NewPoller()
+	poller.Add(client, zmq.POLLIN)
+	for {
+		time.Sleep(time.Duration(rand.Intn(5000)) * time.Millisecond)
+		for burst := rand.Intn(15); burst > 0; burst-- {
+			task_id := fmt.Sprintf("%04X-%s-%d", rand.Intn(0x10000), self, i)
+
+			//  Send request with random hex ID
+			client.Send(task_id, 0)
+
+			//  Wait max ten seconds for a reply, then complain
+			sockets, err := poller.Poll(10 * time.Second)
+			if err != nil {
+				break //  Interrupted
+			}
+
+			if len(sockets) == 1 {
+				reply, err := client.Recv(0)
+				if err != nil {
+					break //  Interrupted
+				}
+				//  Worker is supposed to answer us with our task id
+				id := strings.Fields(reply)[0]
+				if id != task_id {
+					panic("id != task_id")
+				}
+				monitor.Send(reply, 0)
+			} else {
+				monitor.Send("E: CLIENT EXIT - lost task "+task_id, 0)
+				return
+			}
+		}
+	}
+}
+
+//  This is the worker task, which uses a REQ socket to plug into the
+//  load-balancer. It's the same stub worker task you've seen in other
+//  examples:
+
+func worker_task(i int) {
+	worker, _ := zmq.NewSocket(zmq.REQ)
+	defer worker.Close()
+	worker.Connect("ipc://" + self + "-localbe.ipc")
+
+	//  Tell broker we're ready for work
+	worker.SendMessage(WORKER_READY)
+
+	//  Process messages as they arrive
+	for {
+		msg, err := worker.RecvMessage(0)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		//  Workers are busy for 0/1 seconds
+		time.Sleep(time.Duration(rand.Intn(2)) * time.Second)
+		n := len(msg) - 1
+		worker.SendMessage(msg[:n], fmt.Sprintf("%s %s-%d", msg[n], self, i))
+	}
+}
+
+//  The main task begins by setting-up all its sockets. The local frontend
+//  talks to clients, and our local backend talks to workers. The cloud
+//  frontend talks to peer brokers as if they were clients, and the cloud
+//  backend talks to peer brokers as if they were workers. The state
+//  backend publishes regular state messages, and the state frontend
+//  subscribes to all state backends to collect these messages. Finally,
+//  we use a PULL monitor socket to collect printable messages from tasks:
+
+func main() {
+	//  First argument is this broker's name
+	//  Other arguments are our peers' names
+	//
+	if len(os.Args) < 2 {
+		fmt.Println("syntax: peering1 me {you}...")
+		os.Exit(1)
+	}
+	self = os.Args[1]
+	fmt.Printf("I: preparing broker at %s...\n", self)
+	rand.Seed(time.Now().UnixNano())
+
+	//  Prepare local frontend and backend
+	localfe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer localfe.Close()
+	localfe.Bind("ipc://" + self + "-localfe.ipc")
+
+	localbe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer localbe.Close()
+	localbe.Bind("ipc://" + self + "-localbe.ipc")
+
+	//  Bind cloud frontend to endpoint
+	cloudfe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer cloudfe.Close()
+	cloudfe.SetIdentity(self)
+	cloudfe.Bind("ipc://" + self + "-cloud.ipc")
+
+	//  Connect cloud backend to all peers
+	cloudbe, _ := zmq.NewSocket(zmq.ROUTER)
+	defer cloudbe.Close()
+	cloudbe.SetIdentity(self)
+	for _, peer := range os.Args[2:] {
+		fmt.Printf("I: connecting to cloud frontend at '%s'\n", peer)
+		cloudbe.Connect("ipc://" + peer + "-cloud.ipc")
+	}
+	//  Bind state backend to endpoint
+	statebe, _ := zmq.NewSocket(zmq.PUB)
+	defer statebe.Close()
+	statebe.Bind("ipc://" + self + "-state.ipc")
+
+	//  Connect state frontend to all peers
+	statefe, _ := zmq.NewSocket(zmq.SUB)
+	defer statefe.Close()
+	statefe.SetSubscribe("")
+	for _, peer := range os.Args[2:] {
+		fmt.Printf("I: connecting to state backend at '%s'\n", peer)
+		statefe.Connect("ipc://" + peer + "-state.ipc")
+	}
+	//  Prepare monitor socket
+	monitor, _ := zmq.NewSocket(zmq.PULL)
+	defer monitor.Close()
+	monitor.Bind("ipc://" + self + "-monitor.ipc")
+
+	//  After binding and connecting all our sockets, we start our child
+	//  tasks - workers and clients:
+
+	for worker_nbr := 0; worker_nbr < NBR_WORKERS; worker_nbr++ {
+		go worker_task(worker_nbr)
+	}
+
+	//  Start local clients
+	for client_nbr := 0; client_nbr < NBR_CLIENTS; client_nbr++ {
+		go client_task(client_nbr)
+	}
+
+	//  Queue of available workers
+	local_capacity := 0
+	cloud_capacity := 0
+	workers := make([]string, 0)
+
+	primary := zmq.NewPoller()
+	primary.Add(localbe, zmq.POLLIN)
+	primary.Add(cloudbe, zmq.POLLIN)
+	primary.Add(statefe, zmq.POLLIN)
+	primary.Add(monitor, zmq.POLLIN)
+
+	secondary1 := zmq.NewPoller()
+	secondary1.Add(localfe, zmq.POLLIN)
+	secondary2 := zmq.NewPoller()
+	secondary2.Add(localfe, zmq.POLLIN)
+	secondary2.Add(cloudfe, zmq.POLLIN)
+
+	msg := make([]string, 0)
+	for {
+
+		//  If we have no workers ready, wait indefinitely
+		timeout := time.Duration(time.Second)
+		if local_capacity == 0 {
+			timeout = -1
+		}
+		sockets, err := primary.PollAll(timeout)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		//  Track if capacity changes during this iteration
+		previous := local_capacity
+
+		//  Handle reply from local worker
+		msg = msg[0:0]
+
+		if sockets[0].Events&zmq.POLLIN != 0 { // 0 == localbe
+			msg, err = localbe.RecvMessage(0)
+			if err != nil {
+				break //  Interrupted
+			}
+			var identity string
+			identity, msg = unwrap(msg)
+			workers = append(workers, identity)
+			local_capacity++
+
+			//  If it's READY, don't route the message any further
+			if msg[0] == WORKER_READY {
+				msg = msg[0:0]
+			}
+		} else if sockets[1].Events&zmq.POLLIN != 0 { // 1 == cloudbe
+			//  Or handle reply from peer broker
+			msg, err = cloudbe.RecvMessage(0)
+			if err != nil {
+				break //  Interrupted
+			}
+			//  We don't use peer broker identity for anything
+			_, msg = unwrap(msg)
+		}
+
+		if len(msg) > 0 {
+
+			//  Route reply to cloud if it's addressed to a broker
+			to_broker := false
+			for _, peer := range os.Args[2:] {
+				if peer == msg[0] {
+					to_broker = true
+					break
+				}
+			}
+			if to_broker {
+				cloudfe.SendMessage(msg)
+			} else {
+				localfe.SendMessage(msg)
+			}
+		}
+
+		//  If we have input messages on our statefe or monitor sockets we
+		//  can process these immediately:
+
+		if sockets[2].Events&zmq.POLLIN != 0 { // 2 == statefe
+			var status string
+			m, _ := statefe.RecvMessage(0)
+			_, m = unwrap(m) // peer
+			status, _ = unwrap(m)
+			cloud_capacity, _ = strconv.Atoi(status)
+		}
+		if sockets[3].Events&zmq.POLLIN != 0 { // 3 == monitor
+			status, _ := monitor.Recv(0)
+			fmt.Println(status)
+		}
+		//  Now route as many clients requests as we can handle. If we have
+		//  local capacity we poll both localfe and cloudfe. If we have cloud
+		//  capacity only, we poll just localfe. We route any request locally
+		//  if we can, else we route to the cloud.
+
+		for local_capacity+cloud_capacity > 0 {
+			var sockets []zmq.Polled
+			var err error
+			if local_capacity > 0 {
+				sockets, err = secondary2.PollAll(0)
+			} else {
+				sockets, err = secondary1.PollAll(0)
+			}
+			if err != nil {
+				panic(err)
+			}
+
+			if sockets[0].Events&zmq.POLLIN != 0 { // 0 == localfe
+				msg, _ = localfe.RecvMessage(0)
+			} else if len(sockets) > 1 && sockets[1].Events&zmq.POLLIN != 0 { // 1 == cloudfe
+				msg, _ = cloudfe.RecvMessage(0)
+			} else {
+				break //  No work, go back to primary
+			}
+
+			if local_capacity > 0 {
+				localbe.SendMessage(workers[0], "", msg)
+				workers = workers[1:]
+				local_capacity--
+			} else {
+				//  Route to random broker peer
+				random_peer := rand.Intn(len(os.Args)-2) + 2
+				cloudbe.SendMessage(os.Args[random_peer], "", msg)
+			}
+		}
+		//  We broadcast capacity messages to other peers; to reduce chatter
+		//  we do this only if our capacity changed.
+
+		if local_capacity != previous {
+			//  We stick our own identity onto the envelope
+			//  Broadcast new capacity
+			statebe.SendMessage(self, "", local_capacity)
+		}
+	}
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/ppqueue.go b/vendor/src/github.com/pebbe/zmq2/examples/ppqueue.go
new file mode 100644
index 0000000..fdc2310
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/ppqueue.go
@@ -0,0 +1,166 @@
+//
+//  Paranoid Pirate queue.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+const (
+	HEARTBEAT_LIVENESS = 3                       //  3-5 is reasonable
+	HEARTBEAT_INTERVAL = 1000 * time.Millisecond //  msecs
+
+	PPP_READY     = "\001" //  Signals worker is ready
+	PPP_HEARTBEAT = "\002" //  Signals worker heartbeat
+)
+
+//  Here we define the worker class; a structure and a set of functions that
+//  as constructor, destructor, and methods on worker objects:
+
+type worker_t struct {
+	identity  string    //  Identity of worker
+	id_string string    //  Printable identity
+	expire    time.Time //  Expires at this time
+}
+
+//  Construct new worker
+func s_worker_new(identity string) worker_t {
+	return worker_t{
+		identity:  identity,
+		id_string: identity,
+		expire:    time.Now().Add(HEARTBEAT_INTERVAL * HEARTBEAT_LIVENESS),
+	}
+}
+
+//  The ready method puts a worker to the end of the ready list:
+
+func s_worker_ready(self worker_t, workers []worker_t) []worker_t {
+	for i, worker := range workers {
+		if self.id_string == worker.id_string {
+			if i == 0 {
+				workers = workers[1:]
+			} else if i == len(workers)-1 {
+				workers = workers[:i-1]
+			} else {
+				workers = append(workers[:i-1], workers[i:]...)
+			}
+			break
+		}
+	}
+	return append(workers, self)
+}
+
+//  The purge method looks for and kills expired workers. We hold workers
+//  from oldest to most recent, so we stop at the first alive worker:
+
+func s_workers_purge(workers []worker_t) []worker_t {
+	now := time.Now()
+	for i, worker := range workers {
+		if now.Before(worker.expire) {
+			return workers[i:] //  Worker is alive, we're done here
+		}
+	}
+	return workers[0:0]
+}
+
+//  The main task is a load-balancer with heartbeating on workers so we
+//  can detect crashed or blocked worker tasks:
+
+func main() {
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	backend, _ := zmq.NewSocket(zmq.ROUTER)
+	defer frontend.Close()
+	defer backend.Close()
+	frontend.Bind("tcp://*:5555") //  For clients
+	backend.Bind("tcp://*:5556")  //  For workers
+
+	//  List of available workers
+	workers := make([]worker_t, 0)
+
+	//  Send out heartbeats at regular intervals
+	heartbeat_at := time.Tick(HEARTBEAT_INTERVAL)
+
+	poller1 := zmq.NewPoller()
+	poller1.Add(backend, zmq.POLLIN)
+	poller2 := zmq.NewPoller()
+	poller2.Add(backend, zmq.POLLIN)
+	poller2.Add(frontend, zmq.POLLIN)
+
+	for {
+		//  Poll frontend only if we have available workers
+		var sockets []zmq.Polled
+		var err error
+		if len(workers) > 0 {
+			sockets, err = poller2.Poll(HEARTBEAT_INTERVAL)
+		} else {
+			sockets, err = poller1.Poll(HEARTBEAT_INTERVAL)
+		}
+		if err != nil {
+			break //  Interrupted
+		}
+
+		for _, socket := range sockets {
+			switch socket.Socket {
+			case backend:
+				//  Handle worker activity on backend
+				//  Use worker identity for load-balancing
+				msg, err := backend.RecvMessage(0)
+				if err != nil {
+					break //  Interrupted
+				}
+
+				//  Any sign of life from worker means it's ready
+				identity, msg := unwrap(msg)
+				workers = s_worker_ready(s_worker_new(identity), workers)
+
+				//  Validate control message, or return reply to client
+				if len(msg) == 1 {
+					if msg[0] != PPP_READY && msg[0] != PPP_HEARTBEAT {
+						fmt.Println("E: invalid message from worker", msg)
+					}
+				} else {
+					frontend.SendMessage(msg)
+				}
+			case frontend:
+				//  Now get next client request, route to next worker
+				msg, err := frontend.RecvMessage(0)
+				if err != nil {
+					break //  Interrupted
+				}
+				backend.SendMessage(workers[0].identity, msg)
+				workers = workers[1:]
+			}
+		}
+
+		//  We handle heartbeating after any socket activity. First we send
+		//  heartbeats to any idle workers if it's time. Then we purge any
+		//  dead workers:
+
+		select {
+		case <-heartbeat_at:
+			for _, worker := range workers {
+				backend.SendMessage(worker.identity, PPP_HEARTBEAT)
+			}
+		default:
+		}
+		workers = s_workers_purge(workers)
+	}
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/ppworker.go b/vendor/src/github.com/pebbe/zmq2/examples/ppworker.go
new file mode 100644
index 0000000..5ba0349
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/ppworker.go
@@ -0,0 +1,130 @@
+//
+//  Paranoid Pirate worker.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+const (
+	HEARTBEAT_LIVENESS = 3                        //  3-5 is reasonable
+	HEARTBEAT_INTERVAL = 1000 * time.Millisecond  //  msecs
+	INTERVAL_INIT      = 1000 * time.Millisecond  //  Initial reconnect
+	INTERVAL_MAX       = 32000 * time.Millisecond //  After exponential backoff
+
+	//  Paranoid Pirate Protocol constants
+	PPP_READY     = "\001" //  Signals worker is ready
+	PPP_HEARTBEAT = "\002" //  Signals worker heartbeat
+)
+
+//  Helper function that returns a new configured socket
+//  connected to the Paranoid Pirate queue
+
+func s_worker_socket() (*zmq.Socket, *zmq.Poller) {
+	worker, _ := zmq.NewSocket(zmq.DEALER)
+	worker.Connect("tcp://localhost:5556")
+
+	//  Tell queue we're ready for work
+	fmt.Println("I: worker ready")
+	worker.Send(PPP_READY, 0)
+
+	poller := zmq.NewPoller()
+	poller.Add(worker, zmq.POLLIN)
+
+	return worker, poller
+}
+
+//  We have a single task, which implements the worker side of the
+//  Paranoid Pirate Protocol (PPP). The interesting parts here are
+//  the heartbeating, which lets the worker detect if the queue has
+//  died, and vice-versa:
+
+func main() {
+	worker, poller := s_worker_socket()
+
+	//  If liveness hits zero, queue is considered disconnected
+	liveness := HEARTBEAT_LIVENESS
+	interval := INTERVAL_INIT
+
+	//  Send out heartbeats at regular intervals
+	heartbeat_at := time.Tick(HEARTBEAT_INTERVAL)
+
+	rand.Seed(time.Now().UnixNano())
+	for cycles := 0; true; {
+		sockets, err := poller.Poll(HEARTBEAT_INTERVAL)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		if len(sockets) == 1 {
+			//  Get message
+			//  - 3-part envelope + content -> request
+			//  - 1-part HEARTBEAT -> heartbeat
+			msg, err := worker.RecvMessage(0)
+			if err != nil {
+				break //  Interrupted
+			}
+
+			//  To test the robustness of the queue implementation we //
+			//  simulate various typical problems, such as the worker
+			//  crashing, or running very slowly. We do this after a few
+			//  cycles so that the architecture can get up and running
+			//  first:
+			if len(msg) == 3 {
+				cycles++
+				if cycles > 3 && rand.Intn(5) == 0 {
+					fmt.Println("I: simulating a crash")
+					break
+				} else if cycles > 3 && rand.Intn(5) == 0 {
+					fmt.Println("I: simulating CPU overload")
+					time.Sleep(3 * time.Second)
+				}
+				fmt.Println("I: normal reply")
+				worker.SendMessage(msg)
+				liveness = HEARTBEAT_LIVENESS
+				time.Sleep(time.Second) //  Do some heavy work
+			} else if len(msg) == 1 {
+				//  When we get a heartbeat message from the queue, it means the
+				//  queue was (recently) alive, so reset our liveness indicator:
+				if msg[0] == PPP_HEARTBEAT {
+					liveness = HEARTBEAT_LIVENESS
+				} else {
+					fmt.Printf("E: invalid message: %q\n", msg)
+				}
+			} else {
+				fmt.Printf("E: invalid message: %q\n", msg)
+			}
+			interval = INTERVAL_INIT
+		} else {
+			//  If the queue hasn't sent us heartbeats in a while, destroy the
+			//  socket and reconnect. This is the simplest most brutal way of
+			//  discarding any messages we might have sent in the meantime://
+			liveness--
+			if liveness == 0 {
+				fmt.Println("W: heartbeat failure, can't reach queue")
+				fmt.Println("W: reconnecting in", interval)
+				time.Sleep(interval)
+
+				if interval < INTERVAL_MAX {
+					interval = 2 * interval
+				}
+				worker, poller = s_worker_socket()
+				liveness = HEARTBEAT_LIVENESS
+			}
+		}
+
+		//  Send heartbeat to queue if it's time
+		select {
+		case <-heartbeat_at:
+			fmt.Println("I: worker heartbeat")
+			worker.Send(PPP_HEARTBEAT, 0)
+		default:
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/psenvpub.go b/vendor/src/github.com/pebbe/zmq2/examples/psenvpub.go
new file mode 100644
index 0000000..149a802
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/psenvpub.go
@@ -0,0 +1,27 @@
+//
+//  Pubsub envelope publisher.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"time"
+)
+
+func main() {
+	//  Prepare our publisher
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	defer publisher.Close()
+	publisher.Bind("tcp://*:5563")
+
+	for {
+		//  Write two messages, each with an envelope and content
+		publisher.Send("A", zmq.SNDMORE)
+		publisher.Send("We don't want to see this", 0)
+		publisher.Send("B", zmq.SNDMORE)
+		publisher.Send("We would like to see this", 0)
+		time.Sleep(time.Second)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/psenvsub.go b/vendor/src/github.com/pebbe/zmq2/examples/psenvsub.go
new file mode 100644
index 0000000..a3ac500
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/psenvsub.go
@@ -0,0 +1,27 @@
+//
+//  Pubsub envelope subscriber.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+)
+
+func main() {
+	//  Prepare our subscriber
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	defer subscriber.Close()
+	subscriber.Connect("tcp://localhost:5563")
+	subscriber.SetSubscribe("B")
+
+	for {
+		//  Read envelope with address
+		address, _ := subscriber.Recv(0)
+		//  Read message contents
+		contents, _ := subscriber.Recv(0)
+		fmt.Printf("[%s] %s\n", address, contents)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/rrbroker.go b/vendor/src/github.com/pebbe/zmq2/examples/rrbroker.go
new file mode 100644
index 0000000..d05ff51
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/rrbroker.go
@@ -0,0 +1,53 @@
+//
+//  Simple request-reply broker.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+)
+
+func main() {
+	//  Prepare our sockets
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	defer frontend.Close()
+	backend, _ := zmq.NewSocket(zmq.DEALER)
+	defer backend.Close()
+	frontend.Bind("tcp://*:5559")
+	backend.Bind("tcp://*:5560")
+
+	//  Initialize poll set
+	poller := zmq.NewPoller()
+	poller.Add(frontend, zmq.POLLIN)
+	poller.Add(backend, zmq.POLLIN)
+
+	//  Switch messages between sockets
+	for {
+		sockets, _ := poller.Poll(-1)
+		for _, socket := range sockets {
+			switch s := socket.Socket; s {
+			case frontend:
+				for {
+					msg, _ := s.Recv(0)
+					if more, _ := s.GetRcvmore(); more {
+						backend.Send(msg, zmq.SNDMORE)
+					} else {
+						backend.Send(msg, 0)
+						break
+					}
+				}
+			case backend:
+				for {
+					msg, _ := s.Recv(0)
+					if more, _ := s.GetRcvmore(); more {
+						frontend.Send(msg, zmq.SNDMORE)
+					} else {
+						frontend.Send(msg, 0)
+						break
+					}
+				}
+			}
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/rrclient.go b/vendor/src/github.com/pebbe/zmq2/examples/rrclient.go
new file mode 100644
index 0000000..acbced2
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/rrclient.go
@@ -0,0 +1,25 @@
+//
+//  Request-reply client.
+//  Connects REQ socket to tcp://localhost:5559
+//  Sends "Hello" to server, expects "World" back
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+)
+
+func main() {
+	requester, _ := zmq.NewSocket(zmq.REQ)
+	defer requester.Close()
+	requester.Connect("tcp://localhost:5559")
+
+	for request := 0; request < 10; request++ {
+		requester.Send("Hello", 0)
+		reply, _ := requester.Recv(0)
+		fmt.Printf("Received reply %d [%s]\n", request, reply)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/rrworker.go b/vendor/src/github.com/pebbe/zmq2/examples/rrworker.go
new file mode 100644
index 0000000..326d4db
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/rrworker.go
@@ -0,0 +1,33 @@
+//
+//  Hello World worker.
+//  Connects REP socket to tcp://*:5560
+//  Expects "Hello" from client, replies with "World"
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	//  Socket to talk to clients
+	responder, _ := zmq.NewSocket(zmq.REP)
+	defer responder.Close()
+	responder.Connect("tcp://localhost:5560")
+
+	for {
+		//  Wait for next request from client
+		request, _ := responder.Recv(0)
+		fmt.Printf("Received request: [%s]\n", request)
+
+		//  Do some 'work'
+		time.Sleep(time.Second)
+
+		//  Send reply back to client
+		responder.Send("World", 0)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/rtdealer.go b/vendor/src/github.com/pebbe/zmq2/examples/rtdealer.go
new file mode 100644
index 0000000..53b3ef1
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/rtdealer.go
@@ -0,0 +1,84 @@
+//
+//  ROUTER-to-DEALER example.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+const (
+	NBR_WORKERS = 10
+)
+
+func worker_task() {
+	worker, _ := zmq.NewSocket(zmq.DEALER)
+	defer worker.Close()
+	set_id(worker) //  Set a printable identity
+	worker.Connect("tcp://localhost:5671")
+
+	total := 0
+	for {
+		//  Tell the broker we're ready for work
+		worker.Send("", zmq.SNDMORE)
+		worker.Send("Hi Boss", 0)
+
+		//  Get workload from broker, until finished
+		worker.Recv(0) //  Envelope delimiter
+		workload, _ := worker.Recv(0)
+		if workload == "Fired!" {
+			fmt.Printf("Completed: %d tasks\n", total)
+			break
+		}
+		total++
+
+		//  Do some random work
+		time.Sleep(time.Duration(rand.Intn(500)+1) * time.Millisecond)
+	}
+}
+
+func main() {
+	broker, _ := zmq.NewSocket(zmq.ROUTER)
+	defer broker.Close()
+
+	broker.Bind("tcp://*:5671")
+	rand.Seed(time.Now().UnixNano())
+
+	for worker_nbr := 0; worker_nbr < NBR_WORKERS; worker_nbr++ {
+		go worker_task()
+	}
+	//  Run for five seconds and then tell workers to end
+	start_time := time.Now()
+	workers_fired := 0
+	for {
+		//  Next message gives us least recently used worker
+		identity, _ := broker.Recv(0)
+		broker.Send(identity, zmq.SNDMORE)
+		broker.Recv(0) //  Envelope delimiter
+		broker.Recv(0) //  Response from worker
+		broker.Send("", zmq.SNDMORE)
+
+		//  Encourage workers until it's time to fire them
+		if time.Since(start_time) < 5*time.Second {
+			broker.Send("Work harder", 0)
+		} else {
+			broker.Send("Fired!", 0)
+			workers_fired++
+			if workers_fired == NBR_WORKERS {
+				break
+			}
+		}
+	}
+
+	time.Sleep(time.Second)
+}
+
+func set_id(soc *zmq.Socket) {
+	identity := fmt.Sprintf("%04X-%04X", rand.Intn(0x10000), rand.Intn(0x10000))
+	soc.SetIdentity(identity)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/rtreq.go b/vendor/src/github.com/pebbe/zmq2/examples/rtreq.go
new file mode 100644
index 0000000..fdd01fe
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/rtreq.go
@@ -0,0 +1,82 @@
+//
+//  ROUTER-to-REQ example.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+const (
+	NBR_WORKERS = 10
+)
+
+func worker_task() {
+	worker, _ := zmq.NewSocket(zmq.REQ)
+	defer worker.Close()
+	set_id(worker)
+	worker.Connect("tcp://localhost:5671")
+
+	total := 0
+	for {
+		//  Tell the broker we're ready for work
+		worker.Send("Hi Boss", 0)
+
+		//  Get workload from broker, until finished
+		workload, _ := worker.Recv(0)
+		if workload == "Fired!" {
+			fmt.Printf("Completed: %d tasks\n", total)
+			break
+		}
+		total++
+
+		//  Do some random work
+		time.Sleep(time.Duration(rand.Intn(500)+1) * time.Millisecond)
+	}
+}
+
+func main() {
+	broker, _ := zmq.NewSocket(zmq.ROUTER)
+	defer broker.Close()
+
+	broker.Bind("tcp://*:5671")
+	rand.Seed(time.Now().UnixNano())
+
+	for worker_nbr := 0; worker_nbr < NBR_WORKERS; worker_nbr++ {
+		go worker_task()
+	}
+	//  Run for five seconds and then tell workers to end
+	start_time := time.Now()
+	workers_fired := 0
+	for {
+		//  Next message gives us least recently used worker
+		identity, _ := broker.Recv(0)
+		broker.Send(identity, zmq.SNDMORE)
+		broker.Recv(0) //  Envelope delimiter
+		broker.Recv(0) //  Response from worker
+		broker.Send("", zmq.SNDMORE)
+
+		//  Encourage workers until it's time to fire them
+		if time.Since(start_time) < 5*time.Second {
+			broker.Send("Work harder", 0)
+		} else {
+			broker.Send("Fired!", 0)
+			workers_fired++
+			if workers_fired == NBR_WORKERS {
+				break
+			}
+		}
+	}
+
+	time.Sleep(time.Second)
+}
+
+func set_id(soc *zmq.Socket) {
+	identity := fmt.Sprintf("%04X-%04X", rand.Intn(0x10000), rand.Intn(0x10000))
+	soc.SetIdentity(identity)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/spqueue.go b/vendor/src/github.com/pebbe/zmq2/examples/spqueue.go
new file mode 100644
index 0000000..d8d2d28
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/spqueue.go
@@ -0,0 +1,88 @@
+//
+//  Simple Pirate broker.
+//  This is identical to load-balancing pattern, with no reliability
+//  mechanisms. It depends on the client for recovery. Runs forever.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+)
+
+const (
+	WORKER_READY = "\001" //  Signals worker is ready
+)
+
+func main() {
+	frontend, _ := zmq.NewSocket(zmq.ROUTER)
+	backend, _ := zmq.NewSocket(zmq.ROUTER)
+	defer frontend.Close()
+	defer backend.Close()
+	frontend.Bind("tcp://*:5555") //  For clients
+	backend.Bind("tcp://*:5556")  //  For workers
+
+	//  Queue of available workers
+	workers := make([]string, 0)
+
+	poller1 := zmq.NewPoller()
+	poller1.Add(backend, zmq.POLLIN)
+	poller2 := zmq.NewPoller()
+	poller2.Add(backend, zmq.POLLIN)
+	poller2.Add(frontend, zmq.POLLIN)
+
+	//  The body of this example is exactly the same as lbbroker2.
+LOOP:
+	for {
+		//  Poll frontend only if we have available workers
+		var sockets []zmq.Polled
+		var err error
+		if len(workers) > 0 {
+			sockets, err = poller2.Poll(-1)
+		} else {
+			sockets, err = poller1.Poll(-1)
+		}
+		if err != nil {
+			break //  Interrupted
+		}
+		for _, socket := range sockets {
+			switch s := socket.Socket; s {
+			case backend: //  Handle worker activity on backend
+				//  Use worker identity for load-balancing
+				msg, err := s.RecvMessage(0)
+				if err != nil {
+					break LOOP //  Interrupted
+				}
+				var identity string
+				identity, msg = unwrap(msg)
+				workers = append(workers, identity)
+
+				//  Forward message to client if it's not a READY
+				if msg[0] != WORKER_READY {
+					frontend.SendMessage(msg)
+				}
+
+			case frontend:
+				//  Get client request, route to first available worker
+				msg, err := s.RecvMessage(0)
+				if err == nil {
+					backend.SendMessage(workers[0], "", msg)
+					workers = workers[1:]
+				}
+			}
+		}
+	}
+}
+
+//  Pops frame off front of message and returns it as 'head'
+//  If next frame is empty, pops that empty frame.
+//  Return remaining frames of message as 'tail'
+func unwrap(msg []string) (head string, tail []string) {
+	head = msg[0]
+	if len(msg) > 1 && msg[1] == "" {
+		tail = msg[2:]
+	} else {
+		tail = msg[1:]
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/spworker.go b/vendor/src/github.com/pebbe/zmq2/examples/spworker.go
new file mode 100644
index 0000000..e60079c
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/spworker.go
@@ -0,0 +1,55 @@
+//
+//  Simple Pirate worker.
+//  Connects REQ socket to tcp://*:5556
+//  Implements worker part of load-balancing
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+const (
+	WORKER_READY = "\001" //  Signals worker is ready
+)
+
+func main() {
+	worker, _ := zmq.NewSocket(zmq.REQ)
+	defer worker.Close()
+
+	//  Set random identity to make tracing easier
+	rand.Seed(time.Now().UnixNano())
+	identity := fmt.Sprintf("%04X-%04X", rand.Intn(0x10000), rand.Intn(0x10000))
+	worker.SetIdentity(identity)
+	worker.Connect("tcp://localhost:5556")
+
+	//  Tell broker we're ready for work
+	fmt.Printf("I: (%s) worker ready\n", identity)
+	worker.Send(WORKER_READY, 0)
+
+	for cycles := 0; true; {
+		msg, err := worker.RecvMessage(0)
+		if err != nil {
+			break //  Interrupted
+		}
+
+		//  Simulate various problems, after a few cycles
+		cycles++
+		if cycles > 3 && rand.Intn(5) == 0 {
+			fmt.Printf("I: (%s) simulating a crash\n", identity)
+			break
+		} else if cycles > 3 && rand.Intn(5) == 0 {
+			fmt.Printf("I: (%s) simulating CPU overload\n", identity)
+			time.Sleep(3 * time.Second)
+		}
+
+		fmt.Printf("I: (%s) normal reply\n", identity)
+		time.Sleep(time.Second) //  Do some heavy work
+		worker.SendMessage(msg)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/suisnail.go b/vendor/src/github.com/pebbe/zmq2/examples/suisnail.go
new file mode 100644
index 0000000..3719fbc
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/suisnail.go
@@ -0,0 +1,83 @@
+//
+//  Suicidal Snail
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"log"
+	"math/rand"
+	"strconv"
+	"time"
+)
+
+//  This is our subscriber. It connects to the publisher and subscribes to
+//  everything. It sleeps for a short time between messages to simulate doing
+//  too much work. If a message is more than 1 second late, it croaks:
+
+const (
+	MAX_ALLOWED_DELAY = 1000 * time.Millisecond
+)
+
+func subscriber(pipe chan<- string) {
+	//  Subscribe to everything
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	subscriber.SetSubscribe("")
+	subscriber.Connect("tcp://localhost:5556")
+	defer subscriber.Close()
+
+	//  Get and process messages
+	for {
+		msg, _ := subscriber.RecvMessage(0)
+		i, _ := strconv.Atoi(msg[0])
+		clock := time.Unix(int64(i), 0)
+		fmt.Println(clock)
+
+		//  Suicide snail logic
+		if time.Now().After(clock.Add(MAX_ALLOWED_DELAY)) {
+			log.Println("E: subscriber cannot keep up, aborting")
+			break
+		}
+		//  Work for 1 msec plus some random additional time
+		time.Sleep(time.Duration(1 + rand.Intn(2)))
+	}
+	pipe <- "gone and died"
+}
+
+//  This is our publisher task. It publishes a time-stamped message to its
+//  PUB socket every 1 msec:
+
+func publisher(pipe <-chan string) {
+	//  Prepare publisher
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	publisher.Bind("tcp://*:5556")
+	defer publisher.Close()
+
+LOOP:
+	for {
+		//  Send current clock (msecs) to subscribers
+		publisher.SendMessage(time.Now().Unix())
+		select {
+		case <-pipe:
+			break LOOP
+		default:
+		}
+		time.Sleep(time.Millisecond)
+	}
+}
+
+//  The main task simply starts a client, and a server, and then
+//  waits for the client to signal that it has died:
+
+func main() {
+	pubpipe := make(chan string)
+	subpipe := make(chan string)
+	go publisher(pubpipe)
+	go subscriber(subpipe)
+	<-subpipe
+	pubpipe <- "break"
+	time.Sleep(100 * time.Millisecond)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/sync.sh b/vendor/src/github.com/pebbe/zmq2/examples/sync.sh
new file mode 100755
index 0000000..2ac6135
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/sync.sh
@@ -0,0 +1,12 @@
+#!/bin/sh
+echo "Starting subscribers..."
+for i in 1 2 3 4 5 6 7 8 9 10
+do
+    ./syncsub &
+done
+echo "Starting publisher..."
+./syncpub
+# have all subscribers finished?
+sleep 1
+echo Still running instances of syncsub:
+ps | grep syncsub
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/syncpub.go b/vendor/src/github.com/pebbe/zmq2/examples/syncpub.go
new file mode 100644
index 0000000..d14c307
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/syncpub.go
@@ -0,0 +1,57 @@
+//
+//  Synchronized publisher.
+//
+//  This diverts from the C example by introducing time delays.
+//  Without these delays, the subscribers won't catch the END message.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+const (
+	//  We wait for 10 subscribers
+	SUBSCRIBERS_EXPECTED = 10
+)
+
+func main() {
+
+	//  Socket to talk to clients
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	defer publisher.Close()
+	publisher.Bind("tcp://*:5561")
+
+	//  Socket to receive signals
+	syncservice, _ := zmq.NewSocket(zmq.REP)
+	defer syncservice.Close()
+	syncservice.Bind("tcp://*:5562")
+
+	//  Get synchronization from subscribers
+	fmt.Println("Waiting for subscribers")
+	for subscribers := 0; subscribers < SUBSCRIBERS_EXPECTED; subscribers++ {
+		//  - wait for synchronization request
+		syncservice.Recv(0)
+		//  - send synchronization reply
+		syncservice.Send("", 0)
+	}
+	//  Now broadcast exactly 1M updates followed by END
+	fmt.Println("Broadcasting messages")
+	for update_nbr := 0; update_nbr < 1000000; update_nbr++ {
+		publisher.Send("Rhubarb", 0)
+		// subscribers don't get all messages if publisher is too fast
+		// a one microsecond pause may still be too short
+		time.Sleep(time.Microsecond)
+	}
+
+	// a longer pause ensures subscribers are ready to receive this
+	time.Sleep(time.Second)
+	publisher.Send("END", 0)
+
+	// what's another second?
+	time.Sleep(time.Second)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/syncsub.go b/vendor/src/github.com/pebbe/zmq2/examples/syncsub.go
new file mode 100644
index 0000000..e75b04d
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/syncsub.go
@@ -0,0 +1,51 @@
+//
+//  Synchronized subscriber
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"log"
+	"time"
+)
+
+func main() {
+
+	//  First, connect our subscriber socket
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	defer subscriber.Close()
+	subscriber.Connect("tcp://localhost:5561")
+	subscriber.SetSubscribe("")
+
+	//  0MQ is so fast, we need to wait a while...
+	time.Sleep(time.Second)
+
+	//  Second, synchronize with publisher
+	syncclient, _ := zmq.NewSocket(zmq.REQ)
+	defer syncclient.Close()
+	syncclient.Connect("tcp://localhost:5562")
+
+	//  - send a synchronization request
+	syncclient.Send("", 0)
+
+	//  - wait for synchronization reply
+	syncclient.Recv(0)
+
+	//  Third, get our updates and report how many we got
+	update_nbr := 0
+	for {
+		msg, e := subscriber.Recv(0)
+		if e != nil {
+			log.Println(e)
+			break
+		}
+		if msg == "END" {
+			break
+		}
+		update_nbr++
+	}
+	fmt.Printf("Received %d updates\n", update_nbr)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/tasksink.go b/vendor/src/github.com/pebbe/zmq2/examples/tasksink.go
new file mode 100644
index 0000000..e7ef5e9
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/tasksink.go
@@ -0,0 +1,40 @@
+//
+//  Task sink.
+//  Binds PULL socket to tcp://localhost:5558
+//  Collects results from workers via that socket
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	//  Prepare our socket
+	receiver, _ := zmq.NewSocket(zmq.PULL)
+	defer receiver.Close()
+	receiver.Bind("tcp://*:5558")
+
+	//  Wait for start of batch
+	receiver.Recv(0)
+
+	//  Start our clock now
+	start_time := time.Now()
+
+	//  Process 100 confirmations
+	for task_nbr := 0; task_nbr < 100; task_nbr++ {
+		receiver.Recv(0)
+		if task_nbr%10 == 0 {
+			fmt.Print(":")
+		} else {
+			fmt.Print(".")
+		}
+	}
+
+	//  Calculate and report duration of batch
+	fmt.Println("\nTotal elapsed time:", time.Since(start_time))
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/tasksink2.go b/vendor/src/github.com/pebbe/zmq2/examples/tasksink2.go
new file mode 100644
index 0000000..2278bcf
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/tasksink2.go
@@ -0,0 +1,48 @@
+//
+//  Task sink - design 2.
+//  Adds pub-sub flow to send kill signal to workers
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+func main() {
+	//  Socket to receive messages on
+	receiver, _ := zmq.NewSocket(zmq.PULL)
+	defer receiver.Close()
+	receiver.Bind("tcp://*:5558")
+
+	//  Socket for worker control
+	controller, _ := zmq.NewSocket(zmq.PUB)
+	defer controller.Close()
+	controller.Bind("tcp://*:5559")
+
+	//  Wait for start of batch
+	receiver.Recv(0)
+
+	//  Start our clock now
+	start_time := time.Now()
+
+	//  Process 100 confirmations
+	for task_nbr := 0; task_nbr < 100; task_nbr++ {
+		receiver.Recv(0)
+		if task_nbr%10 == 0 {
+			fmt.Print(":")
+		} else {
+			fmt.Print(".")
+		}
+	}
+	fmt.Println("\nTotal elapsed time:", time.Since(start_time))
+
+	//  Send kill signal to workers
+	controller.Send("KILL", 0)
+
+	//  Finished
+	time.Sleep(time.Second) //  Give 0MQ time to deliver
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/taskvent.go b/vendor/src/github.com/pebbe/zmq2/examples/taskvent.go
new file mode 100644
index 0000000..f552631
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/taskvent.go
@@ -0,0 +1,51 @@
+//
+//  Task ventilator.
+//  Binds PUSH socket to tcp://localhost:5557
+//  Sends batch of tasks to workers via that socket
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+func main() {
+	//  Socket to send messages on
+	sender, _ := zmq.NewSocket(zmq.PUSH)
+	defer sender.Close()
+	sender.Bind("tcp://*:5557")
+
+	//  Socket to send start of batch message on
+	sink, _ := zmq.NewSocket(zmq.PUSH)
+	defer sink.Close()
+	sink.Connect("tcp://localhost:5558")
+
+	fmt.Print("Press Enter when the workers are ready: ")
+	var line string
+	fmt.Scanln(&line)
+	fmt.Println("Sending tasks to workers...")
+
+	//  The first message is "0" and signals start of batch
+	sink.Send("0", 0)
+
+	//  Initialize random number generator
+	rand.Seed(time.Now().UnixNano())
+
+	//  Send 100 tasks
+	total_msec := 0
+	for task_nbr := 0; task_nbr < 100; task_nbr++ {
+		//  Random workload from 1 to 100msecs
+		workload := rand.Intn(100) + 1
+		total_msec += workload
+		s := fmt.Sprintf("%d", workload)
+		sender.Send(s, 0)
+	}
+	fmt.Println("Total expected cost:", time.Duration(total_msec)*time.Millisecond)
+	time.Sleep(time.Second) //  Give 0MQ time to deliver
+
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/taskwork.go b/vendor/src/github.com/pebbe/zmq2/examples/taskwork.go
new file mode 100644
index 0000000..186b937
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/taskwork.go
@@ -0,0 +1,44 @@
+//
+//  Task worker.
+//  Connects PULL socket to tcp://localhost:5557
+//  Collects workloads from ventilator via that socket
+//  Connects PUSH socket to tcp://localhost:5558
+//  Sends results to sink via that socket
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"strconv"
+	"time"
+)
+
+func main() {
+	//  Socket to receive messages on
+	receiver, _ := zmq.NewSocket(zmq.PULL)
+	defer receiver.Close()
+	receiver.Connect("tcp://localhost:5557")
+
+	//  Socket to send messages to
+	sender, _ := zmq.NewSocket(zmq.PUSH)
+	defer sender.Close()
+	sender.Connect("tcp://localhost:5558")
+
+	//  Process tasks forever
+	for {
+		s, _ := receiver.Recv(0)
+
+		//  Simple progress indicator for the viewer
+		fmt.Print(s + ".")
+
+		//  Do the work
+		msec, _ := strconv.Atoi(s)
+		time.Sleep(time.Duration(msec) * time.Millisecond)
+
+		//  Send results to sink
+		sender.Send("", 0)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/taskwork2.go b/vendor/src/github.com/pebbe/zmq2/examples/taskwork2.go
new file mode 100644
index 0000000..cca9be6
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/taskwork2.go
@@ -0,0 +1,62 @@
+//
+//  Task worker - design 2.
+//  Adds pub-sub flow to receive and respond to kill signal
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"strconv"
+	"time"
+)
+
+func main() {
+	//  Socket to receive messages on
+	receiver, _ := zmq.NewSocket(zmq.PULL)
+	defer receiver.Close()
+	receiver.Connect("tcp://localhost:5557")
+
+	//  Socket to send messages to
+	sender, _ := zmq.NewSocket(zmq.PUSH)
+	defer sender.Close()
+	sender.Connect("tcp://localhost:5558")
+
+	//  Socket for control input
+	controller, _ := zmq.NewSocket(zmq.SUB)
+	defer controller.Close()
+	controller.Connect("tcp://localhost:5559")
+	controller.SetSubscribe("")
+
+	//  Process messages from receiver and controller
+	poller := zmq.NewPoller()
+	poller.Add(receiver, zmq.POLLIN)
+	poller.Add(controller, zmq.POLLIN)
+	//  Process messages from both sockets
+LOOP:
+	for {
+		sockets, _ := poller.Poll(-1)
+		for _, socket := range sockets {
+			switch s := socket.Socket; s {
+			case receiver:
+				msg, _ := s.Recv(0)
+
+				//  Do the work
+				t, _ := strconv.Atoi(msg)
+				time.Sleep(time.Duration(t) * time.Millisecond)
+
+				//  Send results to sink
+				sender.Send(msg, 0)
+
+				//  Simple progress indicator for the viewer
+				fmt.Printf(".")
+			case controller:
+				//  Any controller command acts as 'KILL'
+				break LOOP //  Exit loop
+			}
+		}
+	}
+	fmt.Println()
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/ticlient.go b/vendor/src/github.com/pebbe/zmq2/examples/ticlient.go
new file mode 100644
index 0000000..4c15e7e
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/ticlient.go
@@ -0,0 +1,81 @@
+//
+//  Titanic client example.
+//  Implements client side of http://rfc.zeromq.org/spec:9
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/mdapi"
+
+	"errors"
+	"fmt"
+	"os"
+	"time"
+)
+
+//  Calls a TSP service
+//  Returns response if successful (status code 200 OK), else NULL
+//
+func ServiceCall(session *mdapi.Mdcli, service string, request ...string) (reply []string, err error) {
+	reply = []string{}
+	msg, err := session.Send(service, request...)
+	if err == nil {
+		switch status := msg[0]; status {
+		case "200":
+			reply = msg[1:]
+			return
+		case "400":
+			fmt.Println("E: client fatal error, aborting")
+			os.Exit(1)
+		case "500":
+			fmt.Println("E: server fatal error, aborting")
+			os.Exit(1)
+		}
+	} else {
+		fmt.Println("E: " + err.Error())
+		os.Exit(0)
+	}
+
+	err = errors.New("Didn't succeed")
+	return //  Didn't succeed, don't care why not
+}
+
+//  The main task tests our service call by sending an echo request:
+
+func main() {
+	var verbose bool
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+	session, _ := mdapi.NewMdcli("tcp://localhost:5555", verbose)
+
+	//  1. Send 'echo' request to Titanic
+	reply, err := ServiceCall(session, "titanic.request", "echo", "Hello world")
+	if err != nil {
+		fmt.Println(err)
+		return
+	}
+
+	var uuid string
+	if err == nil {
+		uuid = reply[0]
+		fmt.Println("I: request UUID", uuid)
+	}
+
+	time.Sleep(100 * time.Millisecond)
+
+	//  2. Wait until we get a reply
+	for {
+		reply, err := ServiceCall(session, "titanic.reply", uuid)
+		if err == nil {
+			fmt.Println("Reply:", reply[0])
+
+			//  3. Close request
+			ServiceCall(session, "titanic.close", uuid)
+			break
+		} else {
+			fmt.Println("I: no reply yet, trying again...")
+			time.Sleep(5 * time.Second) //  Try again in 5 seconds
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/titanic.go b/vendor/src/github.com/pebbe/zmq2/examples/titanic.go
new file mode 100644
index 0000000..98b2e79
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/titanic.go
@@ -0,0 +1,235 @@
+//
+//  Titanic service.
+//
+//  Implements server side of http://rfc.zeromq.org/spec:9
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/mdapi"
+
+	"github.com/pborman/uuid"
+
+	"fmt"
+	"io/ioutil"
+	"os"
+	"strings"
+	"time"
+)
+
+//  Returns freshly allocated request filename for given UUID
+
+const (
+	TITANIC_DIR = ".titanic"
+)
+
+func RequestFilename(uuid string) string {
+	return TITANIC_DIR + "/" + uuid + "req"
+}
+
+//  Returns freshly allocated reply filename for given UUID
+
+func ReplyFilename(uuid string) string {
+	return TITANIC_DIR + "/" + uuid + "rep"
+}
+
+//  The "titanic.request" task waits for requests to this service. It writes
+//  each request to disk and returns a UUID to the client. The client picks
+//  up the reply asynchronously using the "titanic.reply" service:
+
+func TitanicRequest(chRequest chan<- string) {
+	worker, _ := mdapi.NewMdwrk("tcp://localhost:5555", "titanic.request", false)
+
+	reply := []string{}
+	for {
+		//  Send reply if it's not null
+		//  And then get next request from broker
+		request, err := worker.Recv(reply)
+		if err != nil {
+			break //  Interrupted, exit
+		}
+
+		//  Ensure message directory exists
+		os.MkdirAll(TITANIC_DIR, 0700)
+
+		//  Generate UUID and save message to disk
+		uuid := uuid.New()
+		file, err := os.Create(RequestFilename(uuid))
+		fmt.Fprint(file, strings.Join(request, "\n"))
+		file.Close()
+
+		//  Send UUID through to message queue
+		chRequest <- uuid
+
+		//  Now send UUID back to client
+		//  Done by the mdwrk_recv() at the top of the loop
+		reply = []string{"200", uuid}
+	}
+}
+
+//  The "titanic.reply" task checks if there's a reply for the specified
+//  request (by UUID), and returns a 200 OK, 300 Pending, or 400 Unknown
+//  accordingly:
+
+func TitanicReply() {
+	worker, _ := mdapi.NewMdwrk("tcp://localhost:5555", "titanic.reply", false)
+
+	pending := []string{"300"}
+	unknown := []string{"400"}
+	reply := []string{}
+	for {
+		request, err := worker.Recv(reply)
+		if err != nil {
+			break //  Interrupted, exit
+		}
+
+		uuid := request[0]
+		req_filename := RequestFilename(uuid)
+		rep_filename := ReplyFilename(uuid)
+		data, err := ioutil.ReadFile(rep_filename)
+		if err == nil {
+			reply = strings.Split("200\n"+string(data), "\n")
+		} else {
+			_, err := os.Stat(req_filename)
+			if err == nil {
+				reply = pending
+			} else {
+				reply = unknown
+			}
+		}
+	}
+}
+
+//  The "titanic.close" task removes any waiting replies for the request
+//  (specified by UUID). It's idempotent, so safe to call more than once
+//  in a row:
+
+func TitanicClose() {
+	worker, _ := mdapi.NewMdwrk("tcp://localhost:5555", "titanic.close", false)
+
+	ok := []string{"200"}
+	reply := []string{}
+	for {
+		request, err := worker.Recv(reply)
+		if err != nil {
+			break //  Interrupted, exit
+		}
+
+		uuid := request[0]
+		os.Remove(RequestFilename(uuid))
+		os.Remove(ReplyFilename(uuid))
+
+		reply = ok
+	}
+
+}
+
+//  This is the main thread for the Titanic worker. It starts three child
+//  threads; for the request, reply, and close services. It then dispatches
+//  requests to workers using a simple brute-force disk queue. It receives
+//  request UUIDs from the titanic.request service, saves these to a disk
+//  file, and then throws each request at MDP workers until it gets a
+//  response:
+
+func main() {
+	var verbose bool
+	if len(os.Args) > 1 && os.Args[1] == "-v" {
+		verbose = true
+	}
+
+	chRequest := make(chan string)
+	go TitanicRequest(chRequest)
+	go TitanicReply()
+	go TitanicClose()
+
+	//  Ensure message directory exists
+	os.MkdirAll(TITANIC_DIR, 0700)
+
+	// Fill the queue
+	queue := make([]string, 0)
+	files, err := ioutil.ReadDir(TITANIC_DIR)
+	if err == nil {
+		for _, file := range files {
+			name := file.Name()
+			if strings.HasSuffix(name, "req") {
+				uuid := name[:len(name)-3]
+				_, err := os.Stat(ReplyFilename(uuid))
+				if err != nil {
+					queue = append(queue, uuid)
+				}
+			}
+		}
+	}
+
+	//  Main dispatcher loop
+	for {
+		//  We'll dispatch once per second, if there's no activity
+		select {
+		case <-time.After(time.Second):
+		case uuid := <-chRequest:
+			//  Append UUID to queue
+			queue = append(queue, uuid)
+		}
+
+		//  Brute-force dispatcher
+		queue2 := make([]string, 0, len(queue))
+		for _, entry := range queue {
+			if verbose {
+				fmt.Println("I: processing request", entry)
+			}
+			if !ServiceSuccess(entry) {
+				queue2 = append(queue2, entry)
+			}
+		}
+		queue = queue2
+	}
+}
+
+//  Here we first check if the requested MDP service is defined or not,
+//  using a MMI lookup to the Majordomo broker. If the service exists
+//  we send a request and wait for a reply using the conventional MDP
+//  client API. This is not meant to be fast, just very simple:
+
+func ServiceSuccess(uuid string) bool {
+	// If reply already exists, treat as successful
+	_, err := os.Stat(ReplyFilename(uuid))
+	if err == nil {
+		return true
+	}
+
+	//  Load request message, service will be first frame
+	data, err := ioutil.ReadFile(RequestFilename(uuid))
+
+	//  If the client already closed request, treat as successful
+	if err != nil {
+		return true
+	}
+
+	request := strings.Split(string(data), "\n")
+
+	service_name := request[0]
+	request = request[1:]
+
+	//  Create MDP client session with short timeout
+	client, err := mdapi.NewMdcli("tcp://localhost:5555", false)
+	client.SetTimeout(time.Second) //  1 sec
+	client.SetRetries(1)           //  only 1 retry
+
+	//  Use MMI protocol to check if service is available
+	mmi_reply, err := client.Send("mmi.service", service_name)
+	if err != nil || mmi_reply[0] != "200" {
+		return false
+	}
+
+	reply, err := client.Send(service_name, request...)
+	if err != nil {
+		return false
+	}
+
+	file, err := os.Create(ReplyFilename(uuid))
+	fmt.Fprint(file, strings.Join(reply, "\n"))
+	file.Close()
+
+	return true
+
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/tripping.go b/vendor/src/github.com/pebbe/zmq2/examples/tripping.go
new file mode 100644
index 0000000..c3b911a
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/tripping.go
@@ -0,0 +1,82 @@
+//
+//  Round-trip demonstrator.
+//
+//  While this example runs in a single process, that is just to make
+//  it easier to start and stop the example. The client task signals to
+//  main when it's ready.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"time"
+)
+
+func ClientTask(pipe chan<- bool) {
+	client, _ := zmq.NewSocket(zmq.DEALER)
+	client.Connect("tcp://localhost:5555")
+	fmt.Println("Setting up test...")
+	time.Sleep(100 * time.Millisecond)
+
+	fmt.Println("Synchronous round-trip test...")
+	start := time.Now()
+	var requests int
+	for requests = 0; requests < 10000; requests++ {
+		client.Send("hello", 0)
+		client.Recv(0)
+	}
+	fmt.Println(requests, "calls in", time.Since(start))
+
+	fmt.Println("Asynchronous round-trip test...")
+	start = time.Now()
+	for requests = 0; requests < 100000; requests++ {
+		client.Send("hello", 0)
+	}
+	for requests = 0; requests < 100000; requests++ {
+		client.Recv(0)
+	}
+	fmt.Println(requests, "calls in", time.Since(start))
+	pipe <- true
+}
+
+//  Here is the worker task. All it does is receive a message, and
+//  bounce it back the way it came:
+
+func WorkerTask() {
+	worker, _ := zmq.NewSocket(zmq.DEALER)
+	worker.Connect("tcp://localhost:5556")
+
+	for {
+		msg, _ := worker.RecvMessage(0)
+		worker.SendMessage(msg)
+	}
+}
+
+//  Here is the broker task. It uses the zmq_proxy function to switch
+//  messages between frontend and backend:
+
+func BrokerTask() {
+	//  Prepare our sockets
+	frontend, _ := zmq.NewSocket(zmq.DEALER)
+	frontend.Bind("tcp://*:5555")
+	backend, _ := zmq.NewSocket(zmq.DEALER)
+	backend.Bind("tcp://*:5556")
+	zmq.Proxy(frontend, backend, nil)
+}
+
+//  Finally, here's the main task, which starts the client, worker, and
+//  broker, and then runs until the client signals it to stop:
+
+func main() {
+	//  Create threads
+	pipe := make(chan bool)
+	go ClientTask(pipe)
+	go WorkerTask()
+	go BrokerTask()
+
+	//  Wait for signal on client pipe
+	<-pipe
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/udpping1.go b/vendor/src/github.com/pebbe/zmq2/examples/udpping1.go
new file mode 100644
index 0000000..7264fc7
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/udpping1.go
@@ -0,0 +1,103 @@
+//
+//  UDP ping command
+//  Model 1, does UDP work inline
+//
+
+//  this doesn't use ZeroMQ at all
+
+package main
+
+import (
+	"fmt"
+	"log"
+	"syscall"
+	"time"
+)
+
+const (
+	PING_PORT_NUMBER = 9999
+	PING_MSG_SIZE    = 1
+	PING_INTERVAL    = 1000 * time.Millisecond //  Once per second
+)
+
+func main() {
+
+	log.SetFlags(log.Lshortfile)
+
+	//  Create UDP socket
+	fd, err := syscall.Socket(syscall.AF_INET, syscall.SOCK_DGRAM, syscall.IPPROTO_UDP)
+	if err != nil {
+		log.Fatalln(err)
+	}
+
+	//  Ask operating system to let us do broadcasts from socket
+	if err := syscall.SetsockoptInt(fd, syscall.SOL_SOCKET, syscall.SO_BROADCAST, 1); err != nil {
+		log.Fatalln(err)
+	}
+
+	//  Bind UDP socket to local port so we can receive pings
+	if err := syscall.Bind(fd, &syscall.SockaddrInet4{Port: PING_PORT_NUMBER, Addr: [4]byte{0, 0, 0, 0}}); err != nil {
+		log.Fatalln(err)
+	}
+
+	buffer := make([]byte, PING_MSG_SIZE)
+
+	//  We use syscall.Select to wait for activity on the UDP socket.
+	//  We send a beacon once a second, and we collect and report
+	//  beacons that come in from other nodes:
+
+	rfds := &syscall.FdSet{}
+	timeout := &syscall.Timeval{}
+
+	//  Send first ping right away
+	ping_at := time.Now()
+
+	bcast := &syscall.SockaddrInet4{Port: PING_PORT_NUMBER, Addr: [4]byte{255, 255, 255, 255}}
+	for {
+		dur := int64(ping_at.Sub(time.Now()) / time.Microsecond)
+		if dur < 0 {
+			dur = 0
+		}
+		timeout.Sec, timeout.Usec = dur/1000000, dur%1000000
+		FD_ZERO(rfds)
+		FD_SET(rfds, fd)
+		_, err := syscall.Select(fd+1, rfds, nil, nil, timeout)
+		if err != nil {
+			log.Fatalln(err)
+		}
+
+		//  Someone answered our ping
+		if FD_ISSET(rfds, fd) {
+			_, addr, err := syscall.Recvfrom(fd, buffer, 0)
+			if err != nil {
+				log.Fatalln(err)
+			}
+			a := addr.(*syscall.SockaddrInet4)
+			fmt.Printf("Found peer %v.%v.%v.%v:%v\n", a.Addr[0], a.Addr[1], a.Addr[2], a.Addr[3], a.Port)
+		}
+		if time.Now().After(ping_at) {
+			//  Broadcast our beacon
+			fmt.Println("Pinging peers...")
+			buffer[0] = '!'
+			if err := syscall.Sendto(fd, buffer, 0, bcast); err != nil {
+				log.Fatalln(err)
+			}
+			ping_at = time.Now().Add(PING_INTERVAL)
+		}
+	}
+
+}
+
+func FD_SET(p *syscall.FdSet, i int) {
+	p.Bits[i/64] |= 1 << uint(i) % 64
+}
+
+func FD_ISSET(p *syscall.FdSet, i int) bool {
+	return (p.Bits[i/64] & (1 << uint(i) % 64)) != 0
+}
+
+func FD_ZERO(p *syscall.FdSet) {
+	for i := range p.Bits {
+		p.Bits[i] = 0
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/udpping2.go b/vendor/src/github.com/pebbe/zmq2/examples/udpping2.go
new file mode 100644
index 0000000..d51fb8e
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/udpping2.go
@@ -0,0 +1,62 @@
+//
+//  UDP ping command
+//  Model 2, uses the GO net library
+//
+
+//  this doesn't use ZeroMQ at all
+
+package main
+
+import (
+	"fmt"
+	"log"
+	"net"
+	"time"
+)
+
+const (
+	PING_PORT_NUMBER = 9999
+	PING_MSG_SIZE    = 1
+	PING_INTERVAL    = 1000 * time.Millisecond //  Once per second
+)
+
+func main() {
+
+	log.SetFlags(log.Lshortfile)
+
+	//  Create UDP socket
+	bcast := &net.UDPAddr{Port: PING_PORT_NUMBER, IP: net.IPv4bcast}
+	conn, err := net.ListenUDP("udp", bcast)
+	if err != nil {
+		log.Fatalln(err)
+	}
+
+	buffer := make([]byte, PING_MSG_SIZE)
+
+	//  We send a beacon once a second, and we collect and report
+	//  beacons that come in from other nodes:
+
+	//  Send first ping right away
+	ping_at := time.Now()
+
+	for {
+		if err := conn.SetReadDeadline(ping_at); err != nil {
+			log.Fatalln(err)
+		}
+
+		if _, addr, err := conn.ReadFrom(buffer); err == nil {
+			//  Someone answered our ping
+			fmt.Println("Found peer", addr)
+		}
+
+		if time.Now().After(ping_at) {
+			//  Broadcast our beacon
+			fmt.Println("Pinging peers...")
+			buffer[0] = '!'
+			if _, err := conn.WriteTo(buffer, bcast); err != nil {
+				log.Fatalln(err)
+			}
+			ping_at = time.Now().Add(PING_INTERVAL)
+		}
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/udpping3.go b/vendor/src/github.com/pebbe/zmq2/examples/udpping3.go
new file mode 100644
index 0000000..f9d056c
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/udpping3.go
@@ -0,0 +1,25 @@
+//
+//  UDP ping command
+//  Model 3, uses abstract network interface
+//
+
+package main
+
+import (
+	"github.com/pebbe/zmq2/examples/intface"
+
+	"fmt"
+	"log"
+)
+
+func main() {
+	log.SetFlags(log.Lshortfile)
+	iface := intface.New()
+	for {
+		msg, err := iface.Recv()
+		if err != nil {
+			log.Fatalln(err)
+		}
+		fmt.Printf("%q\n", msg)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/version.go b/vendor/src/github.com/pebbe/zmq2/examples/version.go
new file mode 100644
index 0000000..8223995
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/version.go
@@ -0,0 +1,16 @@
+//
+//  Report 0MQ version.
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+)
+
+func main() {
+	major, minor, patch := zmq.Version()
+	fmt.Printf("Current 0MQ version is %d.%d.%d\n", major, minor, patch)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/wuclient.go b/vendor/src/github.com/pebbe/zmq2/examples/wuclient.go
new file mode 100644
index 0000000..8ce51e3
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/wuclient.go
@@ -0,0 +1,46 @@
+//
+//  Weather update client.
+//  Connects SUB socket to tcp://localhost:5556
+//  Collects weather updates and finds avg temp in zipcode
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"os"
+	"strconv"
+	"strings"
+)
+
+func main() {
+	//  Socket to talk to server
+	fmt.Println("Collecting updates from weather server...")
+	subscriber, _ := zmq.NewSocket(zmq.SUB)
+	defer subscriber.Close()
+	subscriber.Connect("tcp://localhost:5556")
+
+	//  Subscribe to zipcode, default is NYC, 10001
+	filter := "10001 "
+	if len(os.Args) > 1 {
+		filter = os.Args[1] + " "
+	}
+	subscriber.SetSubscribe(filter)
+
+	//  Process 100 updates
+	total_temp := 0
+	update_nbr := 0
+	for update_nbr < 100 {
+		msg, _ := subscriber.Recv(0)
+
+		if msgs := strings.Fields(msg); len(msgs) > 1 {
+			if temperature, err := strconv.Atoi(msgs[1]); err == nil {
+				total_temp += temperature
+				update_nbr++
+			}
+		}
+	}
+	fmt.Printf("Average temperature for zipcode '%s' was %dF \n\n", strings.TrimSpace(filter), total_temp/update_nbr)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/wuproxy.go b/vendor/src/github.com/pebbe/zmq2/examples/wuproxy.go
new file mode 100644
index 0000000..fa34418
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/wuproxy.go
@@ -0,0 +1,29 @@
+//
+//  Weather proxy device.
+//
+//  NOT TESTED
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"log"
+)
+
+func main() {
+	//  This is where the weather server sits
+	frontend, _ := zmq.NewSocket(zmq.XSUB)
+	defer frontend.Close()
+	frontend.Connect("tcp://192.168.55.210:5556")
+
+	//  This is our public endpoint for subscribers
+	backend, _ := zmq.NewSocket(zmq.XPUB)
+	defer backend.Close()
+	backend.Bind("tcp://10.1.1.0:8100")
+
+	//  Run the proxy until the user interrupts us
+	err := zmq.Proxy(frontend, backend, nil)
+	log.Fatalln("Proxy interrupted:", err)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/examples/wuserver.go b/vendor/src/github.com/pebbe/zmq2/examples/wuserver.go
new file mode 100644
index 0000000..d249c6d
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/examples/wuserver.go
@@ -0,0 +1,40 @@
+//
+//  Weather update server.
+//  Binds PUB socket to tcp://*:5556
+//  Publishes random weather updates
+//
+
+package main
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"math/rand"
+	"time"
+)
+
+func main() {
+
+	//  Prepare our publisher
+	publisher, _ := zmq.NewSocket(zmq.PUB)
+	defer publisher.Close()
+	publisher.Bind("tcp://*:5556")
+	publisher.Bind("ipc://weather.ipc")
+
+	//  Initialize random number generator
+	rand.Seed(time.Now().UnixNano())
+
+	// loop for a while aparently
+	for {
+
+		//  Get values that will fool the boss
+		zipcode := rand.Intn(100000)
+		temperature := rand.Intn(215) - 80
+		relhumidity := rand.Intn(50) + 10
+
+		//  Send message to all subscribers
+		msg := fmt.Sprintf("%05d %d %d", zipcode, temperature, relhumidity)
+		publisher.Send(msg, 0)
+	}
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/polling.go b/vendor/src/github.com/pebbe/zmq2/polling.go
new file mode 100644
index 0000000..8e22b24
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/polling.go
@@ -0,0 +1,126 @@
+package zmq2
+
+/*
+#include <zmq.h>
+*/
+import "C"
+
+import (
+	"fmt"
+	"time"
+)
+
+// Return type for (*Poller)Poll
+type Polled struct {
+	Socket *Socket // socket with matched event(s)
+	Events State   // actual matched event(s)
+}
+
+type Poller struct {
+	items []C.zmq_pollitem_t
+	socks []*Socket
+	size  int
+}
+
+// Create a new Poller
+func NewPoller() *Poller {
+	return &Poller{
+		items: make([]C.zmq_pollitem_t, 0),
+		socks: make([]*Socket, 0),
+		size:  0}
+}
+
+// Add items to the poller
+//
+// Events is a bitwise OR of zmq.POLLIN and zmq.POLLOUT
+func (p *Poller) Add(soc *Socket, events State) {
+	var item C.zmq_pollitem_t
+	item.socket = soc.soc
+	item.fd = 0
+	item.events = C.short(events)
+	p.items = append(p.items, item)
+	p.socks = append(p.socks, soc)
+	p.size += 1
+}
+
+/*
+Input/output multiplexing
+
+If timeout < 0, wait forever until a matching event is detected
+
+Only sockets with matching socket events are returned in the list.
+
+Example:
+
+    poller := zmq.NewPoller()
+    poller.Add(socket0, zmq.POLLIN)
+    poller.Add(socket1, zmq.POLLIN)
+    //  Process messages from both sockets
+    for {
+        sockets, _ := poller.Poll(-1)
+        for _, socket := range sockets {
+            switch s := socket.Socket; s {
+            case socket0:
+                msg, _ := s.Recv(0)
+                //  Process msg
+            case socket1:
+                msg, _ := s.Recv(0)
+                //  Process msg
+            }
+        }
+    }
+*/
+func (p *Poller) Poll(timeout time.Duration) ([]Polled, error) {
+	return p.poll(timeout, false)
+}
+
+/*
+This is like (*Poller)Poll, but it returns a list of all sockets,
+in the same order as they were added to the poller,
+not just those sockets that had an event.
+
+For each socket in the list, you have to check the Events field
+to see if there was actually an event.
+
+When error is not nil, the return list contains no sockets.
+*/
+func (p *Poller) PollAll(timeout time.Duration) ([]Polled, error) {
+	return p.poll(timeout, true)
+}
+
+func (p *Poller) poll(timeout time.Duration, all bool) ([]Polled, error) {
+	lst := make([]Polled, 0, p.size)
+
+	for _, soc := range p.socks {
+		if !soc.opened {
+			return lst, ErrorSocketClosed
+		}
+	}
+
+	t := timeout
+	if t > 0 {
+		t = t / time.Microsecond
+	}
+	if t < 0 {
+		t = -1
+	}
+	rv, err := C.zmq_poll(&p.items[0], C.int(len(p.items)), C.long(t))
+	if rv < 0 {
+		return lst, errget(err)
+	}
+	for i, it := range p.items {
+		if all || it.events&it.revents != 0 {
+			lst = append(lst, Polled{p.socks[i], State(it.revents)})
+		}
+	}
+	return lst, nil
+}
+
+// Poller as string.
+func (p *Poller) String() string {
+	str := make([]string, 0)
+	for i, poll := range p.items {
+		str = append(str, fmt.Sprintf("%v%v", p.socks[i], State(poll.events)))
+	}
+	return fmt.Sprint("Poller", str)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/reactor.go b/vendor/src/github.com/pebbe/zmq2/reactor.go
new file mode 100644
index 0000000..c76bec7
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/reactor.go
@@ -0,0 +1,194 @@
+package zmq2
+
+import (
+	"errors"
+	"fmt"
+	"time"
+)
+
+type reactor_socket struct {
+	e State
+	f func(State) error
+}
+
+type reactor_channel struct {
+	ch    <-chan interface{}
+	f     func(interface{}) error
+	limit int
+}
+
+type Reactor struct {
+	sockets  map[*Socket]*reactor_socket
+	channels map[uint64]*reactor_channel
+	p        *Poller
+	idx      uint64
+	remove   []uint64
+	verbose  bool
+}
+
+/*
+Create a reactor to mix the handling of sockets and channels (timers or other channels).
+
+Example:
+
+    reactor := zmq.NewReactor()
+    reactor.AddSocket(socket1, zmq.POLLIN, socket1_handler)
+    reactor.AddSocket(socket2, zmq.POLLIN, socket2_handler)
+    reactor.AddChannelTime(time.Tick(time.Second), 1, ticker_handler)
+    reactor.Run(time.Second)
+*/
+func NewReactor() *Reactor {
+	r := &Reactor{
+		sockets:  make(map[*Socket]*reactor_socket),
+		channels: make(map[uint64]*reactor_channel),
+		p:        NewPoller(),
+		remove:   make([]uint64, 0),
+	}
+	return r
+}
+
+// Add socket handler to the reactor.
+//
+// You can have only one handler per socket. Adding a second one will remove the first.
+//
+// The handler receives the socket state as an argument: POLLIN, POLLOUT, or both.
+func (r *Reactor) AddSocket(soc *Socket, events State, handler func(State) error) {
+	r.RemoveSocket(soc)
+	r.sockets[soc] = &reactor_socket{e: events, f: handler}
+	r.p.Add(soc, events)
+}
+
+// Remove a socket handler from the reactor.
+func (r *Reactor) RemoveSocket(soc *Socket) {
+	if _, ok := r.sockets[soc]; ok {
+		delete(r.sockets, soc)
+		// rebuild poller
+		r.p = NewPoller()
+		for s, props := range r.sockets {
+			r.p.Add(s, props.e)
+		}
+	}
+}
+
+// Add channel handler to the reactor.
+//
+// Returns id of added handler, that can be used later to remove it.
+//
+// If limit is positive, at most this many items will be handled in each run through the main loop,
+// otherwise it will process as many items as possible.
+//
+// The handler function receives the value received from the channel.
+func (r *Reactor) AddChannel(ch <-chan interface{}, limit int, handler func(interface{}) error) (id uint64) {
+	r.idx++
+	id = r.idx
+	r.channels[id] = &reactor_channel{ch: ch, f: handler, limit: limit}
+	return
+}
+
+// This function wraps AddChannel, using a channel of type time.Time instead of type interface{}.
+func (r *Reactor) AddChannelTime(ch <-chan time.Time, limit int, handler func(interface{}) error) (id uint64) {
+	ch2 := make(chan interface{})
+	go func() {
+		for {
+			a, ok := <-ch
+			if !ok {
+				close(ch2)
+				break
+			}
+			ch2 <- a
+		}
+	}()
+	return r.AddChannel(ch2, limit, handler)
+}
+
+// Remove a channel from the reactor.
+//
+// Closed channels are removed automaticly.
+func (r *Reactor) RemoveChannel(id uint64) {
+	r.remove = append(r.remove, id)
+}
+
+func (r *Reactor) SetVerbose(verbose bool) {
+	r.verbose = verbose
+}
+
+// Run the reactor.
+//
+// The interval determines the time-out on the polling of sockets.
+// Interval must be positive if there are channels.
+// If there are no channels, you can set interval to -1.
+//
+// The run alternates between polling/handling sockets (using the interval as timeout),
+// and reading/handling channels. The reading of channels is without time-out: if there
+// is no activity on any channel, the run continues to poll sockets immediately.
+//
+// The run exits when any handler returns an error, returning that same error.
+func (r *Reactor) Run(interval time.Duration) (err error) {
+	for {
+
+		// process requests to remove channels
+		for _, id := range r.remove {
+			delete(r.channels, id)
+		}
+		r.remove = r.remove[0:0]
+
+	CHANNELS:
+		for id, ch := range r.channels {
+			limit := ch.limit
+			for {
+				select {
+				case val, ok := <-ch.ch:
+					if !ok {
+						if r.verbose {
+							fmt.Printf("Reactor(%p) removing closed channel %d\n", r, id)
+						}
+						r.RemoveChannel(id)
+						continue CHANNELS
+					}
+					if r.verbose {
+						fmt.Printf("Reactor(%p) channel %d: %q\n", r, id, val)
+					}
+					err = ch.f(val)
+					if err != nil {
+						return
+					}
+					if ch.limit > 0 {
+						limit--
+						if limit == 0 {
+							continue CHANNELS
+						}
+					}
+				default:
+					continue CHANNELS
+				}
+			}
+		}
+
+		if len(r.channels) > 0 && interval < 0 {
+			return errors.New("There are channels, but polling time-out is infinite")
+		}
+
+		if len(r.sockets) == 0 {
+			if len(r.channels) == 0 {
+				return errors.New("No sockets to poll, no channels to read")
+			}
+			time.Sleep(interval)
+			continue
+		}
+
+		polled, e := r.p.Poll(interval)
+		if e != nil {
+			return e
+		}
+		for _, item := range polled {
+			if r.verbose {
+				fmt.Printf("Reactor(%p) %v\n", r, item)
+			}
+			err = r.sockets[item.Socket].f(item.Events)
+			if err != nil {
+				return
+			}
+		}
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/socketget.go b/vendor/src/github.com/pebbe/zmq2/socketget.go
new file mode 100644
index 0000000..03f81f3
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/socketget.go
@@ -0,0 +1,258 @@
+package zmq2
+
+/*
+#include <zmq.h>
+#include "zmq2.h"
+#include <stdint.h>
+*/
+import "C"
+
+import (
+	"strings"
+	"time"
+	"unsafe"
+)
+
+func (soc *Socket) getString(opt C.int, bufsize int) (string, error) {
+	if !soc.opened {
+		return "", ErrorSocketClosed
+	}
+	value := make([]byte, bufsize)
+	size := C.size_t(bufsize)
+	if i, err := C.zmq_getsockopt(soc.soc, opt, unsafe.Pointer(&value[0]), &size); i != 0 {
+		return "", errget(err)
+	}
+	return strings.TrimRight(string(value[:int(size)]), "\x00"), nil
+}
+
+func (soc *Socket) getInt(opt C.int) (int, error) {
+	if !soc.opened {
+		return 0, ErrorSocketClosed
+	}
+	value := C.int(0)
+	size := C.size_t(unsafe.Sizeof(value))
+	if i, err := C.zmq_getsockopt(soc.soc, opt, unsafe.Pointer(&value), &size); i != 0 {
+		return 0, errget(err)
+	}
+	return int(value), nil
+}
+
+func (soc *Socket) getInt64(opt C.int) (int64, error) {
+	if !soc.opened {
+		return 0, ErrorSocketClosed
+	}
+	value := C.int64_t(0)
+	size := C.size_t(unsafe.Sizeof(value))
+	if i, err := C.zmq_getsockopt(soc.soc, opt, unsafe.Pointer(&value), &size); i != 0 {
+		return 0, errget(err)
+	}
+	return int64(value), nil
+}
+
+func (soc *Socket) getUInt64(opt C.int) (uint64, error) {
+	if !soc.opened {
+		return 0, ErrorSocketClosed
+	}
+	value := C.uint64_t(0)
+	size := C.size_t(unsafe.Sizeof(value))
+	if i, err := C.zmq_getsockopt(soc.soc, opt, unsafe.Pointer(&value), &size); i != 0 {
+		return 0, errget(err)
+	}
+	return uint64(value), nil
+}
+
+func (soc *Socket) getUInt32(opt C.int) (uint32, error) {
+	if !soc.opened {
+		return 0, ErrorSocketClosed
+	}
+	value := C.uint32_t(0)
+	size := C.size_t(unsafe.Sizeof(value))
+	if i, err := C.zmq_getsockopt(soc.soc, opt, unsafe.Pointer(&value), &size); i != 0 {
+		return 0, errget(err)
+	}
+	return uint32(value), nil
+}
+
+// ZMQ_TYPE: Retrieve socket type
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc3
+func (soc *Socket) GetType() (Type, error) {
+	v, err := soc.getInt(C.ZMQ_TYPE)
+	return Type(v), err
+}
+
+// ZMQ_RCVMORE: More message data parts to follow
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc4
+func (soc *Socket) GetRcvmore() (bool, error) {
+	v, err := soc.getInt64(C.ZMQ_RCVMORE)
+	return v != 0, err
+}
+
+// ZMQ_HWM: Retrieve high water mark
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc5
+func (soc *Socket) GetHwm() (uint64, error) {
+	return soc.getUInt64(C.ZMQ_HWM)
+}
+
+// ZMQ_RCVTIMEO: Maximum time before a socket operation returns with EAGAIN
+//
+// Returns time.Duration(-1) for infinite
+//
+// Returns ErrorNotImplemented in 0MQ version 2.1
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc6
+func (soc *Socket) GetRcvtimeo() (time.Duration, error) {
+	major, minor, _ := Version()
+	if major == 2 && minor < 2 {
+		return 0, ErrorNotImplemented
+	}
+	v, err := soc.getInt(C.ZMQ_RCVTIMEO)
+	if v < 0 {
+		return time.Duration(-1), err
+	}
+	return time.Duration(v) * time.Millisecond, err
+}
+
+// ZMQ_SNDTIMEO: Maximum time before a socket operation returns with EAGAIN
+//
+// Returns time.Duration(-1) for infinite
+//
+// Returns ErrorNotImplemented in 0MQ version 2.1
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc7
+func (soc *Socket) GetSndtimeo() (time.Duration, error) {
+	major, minor, _ := Version()
+	if major == 2 && minor < 2 {
+		return 0, ErrorNotImplemented
+	}
+	v, err := soc.getInt(C.ZMQ_SNDTIMEO)
+	if v < 0 {
+		return time.Duration(-1), err
+	}
+	return time.Duration(v) * time.Millisecond, err
+}
+
+// ZMQ_SWAP: Retrieve disk offload size
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc8
+func (soc *Socket) GetSwap() (int64, error) {
+	return soc.getInt64(C.ZMQ_SWAP)
+}
+
+// ZMQ_AFFINITY: Retrieve I/O thread affinity
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc9
+func (soc *Socket) GetAffinity() (uint64, error) {
+	return soc.getUInt64(C.ZMQ_AFFINITY)
+}
+
+// ZMQ_IDENTITY: Retrieve socket identity
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc10
+func (soc *Socket) GetIdentity() (string, error) {
+	return soc.getString(C.ZMQ_IDENTITY, 256)
+}
+
+// ZMQ_RATE: Retrieve multicast data rate
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc11
+func (soc *Socket) GetRate() (int64, error) {
+	return soc.getInt64(C.ZMQ_RATE)
+}
+
+// ZMQ_RECOVERY_IVL: Get multicast recovery interval
+//
+// Note: return time is time.Duration
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc12
+func (soc *Socket) GetRecoveryIvl() (time.Duration, error) {
+	v, e := soc.getInt64(C.ZMQ_RECOVERY_IVL)
+	return time.Duration(v) * time.Second, e
+}
+
+// ZMQ_RECOVERY_IVL_MSEC: Get multicast recovery interval in milliseconds
+//
+// Note: return time is time.Duration
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc13
+func (soc *Socket) GetRecoveryIvlMsec() (time.Duration, error) {
+	v, e := soc.getInt64(C.ZMQ_RECOVERY_IVL_MSEC)
+	if v == -1 {
+		return -1, e
+	}
+	return time.Duration(v) * time.Millisecond, e
+}
+
+// ZMQ_MCAST_LOOP: Control multicast loop-back
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc14
+func (soc *Socket) GetMcastLoop() (bool, error) {
+	v, e := soc.getInt64(C.ZMQ_MCAST_LOOP)
+	if v == 0 {
+		return false, e
+	}
+	return true, e
+}
+
+// ZMQ_SNDBUF: Retrieve kernel transmit buffer size
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc15
+func (soc *Socket) GetSndbuf(value int) (uint64, error) {
+	return soc.getUInt64(C.ZMQ_SNDBUF)
+}
+
+// ZMQ_RCVBUF: Retrieve kernel receive buffer size
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc16
+func (soc *Socket) GetRcvbuf(value int) (uint64, error) {
+	return soc.getUInt64(C.ZMQ_RCVBUF)
+}
+
+// ZMQ_LINGER: Retrieve linger period for socket shutdown
+//
+// Returns time.Duration(-1) for infinite
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc17
+func (soc *Socket) GetLinger() (time.Duration, error) {
+	v, err := soc.getInt(C.ZMQ_LINGER)
+	if v < 0 {
+		return time.Duration(-1), err
+	}
+	return time.Duration(v) * time.Millisecond, err
+}
+
+// ZMQ_RECONNECT_IVL: Retrieve reconnection interval
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc18
+func (soc *Socket) GetReconnectIvl() (time.Duration, error) {
+	v, err := soc.getInt(C.ZMQ_RECONNECT_IVL)
+	return time.Duration(v) * time.Millisecond, err
+}
+
+// ZMQ_RECONNECT_IVL_MAX: Retrieve maximum reconnection interval
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc19
+func (soc *Socket) GetReconnectIvlMax() (time.Duration, error) {
+	v, err := soc.getInt(C.ZMQ_RECONNECT_IVL_MAX)
+	return time.Duration(v) * time.Millisecond, err
+}
+
+// ZMQ_BACKLOG: Retrieve maximum length of the queue of outstanding connections
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc20
+func (soc *Socket) GetBacklog() (int, error) {
+	return soc.getInt(C.ZMQ_BACKLOG)
+}
+
+// ZMQ_FD: Retrieve file descriptor associated with the socket
+// see socketget_unix.go and socketget_windows.go
+
+// ZMQ_EVENTS: Retrieve socket event state
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc22
+func (soc *Socket) GetEvents() (State, error) {
+	v, err := soc.getUInt32(C.ZMQ_EVENTS)
+	return State(v), err
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/socketget_unix.go b/vendor/src/github.com/pebbe/zmq2/socketget_unix.go
new file mode 100644
index 0000000..81d1ece
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/socketget_unix.go
@@ -0,0 +1,15 @@
+// +build !windows
+
+package zmq2
+
+/*
+#include <zmq.h>
+*/
+import "C"
+
+// ZMQ_FD: Retrieve file descriptor associated with the socket
+//
+// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc21
+func (soc *Socket) GetFd() (int, error) {
+	return soc.getInt(C.ZMQ_FD)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/socketget_windows.go b/vendor/src/github.com/pebbe/zmq2/socketget_windows.go
new file mode 100644
index 0000000..d44bc4b
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/socketget_windows.go
@@ -0,0 +1,26 @@
+// +build windows
+
+package zmq2
+
+/*
+#include <zmq.h>
+*/
+import "C"
+
+import (
+	"unsafe"
+)
+
+/*
+ZMQ_FD: Retrieve file descriptor associated with the socket
+
+See: http://api.zeromq.org/2-2:zmq-getsockopt#toc21
+*/
+func (soc *Socket) GetFd() (uintptr, error) {
+	value := C.SOCKET(0)
+	size := C.size_t(unsafe.Sizeof(value))
+	if i, err := C.zmq_getsockopt(soc.soc, C.ZMQ_FD, unsafe.Pointer(&value), &size); i != 0 {
+		return uintptr(0), errget(err)
+	}
+	return uintptr(value), nil
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/socketset.go b/vendor/src/github.com/pebbe/zmq2/socketset.go
new file mode 100644
index 0000000..8b9f3fb
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/socketset.go
@@ -0,0 +1,230 @@
+package zmq2
+
+/*
+#include <zmq.h>
+#include "zmq2.h"
+#include <stdint.h>
+#include <stdlib.h>
+*/
+import "C"
+
+import (
+	"time"
+	"unsafe"
+)
+
+func (soc *Socket) setString(opt C.int, s string) error {
+	if !soc.opened {
+		return ErrorSocketClosed
+	}
+	cs := C.CString(s)
+	defer C.free(unsafe.Pointer(cs))
+	if i, err := C.zmq_setsockopt(soc.soc, opt, unsafe.Pointer(cs), C.size_t(len(s))); i != 0 {
+		return errget(err)
+	}
+	return nil
+}
+
+func (soc *Socket) setInt(opt C.int, value int) error {
+	if !soc.opened {
+		return ErrorSocketClosed
+	}
+	val := C.int(value)
+	if i, err := C.zmq_setsockopt(soc.soc, opt, unsafe.Pointer(&val), C.size_t(unsafe.Sizeof(val))); i != 0 {
+		return errget(err)
+	}
+	return nil
+}
+
+func (soc *Socket) setInt64(opt C.int, value int64) error {
+	if !soc.opened {
+		return ErrorSocketClosed
+	}
+	val := C.int64_t(value)
+	if i, err := C.zmq_setsockopt(soc.soc, opt, unsafe.Pointer(&val), C.size_t(unsafe.Sizeof(val))); i != 0 {
+		return errget(err)
+	}
+	return nil
+}
+
+func (soc *Socket) setUInt64(opt C.int, value uint64) error {
+	if !soc.opened {
+		return ErrorSocketClosed
+	}
+	val := C.uint64_t(value)
+	if i, err := C.zmq_setsockopt(soc.soc, opt, unsafe.Pointer(&val), C.size_t(unsafe.Sizeof(val))); i != 0 {
+		return errget(err)
+	}
+	return nil
+}
+
+// ZMQ_HWM: Set high water mark
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc3
+func (soc *Socket) SetHwm(value uint64) error {
+	return soc.setUInt64(C.ZMQ_HWM, value)
+}
+
+// ZMQ_SWAP: Set disk offload size
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc4
+func (soc *Socket) SetSwap(value int64) error {
+	return soc.setInt64(C.ZMQ_SWAP, value)
+}
+
+// ZMQ_AFFINITY: Set I/O thread affinity
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc5
+func (soc *Socket) SetAffinity(value uint64) error {
+	return soc.setUInt64(C.ZMQ_AFFINITY, value)
+}
+
+// ZMQ_IDENTITY: Set socket identity
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc6
+func (soc *Socket) SetIdentity(value string) error {
+	return soc.setString(C.ZMQ_IDENTITY, value)
+}
+
+// ZMQ_SUBSCRIBE: Establish message filter
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc7
+func (soc *Socket) SetSubscribe(filter string) error {
+	return soc.setString(C.ZMQ_SUBSCRIBE, filter)
+}
+
+// ZMQ_UNSUBSCRIBE: Remove message filter
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc8
+func (soc *Socket) SetUnsubscribe(filter string) error {
+	return soc.setString(C.ZMQ_UNSUBSCRIBE, filter)
+}
+
+// ZMQ_RCVTIMEO: Maximum time before a recv operation returns with EAGAIN
+//
+// Use -1 for infinite
+//
+// Returns ErrorNotImplemented in 0MQ version 2.1
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc9
+func (soc *Socket) SetRcvtimeo(value time.Duration) error {
+	major, minor, _ := Version()
+	if major == 2 && minor < 2 {
+		return ErrorNotImplemented
+	}
+	val := int(value / time.Millisecond)
+	if value == -1 {
+		val = -1
+	}
+	return soc.setInt(C.ZMQ_RCVTIMEO, val)
+}
+
+// ZMQ_SNDTIMEO: Maximum time before a send operation returns with EAGAIN
+//
+// Use -1 for infinite
+//
+// Returns ErrorNotImplemented in 0MQ version 2.1
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc10
+func (soc *Socket) SetSndtimeo(value time.Duration) error {
+	major, minor, _ := Version()
+	if major == 2 && minor < 2 {
+		return ErrorNotImplemented
+	}
+	val := int(value / time.Millisecond)
+	if value == -1 {
+		val = -1
+	}
+	return soc.setInt(C.ZMQ_SNDTIMEO, val)
+}
+
+// ZMQ_RATE: Set multicast data rate
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc11
+func (soc *Socket) SetRate(value int64) error {
+	return soc.setInt64(C.ZMQ_RATE, value)
+}
+
+// ZMQ_RECOVERY_IVL: Set multicast recovery interval
+//
+// Note: value is of type time.Duration
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc12
+func (soc *Socket) SetRecoveryIvl(value time.Duration) error {
+	val := int64(value / time.Second)
+	return soc.setInt64(C.ZMQ_RECOVERY_IVL, val)
+}
+
+// ZMQ_RECOVERY_IVL_MSEC: Set multicast recovery interval in milliseconds
+//
+// Note: value is of type time.Duration
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc13
+func (soc *Socket) SetRecoveryIvlMsec(value time.Duration) error {
+	val := int64(value / time.Millisecond)
+	if value < 0 {
+		val = -1
+	}
+	return soc.setInt64(C.ZMQ_RECOVERY_IVL_MSEC, val)
+}
+
+// ZMQ_MCAST_LOOP: Control multicast loop-back
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc14
+func (soc *Socket) SetMcastLoop(value bool) error {
+	val := int64(0)
+	if value {
+		val = 1
+	}
+	return soc.setInt64(C.ZMQ_MCAST_LOOP, val)
+}
+
+// ZMQ_SNDBUF: Set kernel transmit buffer size
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc15
+func (soc *Socket) SetSndbuf(value uint64) error {
+	return soc.setUInt64(C.ZMQ_SNDBUF, value)
+}
+
+// ZMQ_RCVBUF: Set kernel receive buffer size
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc16
+func (soc *Socket) SetRcvbuf(value uint64) error {
+	return soc.setUInt64(C.ZMQ_RCVBUF, value)
+}
+
+// ZMQ_LINGER: Set linger period for socket shutdown
+//
+// Use -1 for infinite
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc17
+func (soc *Socket) SetLinger(value time.Duration) error {
+	val := int(value / time.Millisecond)
+	if value == -1 {
+		val = -1
+	}
+	return soc.setInt(C.ZMQ_LINGER, val)
+}
+
+// ZMQ_RECONNECT_IVL: Set reconnection interval
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc18
+func (soc *Socket) SetReconnectIvl(value time.Duration) error {
+	val := int(value / time.Millisecond)
+	return soc.setInt(C.ZMQ_RECONNECT_IVL, val)
+}
+
+// ZMQ_RECONNECT_IVL_MAX: Set maximum reconnection interval
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc19
+func (soc *Socket) SetReconnectIvlMax(value time.Duration) error {
+	val := int(value / time.Millisecond)
+	return soc.setInt(C.ZMQ_RECONNECT_IVL_MAX, val)
+}
+
+// ZMQ_BACKLOG: Set maximum length of the queue of outstanding connections
+//
+// See: http://api.zeromq.org/2-2:zmq-setsockopt#toc20
+func (soc *Socket) SetBacklog(value int) error {
+	return soc.setInt(C.ZMQ_BACKLOG, value)
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/utils.go b/vendor/src/github.com/pebbe/zmq2/utils.go
new file mode 100644
index 0000000..b58135e
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/utils.go
@@ -0,0 +1,134 @@
+package zmq2
+
+import (
+	"fmt"
+)
+
+/*
+Send multi-part message on socket.
+
+Any `[]string' or `[][]byte' is split into separate `string's or `[]byte's
+
+Any other part that isn't a `string' or `[]byte' is converted
+to `string' with `fmt.Sprintf("%v", part)'.
+
+Returns total bytes sent.
+*/
+func (soc *Socket) SendMessage(parts ...interface{}) (total int, err error) {
+	return soc.sendMessage(0, parts...)
+}
+
+/*
+Like SendMessage(), but adding the NOBLOCK flag.
+*/
+func (soc *Socket) SendMessageDontwait(parts ...interface{}) (total int, err error) {
+	return soc.sendMessage(NOBLOCK, parts...)
+}
+
+func (soc *Socket) sendMessage(dontwait Flag, parts ...interface{}) (total int, err error) {
+	// TODO: make this faster
+
+	// flatten first, just in case the last part may be an empty []string or [][]byte
+	pp := make([]interface{}, 0)
+	for _, p := range parts {
+		switch t := p.(type) {
+		case []string:
+			for _, s := range t {
+				pp = append(pp, s)
+			}
+		case [][]byte:
+			for _, b := range t {
+				pp = append(pp, b)
+			}
+		default:
+			pp = append(pp, t)
+		}
+	}
+
+	n := len(pp)
+	if n == 0 {
+		return
+	}
+	opt := SNDMORE | dontwait
+	for i, p := range pp {
+		if i == n-1 {
+			opt = dontwait
+		}
+		switch t := p.(type) {
+		case string:
+			j, e := soc.Send(t, opt)
+			if e == nil {
+				total += j
+			} else {
+				return -1, e
+			}
+		case []byte:
+			j, e := soc.SendBytes(t, opt)
+			if e == nil {
+				total += j
+			} else {
+				return -1, e
+			}
+		default:
+			j, e := soc.Send(fmt.Sprintf("%v", t), opt)
+			if e == nil {
+				total += j
+			} else {
+				return -1, e
+			}
+		}
+	}
+	return
+}
+
+/*
+Receive parts as message from socket.
+
+Returns last non-nil error code.
+*/
+func (soc *Socket) RecvMessage(flags Flag) (msg []string, err error) {
+	msg = make([]string, 0)
+	for {
+		s, e := soc.Recv(flags)
+		if e == nil {
+			msg = append(msg, s)
+		} else {
+			return msg[0:0], e
+		}
+		more, e := soc.GetRcvmore()
+		if e == nil {
+			if !more {
+				break
+			}
+		} else {
+			return msg[0:0], e
+		}
+	}
+	return
+}
+
+/*
+Receive parts as message from socket.
+
+Returns last non-nil error code.
+*/
+func (soc *Socket) RecvMessageBytes(flags Flag) (msg [][]byte, err error) {
+	msg = make([][]byte, 0)
+	for {
+		b, e := soc.RecvBytes(flags)
+		if e == nil {
+			msg = append(msg, b)
+		} else {
+			return msg[0:0], e
+		}
+		more, e := soc.GetRcvmore()
+		if e == nil {
+			if !more {
+				break
+			}
+		} else {
+			return msg[0:0], e
+		}
+	}
+	return
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/zmq2.go b/vendor/src/github.com/pebbe/zmq2/zmq2.go
new file mode 100644
index 0000000..5b26c04
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/zmq2.go
@@ -0,0 +1,594 @@
+package zmq2
+
+/*
+#cgo !windows pkg-config: libzmq
+#cgo windows CFLAGS: -I/usr/local/include
+#cgo windows LDFLAGS: -L/usr/local/lib -lzmq
+#include <zmq.h>
+#include "zmq2.h"
+#include <stdlib.h>
+#include <string.h>
+
+int
+    zmq2_major = ZMQ_VERSION_MAJOR,
+    zmq2_minor = ZMQ_VERSION_MINOR,
+    zmq2_patch = ZMQ_VERSION_PATCH;
+
+void my_free (void *data, void *hint) {
+    free (data);
+}
+int my_msg_init_data (zmq_msg_t *msg, void *data, size_t size) {
+    return zmq_msg_init_data (msg, data, size, my_free, NULL);
+}
+void *my_memcpy(void *dest, const void *src, size_t n) {
+	return memcpy(dest, src, n);
+}
+*/
+import "C"
+
+import (
+	"errors"
+	"fmt"
+	"runtime"
+	"strings"
+	"unsafe"
+)
+
+var (
+	ErrorContextClosed  = errors.New("Context is closed")
+	ErrorSocketClosed   = errors.New("Socket is closed")
+	ErrorNotImplemented = errors.New("Not implemented, requires 0MQ version 2.2")
+)
+
+var (
+	major, minor, patch int
+
+	defaultCtx    *Context
+	old           []*Context
+	nr_of_threads int
+)
+
+func init() {
+	var err error
+	nr_of_threads = 1
+	defaultCtx = &Context{}
+	defaultCtx.ctx, err = C.zmq_init(C.int(nr_of_threads))
+	defaultCtx.opened = true
+	if defaultCtx.ctx == nil {
+		panic("Init of ZeroMQ context failed: " + errget(err).Error())
+	}
+	old = make([]*Context, 0)
+	major, minor, patch = Version()
+	if major != 2 {
+		panic("Using zmq2 with ZeroMQ major version " + fmt.Sprint(major))
+	}
+	if major != int(C.zmq2_major) || minor != int(C.zmq2_minor) || patch != int(C.zmq2_patch) {
+		panic(
+			fmt.Sprintf(
+				"zmq2 was installed with ZeroMQ version %d.%d.%d, but the application links with version %d.%d.%d",
+				int(C.zmq2_major), int(C.zmq2_minor), int(C.zmq2_patch),
+				major, minor, patch))
+	}
+}
+
+//. Util
+
+// Report 0MQ library version.
+func Version() (major, minor, patch int) {
+	var maj, min, pat C.int
+	C.zmq_version(&maj, &min, &pat)
+	return int(maj), int(min), int(pat)
+}
+
+// Get 0MQ error message string.
+func Error(e int) string {
+	return C.GoString(C.zmq_strerror(C.int(e)))
+}
+
+//. Context
+
+/*
+A context that is not the default context.
+*/
+type Context struct {
+	ctx           unsafe.Pointer
+	nr_of_threads int
+	opened        bool
+	err           error
+}
+
+// Create a new context.
+//
+// The argument specifies the size of the ØMQ thread pool to handle I/O operations.
+// If your application is using only the inproc transport for messaging you may set
+// this to zero, otherwise set it to at least one.
+func NewContext(nr_of_threads int) (ctx *Context, err error) {
+	ctx = &Context{}
+	c, e := C.zmq_init(C.int(nr_of_threads))
+	if c == nil {
+		err = errget(e)
+		ctx.err = err
+	} else {
+		ctx.ctx = c
+		ctx.opened = true
+		ctx.nr_of_threads = nr_of_threads
+		runtime.SetFinalizer(ctx, (*Context).Term)
+	}
+	return
+}
+
+/*
+Terminates the current default and all old default contexts.
+
+For linger behavior, see: http://api.zeromq.org/2-2:zmq-term
+*/
+func Term() error {
+	if defaultCtx.opened {
+		defaultCtx.opened = false
+		n, err := C.zmq_term(defaultCtx.ctx)
+		if n != 0 {
+			defaultCtx.err = errget(err)
+		}
+	}
+	if defaultCtx.err != nil {
+		return defaultCtx.err
+	}
+
+	for _, oldCtx := range old {
+		if oldCtx.opened {
+			oldCtx.opened = false
+			n, err := C.zmq_term(oldCtx.ctx)
+			if n != 0 {
+				oldCtx.err = errget(err)
+			}
+		}
+		if oldCtx.err != nil {
+			return oldCtx.err
+		}
+	}
+
+	return nil
+}
+
+/*
+Terminates the context.
+
+For linger behavior, see: http://api.zeromq.org/2-2:zmq-term
+*/
+func (ctx *Context) Term() error {
+	if ctx.opened {
+		ctx.opened = false
+		n, err := C.zmq_term(ctx.ctx)
+		if n != 0 {
+			ctx.err = errget(err)
+		}
+	}
+	return ctx.err
+}
+
+// Returns the size of the 0MQ thread pool in the current default context.
+func GetIoThreads() (int, error) {
+	return nr_of_threads, nil
+}
+
+// Returns the size of the 0MQ thread pool.
+func (ctx *Context) GetIoThreads() (int, error) {
+	return ctx.nr_of_threads, nil
+}
+
+/*
+This function specifies the size of the ØMQ thread pool to handle I/O operations.
+If your application is using only the inproc transport for messaging you may set
+this to zero, otherwise set it to at least one.
+
+This function creates a new default context without closing the old one.
+Use it before creating any sockets.
+
+Default value: 1
+*/
+func SetIoThreads(n int) error {
+	if n != nr_of_threads {
+		c, err := C.zmq_init(C.int(n))
+		if c == nil {
+			return errget(err)
+		}
+		old = append(old, defaultCtx) // keep a reference, to prevent garbage collection
+		defaultCtx = &Context{
+			ctx:    c,
+			opened: true,
+		}
+		nr_of_threads = n
+	}
+	return nil
+}
+
+//. Sockets
+
+// Specifies the type of a socket, used by NewSocket()
+type Type int
+
+const (
+	// Constants for NewSocket()
+	// See: http://api.zeromq.org/2-2:zmq-socket#toc3
+	REQ    = Type(C.ZMQ_REQ)
+	REP    = Type(C.ZMQ_REP)
+	DEALER = Type(C.ZMQ_DEALER)
+	ROUTER = Type(C.ZMQ_ROUTER)
+	PUB    = Type(C.ZMQ_PUB)
+	SUB    = Type(C.ZMQ_SUB)
+	XPUB   = Type(C.ZMQ_XPUB)
+	XSUB   = Type(C.ZMQ_XSUB)
+	PUSH   = Type(C.ZMQ_PUSH)
+	PULL   = Type(C.ZMQ_PULL)
+	PAIR   = Type(C.ZMQ_PAIR)
+)
+
+/*
+Socket type as string.
+*/
+func (t Type) String() string {
+	switch t {
+	case REQ:
+		return "REQ"
+	case REP:
+		return "REP"
+	case DEALER:
+		return "DEALER"
+	case ROUTER:
+		return "ROUTER"
+	case PUB:
+		return "PUB"
+	case SUB:
+		return "SUB"
+	case XPUB:
+		return "XPUB"
+	case XSUB:
+		return "XSUB"
+	case PUSH:
+		return "PUSH"
+	case PULL:
+		return "PULL"
+	case PAIR:
+		return "PAIR"
+	}
+	return "<INVALID>"
+}
+
+// Used by  (*Socket)Send() and (*Socket)Recv()
+type Flag int
+
+const (
+	// Flags for (*Socket)Send(), (*Socket)Recv()
+	// For Send, see: http://api.zeromq.org/2-2:zmq-send#toc2
+	// For Recv, see: http://api.zeromq.org/2-2:zmq-recv#toc2
+	NOBLOCK = Flag(C.ZMQ_NOBLOCK)
+	SNDMORE = Flag(C.ZMQ_SNDMORE)
+)
+
+/*
+Socket flag as string.
+*/
+func (f Flag) String() string {
+	ff := make([]string, 0)
+	if f&NOBLOCK != 0 {
+		ff = append(ff, "NOBLOCK")
+	}
+	if f&SNDMORE != 0 {
+		ff = append(ff, "SNDMORE")
+	}
+	if len(ff) == 0 {
+		return "<NONE>"
+	}
+	return strings.Join(ff, "|")
+}
+
+// Used by (soc *Socket)GetEvents()
+type State uint32
+
+const (
+	// Flags for (*Socket)GetEvents()
+	// See: http://api.zeromq.org/2-2:zmq-getsockopt#toc22
+	POLLIN  = State(C.ZMQ_POLLIN)
+	POLLOUT = State(C.ZMQ_POLLOUT)
+)
+
+// Uses by Device()
+type Dev int
+
+const (
+	// Constants for Device()
+	// See: http://api.zeromq.org/2-2:zmq-device#toc2
+	QUEUE     = Dev(C.ZMQ_QUEUE)
+	FORWARDER = Dev(C.ZMQ_FORWARDER)
+	STREAMER  = Dev(C.ZMQ_STREAMER)
+)
+
+/*
+Dev as string
+*/
+func (d Dev) String() string {
+	switch d {
+	case QUEUE:
+		return "QUEUE"
+	case FORWARDER:
+		return "FORWARDER"
+	case STREAMER:
+		return "STREAMER"
+	}
+	return "<INVALID>"
+}
+
+/*
+Socket state as string.
+*/
+func (s State) String() string {
+	ss := make([]string, 0)
+	if s&POLLIN != 0 {
+		ss = append(ss, "POLLIN")
+	}
+	if s&POLLOUT != 0 {
+		ss = append(ss, "POLLOUT")
+	}
+	if len(ss) == 0 {
+		return "<NONE>"
+	}
+	return strings.Join(ss, "|")
+}
+
+/*
+Socket functions starting with `Set` or `Get` are used for setting and
+getting socket options.
+*/
+type Socket struct {
+	soc    unsafe.Pointer
+	ctx    *Context
+	opened bool
+	err    error
+}
+
+/*
+Socket as string.
+*/
+func (soc Socket) String() string {
+	if !soc.opened {
+		return "Socket(CLOSED)"
+	}
+	t, err := soc.GetType()
+	if err != nil {
+		return fmt.Sprintf("Socket(%v)", err)
+	}
+	i, err := soc.GetIdentity()
+	if err == nil && i != "" {
+		return fmt.Sprintf("Socket(%v,%q)", t, i)
+	}
+	return fmt.Sprintf("Socket(%v,%p)", t, soc.soc)
+}
+
+/*
+Create 0MQ socket in the default context.
+
+WARNING:
+The Socket is not thread safe. This means that you cannot access the same Socket
+from different goroutines without using something like a mutex.
+
+For a description of socket types, see: http://api.zeromq.org/2-2:zmq-socket#toc3
+*/
+func NewSocket(t Type) (soc *Socket, err error) {
+	return defaultCtx.NewSocket(t)
+}
+
+/*
+Create 0MQ socket.
+
+WARNING:
+The Socket is not thread safe. This means that you cannot access the same Socket
+from different goroutines without using something like a mutex.
+
+For a description of socket types, see: http://api.zeromq.org/2-2:zmq-socket#toc3
+*/
+func (ctx *Context) NewSocket(t Type) (soc *Socket, err error) {
+	soc = &Socket{}
+	if !ctx.opened {
+		return soc, ErrorContextClosed
+	}
+	s, e := C.zmq_socket(ctx.ctx, C.int(t))
+	if s == nil {
+		err = errget(e)
+		soc.err = err
+	} else {
+		soc.soc = s
+		soc.ctx = ctx
+		soc.opened = true
+		runtime.SetFinalizer(soc, (*Socket).Close)
+	}
+	return
+}
+
+// If not called explicitly, the socket will be closed on garbage collection
+func (soc *Socket) Close() error {
+	if soc.opened {
+		soc.opened = false
+		if i, err := C.zmq_close(soc.soc); int(i) != 0 {
+			soc.err = errget(err)
+		}
+		soc.soc = unsafe.Pointer(nil)
+		soc.ctx = nil
+	}
+	return soc.err
+}
+
+/*
+Accept incoming connections on a socket.
+
+For a description of endpoint, see: http://api.zeromq.org/2-2:zmq-bind#toc2
+*/
+func (soc *Socket) Bind(endpoint string) error {
+	if !soc.opened {
+		return ErrorSocketClosed
+	}
+	s := C.CString(endpoint)
+	defer C.free(unsafe.Pointer(s))
+	if i, err := C.zmq_bind(soc.soc, s); int(i) != 0 {
+		return errget(err)
+	}
+	return nil
+}
+
+/*
+Create outgoing connection from socket.
+
+For a description of endpoint, see: http://api.zeromq.org/2-2:zmq-connect#toc2
+*/
+func (soc *Socket) Connect(endpoint string) error {
+	if !soc.opened {
+		return ErrorSocketClosed
+	}
+	s := C.CString(endpoint)
+	defer C.free(unsafe.Pointer(s))
+	if i, err := C.zmq_connect(soc.soc, s); int(i) != 0 {
+		return errget(err)
+	}
+	return nil
+}
+
+/*
+Receive a message part from a socket.
+
+For a description of flags, see: http://api.zeromq.org/2-2:zmq-recv#toc2
+*/
+func (soc *Socket) Recv(flags Flag) (string, error) {
+	b, err := soc.RecvBytes(flags)
+	return string(b), err
+}
+
+/*
+Receive a message part from a socket.
+
+For a description of flags, see: http://api.zeromq.org/2-2:zmq-recv#toc2
+*/
+func (soc *Socket) RecvBytes(flags Flag) ([]byte, error) {
+	if !soc.opened {
+		return []byte{}, ErrorSocketClosed
+	}
+	var msg C.zmq_msg_t
+	if i, err := C.zmq_msg_init(&msg); i != 0 {
+		return []byte{}, errget(err)
+	}
+	defer C.zmq_msg_close(&msg)
+
+	var size C.int
+	var err error
+
+	var i C.int
+	i, err = C.zmq_recv(soc.soc, &msg, C.int(flags))
+	if i == 0 {
+		size = C.int(C.zmq_msg_size(&msg))
+	} else {
+		size = -1
+	}
+
+	if size < 0 {
+		return []byte{}, errget(err)
+	}
+	if size == 0 {
+		return []byte{}, nil
+	}
+	data := make([]byte, int(size))
+	C.my_memcpy(unsafe.Pointer(&data[0]), C.zmq_msg_data(&msg), C.size_t(size))
+	return data, nil
+}
+
+/*
+Send a message part on a socket.
+
+For a description of flags, see: http://api.zeromq.org/2-2:zmq-send#toc2
+*/
+func (soc *Socket) Send(data string, flags Flag) (int, error) {
+	return soc.SendBytes([]byte(data), flags)
+}
+
+/*
+Send a message part on a socket.
+
+For a description of flags, see: http://api.zeromq.org/2-2:zmq-send#toc2
+*/
+func (soc *Socket) SendBytes(data []byte, flags Flag) (int, error) {
+	if !soc.opened {
+		return 0, ErrorSocketClosed
+	}
+	datac := C.CString(string(data))
+	var msg C.zmq_msg_t
+	if i, err := C.my_msg_init_data(&msg, unsafe.Pointer(datac), C.size_t(len(data))); i != 0 {
+		return -1, errget(err)
+	}
+	defer C.zmq_msg_close(&msg)
+	n, err := C.zmq_send(soc.soc, &msg, C.int(flags))
+	if n != 0 {
+		return -1, errget(err)
+	}
+	return int(n), nil
+}
+
+/*
+Start built-in ØMQ device
+
+see: http://api.zeromq.org/2-2:zmq-device#toc2
+*/
+func Device(device Dev, frontend, backend *Socket) error {
+	if !(frontend.opened && backend.opened) {
+		return ErrorSocketClosed
+	}
+	_, err := C.zmq_device(C.int(device), frontend.soc, backend.soc)
+	return errget(err)
+}
+
+/*
+Emulate the proxy that will be built-in in 0MQ version 3
+
+See: http://api.zeromq.org/3-2:zmq-proxy
+*/
+func Proxy(frontend, backend, capture *Socket) error {
+	items := NewPoller()
+	items.Add(frontend, POLLIN)
+	items.Add(backend, POLLIN)
+	for {
+		sockets, err := items.Poll(-1)
+		if err != nil {
+			return err
+		}
+		for _, socket := range sockets {
+			for more := true; more; {
+				msg, err := socket.Socket.RecvBytes(0)
+				if err != nil {
+					return err
+				}
+				more, err = socket.Socket.GetRcvmore()
+				if err != nil {
+					return err
+				}
+				fl := SNDMORE
+				if !more {
+					fl = 0
+				}
+
+				if capture != nil {
+					_, err = capture.SendBytes(msg, fl)
+					if err != nil {
+						return err
+					}
+				}
+
+				switch socket.Socket {
+				case frontend:
+					_, err = backend.SendBytes(msg, fl)
+				case backend:
+					_, err = frontend.SendBytes(msg, fl)
+				}
+				if err != nil {
+					return err
+				}
+			}
+		}
+	}
+	return nil
+}
diff --git a/vendor/src/github.com/pebbe/zmq2/zmq2.h b/vendor/src/github.com/pebbe/zmq2/zmq2.h
new file mode 100644
index 0000000..1ff2d08
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/zmq2.h
@@ -0,0 +1,9 @@
+#if ZMQ_VERSION_MAJOR != 2
+#error "You need ZeroMQ version 2 to build this"
+#endif
+#ifndef ZMQ_RCVTIMEO
+#define ZMQ_RCVTIMEO 0
+#endif
+#ifndef ZMQ_SNDTIMEO
+#define ZMQ_SNDTIMEO 0
+#endif
diff --git a/vendor/src/github.com/pebbe/zmq2/zmq2_test.go b/vendor/src/github.com/pebbe/zmq2/zmq2_test.go
new file mode 100644
index 0000000..a7a0a4b
--- /dev/null
+++ b/vendor/src/github.com/pebbe/zmq2/zmq2_test.go
@@ -0,0 +1,86 @@
+package zmq2_test
+
+import (
+	zmq "github.com/pebbe/zmq2"
+
+	"fmt"
+	"runtime"
+)
+
+func Example_test_version() {
+	major, minor, _ := zmq.Version()
+	fmt.Println("Version major:", major)
+	fmt.Println("Version minor == 1|2:", minor == 1 || minor == 2)
+	// Output:
+	// Version major: 2
+	// Version minor == 1|2: true
+}
+
+func Example_test_hwm() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func Example_test_pair_inproc() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func Example_test_pair_ipc() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func Example_test_pair_tcp() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func Example_test_reqrep_inproc() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func Example_test_reqrep_ipc() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func Example_test_reqrep_tcp() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func Example_test_shutdown_stress() {
+
+	fmt.Println("Done")
+	// Output:
+	// Done
+}
+
+func checkErr(err error) bool {
+	if err != nil {
+		_, filename, lineno, ok := runtime.Caller(1)
+		if ok {
+			fmt.Printf("%v:%v: %v\n", filename, lineno, err)
+		} else {
+			fmt.Println(err)
+		}
+		return true
+	}
+	return false
+}
